<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      
    </description>
    
        
            <item>
                <title>Yin Tat Lee (coPI) named 2020 Sloan Research Fellows</title>
                <link>http://localhost:4000/news/2020/02/12/sloan/</link>
                <content:encoded>
                    <![CDATA[
                    <p>We are very happy to announce that Yin Tat Lee (ADSI coPI) have been named 2020 Sloan Research Fellows by the Alfred P. Sloan Foundation. The program recognizes early-career scientists in the United States and Canada who are nominated and judged by their peers based on their creativity, leadership, and achievements in research. More details can be found in the Allen School news <a href="https://news.cs.washington.edu/2020/02/12/hannaneh-hajishirzi-and-yin-tat-lee-named-2020-sloan-research-fellows/">post</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2020/02/12/sloan/</guid>
                <description>
                    
                    ADSI coPI, Yin Tat Lee, have been named 2020 Sloan Research Fellows by the Alfred P. Sloan Foundation. The program recognizes early-career scientists in the United States and Canada who are nominated and judged by their peers based on their creativity, leadership, and achievements in research.
                    
                </description>
                <pubDate>Wed, 12 Feb 2020 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Corinne Jones won the ASA Statistical Computing Award</title>
                <link>http://localhost:4000/news/2020/01/17/Corinne-Jones/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Corinne Jones won the 2020 Chambers Student Paper Award from the ASA Statistical Computing Section, for her paper “End-to-end Learning,
with or without Labels” (<a href="https://arxiv.org/abs/1912.12979">arxiv report</a>; <a href="https://github.com/cjones6/xsdc">github repository</a>).</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2020/01/17/Corinne-Jones/</guid>
                <description>
                    
                    Corinne Jones won the 2020 Chambers Student Paper Award from the ASA Statistical Computing Section, for her paper &quot;End-to-end Learning, with or without Labels”.
                    
                </description>
                <pubDate>Fri, 17 Jan 2020 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Talk Announcement</title>
                <link>http://localhost:4000/news/2020/01/03/SteffenGrunewalder/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Steffen will be visiting From Jan 21 to Feb 1 as part of the ADSI Visiting Faculty Program. He will be sitting in the ADSI faculty office, room CSE2 315. During his visit, Steffen will be giving a ML lunch seminar on Thurs Jan 23rd at noon. The details are follows:</p>

<p>Title:
Oblivious data for kernel methods</p>

<p>Abstract:
I’ll present an approach to reduce the influence of sensitive features in data in the context of kernel methods. The resulting method uses Hilbert space valued conditional expectations to create new features that are close approximations of the original (non-sensitive) features while having a reduced dependence on the sensitive features. I’ll provide optimality statements about these new features and a bound on the alpha-mixing coefficient between the sensitive features and these new features. In practice, standard techniques to estimate conditional expectations can be used to generate these features. I’ll discuss a plug-in approach for estimating conditional expectation which uses properties of the empirical process to control estimation errors.</p>

<p>Short bio:
Steffen is an assistant professor in the department of mathematics and statistics at Lancaster University, UK. He joined the department in autumn 2014. His main area of research is large-scale machine learning with a focus on the interplay of kernel methods, convex optimization in Hilbert spaces and empirical process theory. Prior to joining Lancaster University, Steffen was a postdoctoral researcher in computer science at University College London. He completed his PhD in machine learning at the Technical University Berlin.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2020/01/03/SteffenGrunewalder/</guid>
                <description>
                    
                    Upcoming talk at UW by Steffen Grunewalder. Steffen will be visiting From Jan 21 to Feb 1 as part of the ADSI Visiting Faculty Program. During his visit, Steffen will be giving a ML lunch seminar on Thurs Jan 23rd at noon.
                    
                </description>
                <pubDate>Fri, 03 Jan 2020 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>&lt;a href='https://www.ece.uw.edu/spotlight/lytle2019/'&gt;Talk Announcement&lt;/a&gt;</title>
                <link>http://localhost:4000/news/2019/11/27/Stephane_Mallat/</link>
                <content:encoded>
                    <![CDATA[
                    

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/11/27/Stephane_Mallat/</guid>
                <description>
                    
                    Upcoming talks at UW by Stéphane Mallat. Hosted by Prof Les Atlas and PI Maryam Fazel, Stéphane will be visiting on Tuesday December 3rd. During his visit, Stéphane will give two talks. The first talk is the ECE colloquium titled Interpretable Deep Networks for Classification, Generation and Physics. The second talk is the ECE Lytle Lecture titled Mathematical Mysteries of Deep Neural Networks. Details can be found &lt;a href='https://www.ece.uw.edu/lytle-lecture-series/'&gt;here&lt;/a&gt;.
                    
                </description>
                <pubDate>Wed, 27 Nov 2019 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>&lt;a href='https://blogs.uw.edu/tops/marco-cuturi-computational-optimal-transport/'&gt;Talk Announcement&lt;/a&gt;</title>
                <link>http://localhost:4000/news/2019/11/16/Cuturi_Core/</link>
                <content:encoded>
                    <![CDATA[
                    

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/11/16/Cuturi_Core/</guid>
                <description>
                    
                    Upcoming talk at UW by Marco Cuturi. Marco will be visiting on Tuesday November 19th as part of the ADSI Visiting Faculty Program. During his visit, Marco will be giving an ADSI Distinguished Lecture on Computational Optimal Transport. Details can be found &lt;a href='https://blogs.uw.edu/tops/marco-cuturi-computational-optimal-transport/'&gt;here&lt;/a&gt;.
                    
                </description>
                <pubDate>Sat, 16 Nov 2019 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Promotion News</title>
                <link>http://localhost:4000/news/2019/10/11/promotion/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Dmitriy Drusvyatskiy (Math), Zaid Harchaoui (Statistics) have been promoted to Associate Professor and Maryam Fazel (ECE) has been promoted to full Professor.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/10/11/promotion/</guid>
                <description>
                    
                    Dmitriy Drusvyatskiy (Math), Zaid Harchaoui (Statistics) have been promoted to Associate Professor and Maryam Fazel (ECE) has been promoted to full Professor.
                    
                </description>
                <pubDate>Fri, 11 Oct 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Dmitriy Drusvyatskiy receives the INFORMS Young Researcher Prize</title>
                <link>http://localhost:4000/news/2019/10/11/Dima-INFORMS/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Dmitriy Drusvyatskiy and Damek Davis (Cornell) are the joint recipients of the 2019 INFORMS Optimization Society Prize for Young Researchers for their outstanding paper in optimization (details forthcoming). This prize serves as an esteemed recognition of promising colleagues who are at the beginning of their academic or industrial career.</p>

<p>More details can be found <a href="https://math.washington.edu/news/2019/09/30/dmitriy-drusvyatskiy-receives-informs-young-researcher-prize">here</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/10/11/Dima-INFORMS/</guid>
                <description>
                    
                    Dmitriy Drusvyatskiy and Damek Davis (Cornell) are the joint recipients of the 2019 INFORMS Optimization Society Prize for Young Researchers for their outstanding paper in optimization.
                    
                </description>
                <pubDate>Fri, 11 Oct 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Summer Workshop on Algorithmic Foundations of Learning and Control</title>
                <link>http://localhost:4000/news/2019/08/21/workshop/</link>
                <content:encoded>
                    <![CDATA[
                    <center><img src="http://ads-institute.uw.edu/images/workshop.jpg" /></center>

<p><a href="https://ajwagen.github.io/adsi_learning_and_control/">The ADSI Summer Workshop on Algorithmic Foundations of Learning and Control</a> was organized by UW and U Wisconsin over 2.5 days (August 19-21), held on the UW Campus.</p>

<p>The workshop provided a forum to discuss timely topics bridging the different research communities: the statistical and reinforcement learning community, the optimization and control theory community, as well as robotics practitioners. The event brought together researchers with diverse backgrounds in computer science, control theory, statistics and math, to discuss theoretical and foundational questions arising from dynamical systems that aim to learn from, and take action in, their environments (such as robotic systems that perform manipulation and navigation).</p>

<p>The format included five 45-minute talks per day (and 3 on the last day), with breaks for discussion, as well as two panel sessions where the speakers of each day formed a panel and debated questions raised by the audience. The timely and exciting intersection of fields, and the carefully chosen list of speakers, contributed to lively debates and discussions about different viewpoints. For example, one of the talks (on online learning and control of linear dynamical systems) motivated discussions about how the theoretical computer science community and the control theory community use different terminology for similar concepts, and how to progress towards connecting these disciplines better and providing a common language to address contemporary challenges in learning and artificial intelligence.</p>

<p><img style="float: right; width:50%;" src="http://ads-institute.uw.edu/images/workshop2.jpg" /></p>

<p>Speakers included Todorov, Boots (UW), Parrilo (MIT), Recht (UC Berkeley), Szepesvari (U Alberta &amp; Google), Kumar (Google Brian), Brunskill, Ye (Stanford), Russo (Columbia), Wang (Princeton), Ozay (Michigan), Mansour (Tel Aviv University), Agarwal (Microsoft Research).</p>

<p>The summer school was made possible by support from National Science Foundation. The program was organized by Maryam Fazel, Kevin Jamieson, and Sham Kakade.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/08/21/workshop/</guid>
                <description>
                    
                    A 3-day Summer Workshop on Learning and Control organized by ADSI. It includes 13 talks and 2 panel sessions.
                    
                </description>
                <pubDate>Wed, 21 Aug 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Summer School on Foundations of Data Science</title>
                <link>http://localhost:4000/news/2019/08/17/summer-school/</link>
                <content:encoded>
                    <![CDATA[
                    <center><img src="http://ads-institute.uw.edu/images/school.jpg" /></center>

<p><a href="https://alecgt.github.io/adsi_summer/">The ADSI Summer School on Foundations of Data Science</a> was organized by U. Washington and U.W. Madison over 4.5 days (August 13-17, 2019) at the University of Washington in Seattle. The event gave a large and advanced introduction to fundamental aspects of data science, including stochastic optimization, convex optimization, convex geometry and sampling, computational imaging, deep generative models, fairness in machine learning, reinforcement learning and active machine learning. The format was mostly lectures accompanied with notes. They were generally organized by a first part covering basic concepts and a second part introducing present research with an emphasis on recent discoveries of fundamental principles. An interactive lab was also conducted in computational imaging.</p>

<p>The target audience was graduate students and postdocs, with some knowledge of some of the fundamental areas of data science, but a desire to learn about other topics in the area. 
Lecturers included Paquette (U. Waterloo); Diakonikolas, Hardt (U.C. Berkeley); Lee, Jamieson, Oh (U. Washington); Nowak (U.W. Madison); Willett (U. Chicago); Agarwal (Microsoft Research). A poster session was also organised at which participants presented their research.</p>

<p>The summer school was made possible by support from National Science Foundation. The program was organized by Dmitriy Drusvyatskiy and Zaid Harchaoui.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/08/17/summer-school/</guid>
                <description>
                    
                    A 5-day Summer School on Foundations of Data Science organized by ADSI. 65 graduate students and postdocs from different schools joined.
                    
                </description>
                <pubDate>Sat, 17 Aug 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Announcing the ADSI Workshop on Learning and Control</title>
                <link>http://localhost:4000/news/2019/06/10/Workshop/</link>
                <content:encoded>
                    <![CDATA[
                    <p>The ADSI faculty announce the organization of a workshop in August 2019 on the Algorithmic Foundations of Learning and Control. The workshop is geared towards researchers in computer science, control theory, statistics, and mathematics. It aims to bring together people from a diverse set of backgrounds who are interested in bridging the gap between reinforcement learning, control theory, and statistical learning.</p>

<p>The workshop will feature lectures from distinguished speakers spanning these disciplines as well as open problem and discussion sessions. More details can be found on the workshop <a href="https://ajwagen.github.io/adsi_learning_and_control/">website</a>. Researchers currently working in the area are encouraged to apply.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/06/10/Workshop/</guid>
                <description>
                    
                    ADSI is pleased to announce a workshop on learning and control to be hosted at the University of Washington in August 2019. The program includes a distinguished panel of speakers bridging a diverse collection of perspectives at the forefront of these topics.
                    
                </description>
                <pubDate>Mon, 10 Jun 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Announcing the 2019 ADSI Summer School</title>
                <link>http://localhost:4000/news/2019/06/10/SummerSchool/</link>
                <content:encoded>
                    <![CDATA[
                    <p>The ADSI faculty are pleased to announce the development of a summer school that aims to promote discussion and collaboration within several research areas at the core of algorithmic data science. The ADSI Summer School on the Foundations of Data Science, jointly organized with <a href="https://ifds.wisc.edu/">IFDS</a> at the University of Wisconsin-Madison, is organized around a lecture series emphasizing topics in optimization, computation, and learning theory that lie at the foundation of modern machine learning. The program includes both ADSI faculty and distinguished invited speakers.</p>

<p>The summer school will feature opportunities for discussion and collaboration between participants, as well as a venue for students to share their own work. More details, including application information, can be found on the summer school <a href="https://alecgt.github.io/adsi_summer/">website</a>. The organizers encourage all interested students or researchers to apply.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/06/10/SummerSchool/</guid>
                <description>
                    
                    ADSI is pleased to announce a summer school to be hosted at the University of Washington in August 2019. The program will offer a platform for leading experts to communicate recent advances, educate participating students, and build connections between diverse areas of data science.
                    
                </description>
                <pubDate>Mon, 10 Jun 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Yin Tat Lee receives Microsoft Research Faculty Fellowship</title>
                <link>http://localhost:4000/news/2019/04/30/microsoft-fellowship/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Professor Yin Tat Lee of the Allen School’s Theory of Computation group has been named a 2019 Microsoft Research Faculty Fellow. Since 2005, Microsoft has used its Faculty Fellowship program to recognize promising, early-career researchers whose exceptional research talent makes them emerging leaders in their fields. Lee, who joined the University of Washington faculty in 2017, is one of only five recipients selected by Microsoft this year from universities and colleges across North America.</p>

<p>More details can be found in the Allen School news <a href="https://news.cs.washington.edu/2019/04/30/yin-tat-lee-receives-microsoft-research-faculty-fellowship/">post</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/04/30/microsoft-fellowship/</guid>
                <description>
                    
                    ADSI coPI, Yin Tat Lee, has been named a 2019 Microsoft Research Faculty Fellow. Since 2005, Microsoft has used its Faculty Fellowship program to recognize promising, early-career researchers whose exceptional research talent makes them emerging leaders in their fields.
                    
                </description>
                <pubDate>Tue, 30 Apr 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Upcoming West Coast Optimization Meeting</title>
                <link>http://localhost:4000/news/2019/04/29/WCOM/</link>
                <content:encoded>
                    <![CDATA[
                    <p>The Annual West Coast Optimization Meeting (WCOM) will take place on May 4th. The up-to-date information, including the list of speakers and the schedule, can be found on the <a href="https://uw-amo.github.io/WCOM2019/index.html">WCOM webpage</a>.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2019/04/29/WCOM/</guid>
                <description>
                    
                    An optimization conference at the UW this Spring
                    
                </description>
                <pubDate>Mon, 29 Apr 2019 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Training Deep Structured Prediction Models at Scale</title>
                <link>http://localhost:4000/blog/2018/12/17/deep-struct-pred/</link>
                <content:encoded>
                    <![CDATA[
                    <p>This blog post decribes the 
<a href="https://papers.nips.cc/paper/7726-a-smoother-way-to-train-structured-prediction-models">recent NeurIPS 2018 paper</a> 
and <a href="https://github.com/krishnap25/casimir">companion code</a> on smooth training of max-margin structured prediction models. Training structured prediction models consists in optimizing a non-smooth objective using an inference combinatorial optimization algorithm.</p>

<p>We propose a framework called <a href="https://github.com/krishnap25/casimir">Casimir</a> based on the Catalyst acceleration and infimal-convolution smoothing allowing us to break the non-smoothness barrier and obtain fast incremental algorithms for large-scale training of deep structured prediction models.</p>

<h2 id="setting">Setting</h2>
<p>Structured prediction consists in predicting complex outputs such as sequences, trees or lattices. For instance, named entity recognition can be cast as the task of predicting a sequence of tags, one for each word which identifies the word as a named entity.</p>

<!-- ![NER](http://ads-institute.uw.edu/images/201812_smoother/ner.png) -->
<p align="center">
  <img src="http://ads-institute.uw.edu/images/201812_smoother/ner.png" />
</p>

<p>In this example, an output is a chain of tags, where each tag can take values from a dictionary. In general, the set \(\mathcal{Y}\) of all outputs is finite but too large to enumerate.
To overcome this difficulty, a <em>score function</em> \(\phi(\cdot, \cdot; w)\), parameterized by \(w\), is defined to measure the compatibility of the input-output pair \((x, y)\) as \(\phi(x,y;w)\). This score function decomposes over the structure of the outputs (e.g., a chain) so that predictions can be made by an inference procedure which finds</p>

\[y^*(x ; w) \in \operatorname*{arg max}_{y \in \mathcal{Y}} \phi(x, y ; w) \,.\]

<p>The inference problem can be solved in various settings of interest by efficient combinatorial algorithms such as the Viterbi algorithm for named entity recognition.</p>

<p>The goal of the <em>learning problem</em> is to find the best parameter \(w\) so that inference \(y^*(x ; w)\) produces the correct output. Given a loss \(\ell\) such as the Hamming loss, max-margin structured prediction aims to minimize a surrogate called the structural hinge loss, which is defined for an input-output pair \((x_i, y_i)\) as</p>

\[f_i(w) = \max_{y \in \mathcal{Y}} \psi_i(y; w)  \,,\]

<p>where \(\psi_i\) is defined as a generalization of the margin used in classical support vector machine,</p>

\[\psi_i(y; w) = \phi(x_i, y ; w) + \ell(y_i, y) - \phi(x_i, y_i ; w) \,.\]

<p>The <em>optimization problem</em> is defined for samples \(\{(x_i, y_i)\}_{i=1}^n\) as the regularized empirical surrogate risk minimization</p>

\[\min_w \left[ F(w) = \frac{1}{n}\sum_{i=1}^n f_i(w) + \frac{\lambda}{2}\|w\|_2^2 \right] \,.\]

<p>The subgradient of \(f_i\) is computed by running the inference procedure as</p>

\[\partial f_i(w) \in \operatorname*{arg max}_{y \in \mathcal{Y}} \psi_i(y ; w) \,.\]

<p>Though the above formulation allows one to use tractable first-order information through combinatorial procedures, its non-smoothness prevents us from using fast incremental optimization algorithms. We overcome this challenge by blending an extrapolation scheme for acceleration and an adaptive smoothing scheme.</p>

<h2 id="smoothing">Smoothing</h2>
<p>We now wish to smooth the objective function \(F\) in order to apply incremental algorithms for smooth optimization. This is not straightforward because each \(f_i\) is computed by a discrete inference algorithms.</p>

<p>To smooth \(f_i\), we first note that it can be written as the composition \(f_i = h \circ g_i\), where</p>

\[g_i(w) =  \big(\psi_i(y;w) \big)_{y \in \mathcal{Y}} \,,
\quad \text{and} \quad 
h(z) =  \max_{i \in |\mathcal{Y}|} z_i.\]

<p>The non-smooth max function \(h\) is simply smoothed by adding a strongly convex function \(\omega\) to its dual formulation as</p>

\[h_{\mu\omega}(w) = \max_{u \in \Delta^{|\mathcal{Y}|}} \{
	z^\top u - \mu \omega(u) \} \,,\]

<p>where \(\Delta^m\) is the simplex in \(\mathbb{R}^m\).
It can be shown that \(h_{\mu\omega}\) is a smooth approximation of \(h\) upto \(O(\mu)\) [<a href="https://link.springer.com/article/10.1007/s10107-004-0552-5">N05</a>,<a href="https://epubs.siam.org/doi/abs/10.1137/100818327">BT12</a>]. Common choices of \(\omega\) are given below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Smoothing type</strong></th>
      <th style="text-align: center">\(\omega(u)\)</th>
      <th style="text-align: center"><strong>Smoothing computation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">entropy</td>
      <td style="text-align: center">\(H(u) = \langle u, \log u \rangle\)</td>
      <td style="text-align: center">log-sum-exp</td>
    </tr>
    <tr>
      <td style="text-align: center">\(\ell_2^2\)</td>
      <td style="text-align: center">\(\ell_2^2(u) = \tfrac{1}{2}|u|^2_2\)</td>
      <td style="text-align: center">projection on simplex</td>
    </tr>
  </tbody>
</table>

<p>The smooth structural hinge loss is obtained by replacing the non-smooth \(h\) with its smooth counterpart as</p>

\[f_{i, \mu \omega} = h_{\mu\omega} \circ g_i.\]

<p>In the structured prediction setting, entropy smoothing is equivalent to a conditional random field
[<a href="https://dl.acm.org/citation.cfm?id=655813">LMP01</a>], which is only tractable for tree structured outputs.
On the other hand, the sparse outputs of \(\ell_2^2\) smoothing can be well approximated by picking a small integer \(K\) and considering the top-\(K\) highest scoring outputs. This makes \(\ell_2^2\) smoothing more feasible for tree structured outputs as well as select loopy output structures. See the illustration below for the example of named entity recognition.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://ads-institute.uw.edu/images/201812_smoother/viterbi-max.png" alt="nonsmooth" /></th>
      <th style="text-align: center"><img src="http://ads-institute.uw.edu/images/201812_smoother/viterbi-K.png" alt="l2 smoothing" /></th>
      <th style="text-align: center"><img src="http://ads-institute.uw.edu/images/201812_smoother/viterbi-exp.png" alt="entropy" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Non-smooth</em></td>
      <td style="text-align: center"><em>\(\ell_2^2\) smoothing</em></td>
      <td style="text-align: center"><em>entropy smoothing</em></td>
    </tr>
  </tbody>
</table>

<p>Formally, we define inference oracles, namely the max, top-\(K\) and exp oracles as first order oracles for the structural hinge loss and its smoothed variants with \(\ell_2^2\) and entropy smoothing respectively. 
This allows us to measure the complexity of optimization algorithms, which we discuss next.
The table below shows how the smooth inference oracles are implemented for a given max oracle.
Their computational complexity is given in terms of \(\mathcal{T}\), the cost of max oracle and \(p\), the size of each output \(y\).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Max oracle</strong></th>
      <th style="text-align: center"><strong>Top-\(K\) oracle</strong></th>
      <th style="text-align: center"><strong>Exp oracle</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Max-product</td>
      <td style="text-align: center">Top-\(K\)  max-product, \(\widetilde O(K\mathcal{T})\) time</td>
      <td style="text-align: center">Sum-product, \(O(\mathcal{T})\) time</td>
    </tr>
    <tr>
      <td style="text-align: center">Graph cut</td>
      <td style="text-align: center">BMMF, \(O(pK \mathcal{T})\) time</td>
      <td style="text-align: center">Intractable</td>
    </tr>
    <tr>
      <td style="text-align: center">Graph matching</td>
      <td style="text-align: center">BMMF, \(O(K \mathcal{T})\) time</td>
      <td style="text-align: center">Intractable</td>
    </tr>
    <tr>
      <td style="text-align: center">Branch and bound search</td>
      <td style="text-align: center">Top-\(K\) search</td>
      <td style="text-align: center">Intractable</td>
    </tr>
  </tbody>
</table>

<p>Here, BMMF is the Best max marginal first algorithm of [<a href="https://www.semanticscholar.org/paper/Finding-the-M-Most-Probable-Configurations-Using-Yanover-Weiss/1c38cd37dc563b5182aa49f3f4a735a10caa5daa">YW03</a>].</p>

<h2 id="optimization-algorithms">Optimization algorithms</h2>
<p>The optimization algorithms depend on the nature of the map \(w \mapsto \phi(x, y ; w)\). The structural hinge loss, and thus \(F\), are convex if this map is linear. Otherwise, \(F\) could be nonconvex in general.</p>

<h3 id="convex-structured-prediction">Convex structured prediction</h3>
<p>Classical structured prediction [<a href="https://www.aaai.org/Papers/ICML/2003/ICML03-004.pdf">ATH03</a>, <a href="https://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf">TGK04</a>] uses a linear score \(\phi(x, y;w)=w^\top \Phi(x, y)\) where \(\Phi(x, y)\) is a hand-engineered feature map.
The objective \(F\) is convex here and we extend the generic Catalyst acceleration scheme [<a href="http://www.jmlr.org/papers/volume18/17-748/17-748.pdf">LMH18</a>] to optimize nonsmooth objectives by using smooth surrogates. See also <a href="http://ads-institute.uw.edu//blog/2018/02/06/catalyst/">this blog post</a> for a review. In particular, each iteration considers smoothed and regularized surrogates of the form</p>

\[F_{\mu, \kappa}(w ; z) = \frac{1}{n}\sum_{i=1}^n f_{i, \mu}(w) + \frac{\lambda}{2}\|w\|^2_2 + \frac{\kappa}{2}\|w-z\|^2_2 \,.\]

<h4 id="algorithm">Algorithm</h4>
<p>Starting at \(z_0 = w_0\), at each step \(k\), do</p>

<ul>
  <li>Approximately solve using a linearly convergent algorithm \(\mathcal{M}\):</li>
</ul>

\[w_{k+1} \approx \operatorname*{arg min}_w F_{\mu_k, \kappa_k}(w;z_k)\]

<ul>
  <li>Extrapolate to get</li>
</ul>

\[z_{k+1} = w_k + \beta_k (w_{k+1} - w_{k})\]

<p>When using, for instance, SVRG [<a href="https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf">JZ15</a>] as the linearly convergent algorithm \(\mathcal{M}\), we are guaranteed to get approximate solution \(F(w_k)-F^* \leq \epsilon\) after \(N\) iterations where</p>

\[\mathbb{E}(N) = 
\begin{cases} O \left(n + \sqrt{\frac{n}{\lambda \epsilon}} \right), \quad  \mbox{if fixed smoothing}\\
O \left(n + {\frac{1}{\lambda \epsilon}} \right), \quad  \mbox{if adaptive smoothing}
\end{cases}\]

<h3 id="deep-structured-prediction">Deep structured prediction</h3>
<p>More broadly, deep structured prediction attempts to learn the feature map \(\Phi\). The score function \(\phi(x, y ; w) = w_2^\top \Phi(x, y; w_1)\) is nonlinear in \(w = (w_1, w_2)\) so that \(F\) is nonconvex. In this case, the prox-linear algorithm [<a href="https://link.springer.com/article/10.1007/BF01584377">B85</a>,<a href="https://link.springer.com/article/10.1007/s10107-018-1311-3">DP18</a>] is applicable. It was described previously <a href="http://ads-institute.uw.edu//blog/2018/01/31/prox-linear/">here</a>. This amounts to considering a linear approximation to \(\psi\) as</p>

\[\psi_i(y ; w, z) = \psi_i(y ; z) + \nabla_z \psi_i(y ; z)(w-z) 
\quad \text{and} \quad f_i(w ; z) = \max_{y \in \mathcal{Y}} \psi_i(y ; w, z) \,,\]

<p>to get a regularized convex model</p>

\[F_\gamma(w ; z) = \frac{1}{n}\sum_{i=1}^n f_i(w ; z) + \frac{\lambda}{2}\|w\|_2^2  + \frac{1}{2\gamma} \|w-z\|_2^2 \,.\]

<p>The convex sub-problem above can then be optimized by the convex optimization algorithm described earlier.</p>

<h2 id="numerical-experiments">Numerical experiments</h2>
<p>Given below are results of numerical experiments for named entity recognition on CoNLL-2003 dataset and visual object localization on 
the PASCAL VOC dataset. 
We first show the performance of the proposed algorithm for the convex case, where the feature maps are predefined.
Casimir-SVRG-const and Casimir-SVRG-adapt are the two variants with constant and adaptive smoothing respectively.</p>

<p><img src="http://ads-institute.uw.edu/images/201812_smoother/ner_cvx.png" alt="ner" />
<img src="http://ads-institute.uw.edu/images/201812_smoother/loc_cvx.png" alt="localization" /></p>

<p>Next, we consider deep structured prediction where the feature map is learnt using a convolutional neural network.</p>

<p><img src="http://ads-institute.uw.edu/images/201812_smoother/loc_ncvx.png" alt="localization" /></p>

<p>A <em>software package</em> called <a href="https://github.com/krishnap25/casimir"><strong>Casimir</strong></a> implementing all these algorithms and more is available 
<a href="https://homes.cs.washington.edu/~pillutla/documentation/casimir/">here</a>.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/12/17/deep-struct-pred/</guid>
                <description>
                    
                    This post discusses the use of smoothing and accelerated incremental algorithms for faster training of structured prediction models
                    
                </description>
                <pubDate>Mon, 17 Dec 2018 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Yin Tat Lee (coPI) wins the Best Paper Award at NeurIPS 2018</title>
                <link>http://localhost:4000/news/2018/12/07/neurips-best-paper/</link>
                <content:encoded>
                    <![CDATA[
                    <p>We are very happy to announce that Yin Tat Lee (ADSI coPI), along with a team of researchers that includes ADSI member <a href="http://sbubeck.com/">Sebastien Bubeck</a> of Microsoft Research, has won the Best Paper Award at the 32nd Neural Information Processing Systems (NeurIPS) 2018. The <a href="https://arxiv.org/abs/1806.00291">paper</a> presents two new algorithms that achieve optimal convergence rates for optimizing non-smooth convex functions in distributed networks. More details can be found in the Allen School news <a href="https://news.cs.washington.edu/2018/12/06/allen-schools-yin-tat-lee-earns-best-paper-award-at-neurips-2018-for-new-algorithms-for-distributed-optimization/">post</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/12/07/neurips-best-paper/</guid>
                <description>
                    
                    ADSI coPI, Yin Tat Lee, along with his co-authors, has won the Best Paper Award at the prestigious Machine Learning conference, NeurIPS 2018, for their contribution to distributed optimization.
                    
                </description>
                <pubDate>Fri, 07 Dec 2018 00:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Talk Announcement</title>
                <link>http://localhost:4000/news/2018/10/31/francis-bach-talk/</link>
                <content:encoded>
                    <![CDATA[
                    <p>We are happy to announce that <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a> will be visiting the week
of Nov. 5, 2018, as part of the ADSI Distinguished Visiting Faculty
Program.</p>

<p>During his visit, Francis will be giving i) an ADSI Advanced Lecture
on stochastic gradient descent, and ii) an ADSI Distinguished Lecture on
machine learning research.</p>

<p>Please see the flyers for the title, abstract, time, and location of the ADSI Distinguished Lecture.</p>

<p>Please note that registration for the lecture is now closed.</p>

<p><img src="http://ads-institute.uw.edu/images/Bach-Flyer-1.png" alt="Flyer for the talk" /></p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/10/31/francis-bach-talk/</guid>
                <description>
                    
                    Upcoming talk at UW by Prof. Francis Bach.  Francis Bach will be visiting the week of Nov. 5, 2018, as part of the ADSI Distinguished Visiting Faculty Program. During his visit, Francis will be giving i) an ADSI Advanced Lecture on stochastic gradient descent, and ii) an ADSI Distinguished Lecture on machine learning research.
                    
                </description>
                <pubDate>Wed, 31 Oct 2018 17:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Stochastic Central Path &amp; Projection Maintenance</title>
                <link>http://localhost:4000/blog/2018/10/20/faster-lp/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Linear programs are problems that can be expressed as</p>

\[\min_{x}c^{\top}x\text{ subject to }Ax=b, x\geq0\]

<p>where \(x\in\mathbb{R}^{n}\) are variables, \(A\in\mathbb{R}^{m\times n}\)
is the constraints matrix, \(b\in\mathbb{R}^{m}\)
and \(c\in\mathbb{R}^{n}\) are the coefficients. Many practical problems
in both operations research and computer science can be expressed
or approximated as linear programs. Examples include shortest path,
maximum flow, multi commodity flow, bipartite matching, scheduling
problems, personnel/inventory management, etc. In this blog, we will 
discuss the <a href="https://arxiv.org/abs/1810.07896">recent result</a> of <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Michael_B=">Michael Cohen</a>, <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Zhao">Zhao Song</a> and me about<br />
an nearly ‘‘optimal’’ algorithm for linear programs.</p>

<h2 id="background">Background</h2>

<p>For an arbitrary linear program \(\min_{Ax=b,x\geq0}c^{\top}x\) with
\(n\) variables and \(d\) constraints, the fastest algorithm <a href="https://arxiv.org/abs/1312.6713">[LS15]</a>
takes \(O^{*}(\sqrt{d}\cdot\mathrm{nnz}(A)+d^{2.5})\) where \(\mathrm{nnz}(A)\)
is the number of non-zeros in \(A\), \(O^{*}\) hides all \(n^{o(1)}\)
and \(\log^{O(1)}(1/\epsilon)\) factors, and \(\epsilon\) is the target
accuracy.</p>

<p>For the generic case \(d=\Omega(n)\) we focus in this blog, the current
fastest runtime is dominated by \(O^{*}(n^{2.5})\). This runtime has
not be been improved since the result by <a href="https://doi.org/10.1109/SFCS.1989.63499">Vaidya on 1989</a>.
The \(n^{2.5}\) bound originated from two factors: the cost per iteration
\(n^{2}\) and the number of iterations \(\sqrt{n}\). The \(n^{2}\) cost
per iteration looks optimal because this is the cost to compute \(Ax\)
for a dense \(A\). Therefore, many efforts have been focused on decreasing
the number of iterations while maintaining the cost per iteration.
As for many important linear programs (and convex programs), the number
of iterations has been decreased, including maximum flow <a href="https://arxiv.org/abs/1608.06016">[M16]</a>,
minimum cost flow <a href="https://arxiv.org/abs/1605.01717">[CMSV17]</a>, geometric median <a href="https://arxiv.org/abs/1606.05225">[CLMPS16]</a>, matrix
scaling and balancing <a href="https://arxiv.org/abs/1704.02310">[CMTV17]</a>, and \(\ell_p\)
regression <a href="https://arxiv.org/abs/1711.01328">[BCLL18]</a>. Unfortunately, beating \(\sqrt{n}\) iterations
(or \(\sqrt{d}\) when \(d\ll n\)) for the general case remains one of
the biggest open problems in optimization.</p>

<p>In this blog, we will discuss how to avoid this open problem and develop
a stochastic central path method that has a runtime of</p>

\[O^{*}(n^{\omega}+n^{2+1/3})=O^{*}(n^{\omega})\]

<p>where \(\omega\sim2.37286\) is the exponent of matrix multiplication [<a href="https://www.sciencedirect.com/science/article/pii/S0747717108800132">CW87</a>, <a href="https://dl.acm.org/citation.cfm?id=2214056">Wil12</a>, <a href="https://www.cambridge.org/core/journals/proceedings-of-the-royal-society-of-edinburgh-section-a-mathematics/article/improved-bound-for-complexity-of-matrix-multiplication/998F772AF916572803EBA9C1AD7B4FC1">DS13</a>, <a href="https://arxiv.org/abs/1401.7714">Le14</a>].
This achieves the natural barrier for solving linear programs because
linear system is a special case of linear program and that the currently
fastest way to solve general linear systems involves matrix multiplication.</p>

<h3 id="main-result">Main Result</h3>
<p>Given a linear program \(\min_{Ax=b,x\geq0}c^{\top}x\) with no redundant
constraints. Assume that the polytope has diameter \(R\) in \(\ell_{1}\)
norm, namely, for any \(x\geq0\) with \(Ax=b\), we have \(\|x\|_{1}\leq R\).
Then, for any \(0&lt;\delta\leq1\), we can find \(x\geq0\) such that
\(c^{\top}x  \leq\min_{Ax=b,x\geq0}c^{\top}x+\delta\cdot\|c\|_{\infty}R\)</p>

<p>and</p>

\[\|Ax-b\|_{1} \leq\delta\cdot\left(R\sum_{i,j} | A_{i,j} |+\|b\|_{1}\right)\]

<p>in expected time</p>

\[\left(n^{\omega+o(1)}+n^{2.5-\alpha/2+o(1)}+n^{2+1/6+o(1)}\right)\cdot\log(\frac{n}{\delta})\]

<p>where \(\omega\) is the exponent of matrix multiplication, \(\alpha\) is the dual exponent of matrix multiplication, defined by
 the supremum among all \(a\geq 0\) such that it takes \(n^{2+o(1)}\) time
to multiply an \(n \times n\) matrix by an \(n \times n^a\) matrix.</p>

<h2 id="central-path-method">Central Path Method</h2>

<p>Our algorithm relies on two new ingredients: stochastic central path and projection maintenance. The central path method consider the linear program</p>

\[\min_{Ax=b,x\geq0}c^{\top}x\quad\text{(primal)}\quad\text{and}\quad\max_{A^{\top}y\leq c}b^{\top}y\quad\text{(dual)}\]

<p>with \(A \in \mathbb{R}^{d \times n}\). Any solution of the linear program satisfies
the following optimality conditions:</p>

\[\begin{align}
x_{i}s_{i} &amp; =0\text{ for all }i,\\
Ax &amp; =b,\\
A^{\top}y+s &amp; =c,\\
x_{i},s_{i} &amp; \geq0\text{ for all }i.
\end{align}\]

<p>We call \((x,s,y)\) feasible if it satisfies the last three equations above.
For any feasible \((x,s,y)\), the duality gap is \(\sum_{i}x_{i}s_{i}\).
The central path method find a solution of the linear
program by following the central path which uniformly decrease the duality gap. The central path \((x_{t},s_{t},y_{t})\in\mathbb{R}^{n+n+d}\) is a path parameterized by \(t\) and defined by</p>

\[\begin{align}
x_{t,i}s_{t,i} &amp; =t\text{ for all }i,\\
Ax_{t} &amp; =b,\\
A^{\top}y_{t}+s_{t} &amp; =c,\\
x_{t,i},s_{t,i} &amp; \geq0\text{ for all }i.
\end{align}\]

<p>It is known <a href="https://pubsonline.informs.org/doi/10.1287/moor.19.1.53">[YTM94]</a> how to transform linear programs by adding \(O(n)\) many variables
and constraints so that:</p>
<ul>
  <li>The optimal solution remains the same.</li>
  <li>The central path at \(t=1\) is near \((1_n,1_n,0_d)\) where \(1_n\) and \(0_d\)
are all \(1\) and all \(0\) vectors with appropriate lengths.</li>
  <li>It is easy to covert an approximate solution of the transformed program to the original one.</li>
</ul>

<p>Therefore, it suffices to show how to move gradually \((x_{1},s_{1},y_{1})\) to \((x_{t},s_{t},y_{t})\) for small enough \(t\).</p>

<p><img src="http://ads-institute.uw.edu/images/faster_lp.png" alt="Central Path" /></p>

<h3 id="short-step-central-path-method">Short Step Central Path Method</h3>

<p>The short step central path method maintains \(x_{i}s_{i}=\mu_{i}\) for some vector \(\mu\) such that</p>

\[\sum_i (\mu_i - t)^2 = O(t^2)\]

<p>for some scalar \(t &gt; 0\).</p>

<p>To move from \(\mu\) to
\(\mu+\delta_{\mu}\) approximately, we approximate
the term \((x+\delta_{x})_i(s+\delta_{s})_i\) by \(x_i s_i+x_i\delta_{s,i}+s_i\delta_{x,i}\)
and obtain the following system <a id="delta_x_s_y_mu">#1</a>:</p>

\[\begin{align}
X\delta_{s}+S\delta_{x} &amp; =\delta_{\mu},\notag\\
A\delta_{x} &amp; =0,\label{eq:delta_x_s_y_mu}\\
A^{\top}\delta_{y}+\delta_{s} &amp; =0,\notag
\end{align}\]

<p>where \(X=\mathrm{diag}(x)\) and \(S=\mathrm{diag}(s)\). This equation is the linear approximation of the
original goal (moving from \(\mu\) to \(\mu+\delta_{\mu}\)), and
that the step is explicitly given by the formula <a id="d_step">#2</a></p>

\[\delta_{x}=\frac{X}{\sqrt{XS}}(I-P)\frac{1}{\sqrt{XS}}\delta_{\mu}\text{ and }\delta_{s}=\frac{S}{\sqrt{XS}}P\frac{1}{\sqrt{XS}}\delta_{\mu}\]

<p>where \(P=\sqrt{\frac{X}{S}}A^{\top}\left(A\frac{X}{S}A^{\top}\right)^{-1}A\sqrt{\frac{X}{S}}\)
is an orthogonal projection and the formulas \(\frac{X}{\sqrt{XS}}, \frac{X}{S}, \cdots\) are the diagonal matrices of the corresponding vectors.</p>

<p>A standard choice of \(\delta_{\mu,i}\) is \(- t/\sqrt{n}\) for all \(i\) and this requires \(\tilde{O}(\sqrt{n})\) iterations to converge. Combining this with the inverse maintenance technique [V87], this gives a total runtime of \(n^{2.5}\).
We remark that \(\sum_i (\mu_i - t)^2 = O(t^2)\) is an invariant of the algorithm and the progress is measured by \(t\) because the duality gap is roughly \(n t\).</p>

<h3 id="stochastic-central-path-method">Stochastic Central Path Method</h3>
<p>Now, we discuss how to modify the short step central path to decrease the cost per iteration to roughly \(n^{\omega-\frac{1}{2}}\). 
Since our goal is to implement a central path method in sub-quadratic time per iteration, we even do not have the budget to compute \(Ax\) every iterations. Therefore, instead of maintaining \(\left(A\frac{X}{S}A^{\top}\right)^{-1}\) shown in previous papers, we will study the problem of maintaining
a projection matrix</p>

\[P=\sqrt{\frac{X}{S}}A^{\top}\left(A\frac{X}{S}A^{\top}\right)^{-1}A\sqrt{\frac{X}{S}}\]

<p>due to the <a href="#d_step">formula #2</a> of \(\delta_x\) and \(\delta_s\).</p>

<p>However, even if the projection matrix \(P\) is given explicitly for free, it is difficult
to multiply the dense projection matrix with a dense vector \(\delta_\mu\) in time \(o(n^{2})\).
To avoid moving along a dense \(\delta_{\mu}\), we move along an \(O(k)\) sparse direction \(\tilde{\delta}_{\mu}\) defined by</p>

\[\begin{align}
\tilde{\delta}_{\mu,i}=\begin{cases}
\delta_{\mu,i}/p_{i} , &amp; \text{with probability }p_{i}:= k\cdot\left(\frac{\delta_{\mu,i}^{2}}{ \sum_{l}\delta_{\mu,l}^{2} }+\frac{1}{n}\right);\\
0 , &amp; \text{else}.
\end{cases}\label{eq:tilde_delta_mu}
\end{align}\]

<p>The sparse direction is defined so that we are moving
in the same direction in expectation (\(\mathbb{E} [ \tilde{\delta}_{\mu,i} ] = \delta_{\mu,i}\))
and that the direction has as small variance as possible (\(\mathbb{E} [ \tilde{\delta}_{\mu,i}^{2} ] \leq\frac{\sum_{i}\delta_{\mu,i}^{2}}{k}\)).
If the projection matrix is given explicitly, we can apply the projection matrix on \(\tilde{\delta}_{\mu}\) in time \(O(nk)\).
Picking \(k\sim\sqrt{n}\), the total cost of
projection vector multiplications is about \(n^{2}\).</p>

<p>During the whole algorithm, we maintain a projection matrix</p>

\[\overline{P}=\sqrt{\frac{\overline{X}}{\overline{S}}}A^{\top}\left(A\frac{\overline{X}}{\overline{S}}A^{\top}\right)^{-1}A\sqrt{\frac{\overline{X}}{\overline{S}}}\]

<p>for vectors \(\overline{x}\) and \(\overline{s}\) such that \(\overline{x}_{i}=\Theta(x_{i})\) and \(\overline{s}=\Theta(s_{i})\)
for all \(i\). Since we maintain the projection at a nearby point \((\overline{x}, \overline{s})\), our stochastic step \(x\leftarrow x+\tilde{\delta}_{x}\),
\(s\leftarrow s+\tilde{\delta}_{s}\) and \(y\leftarrow y+ \tilde{\delta}_{y}\) are defined by <a id="tilde_delta_x_s_y_mu"></a></p>

\[\begin{align}
\overline{X}\tilde{\delta}_{s}+\overline{S}\tilde{\delta}_{x} &amp; =\tilde{\delta}_{\mu},\notag\\
A \tilde{\delta}_{x} &amp; =0,\label{eq:tilde_delta_x_s_y_mu}\\
A^{\top}\tilde{\delta}_{y}+ \tilde{\delta}_{s} &amp; =0,\notag
\end{align}\]

<p>which is different from (system #1)[#delta_x_s_y_mu] on both sides of the first equation. Similar to <a href="#d_step">formula #2</a>, one can show that</p>

\[\tilde{\delta}_{x}=\frac{\overline{X}}{\sqrt{\overline{X}\overline{S}}}(I-\overline{P})\frac{1}{\sqrt{\overline{X}\overline{S}}}\tilde{\delta}_{\mu}\text{ and }\tilde{\delta}_{s}=\frac{\overline{S}}{\sqrt{\overline{X}S}}\overline{P}\frac{1}{\sqrt{\overline{X}\overline{S}}}\tilde{\delta}_{\mu}.\]

<p>The previously fastest algorithm involves maintaining the matrix inverse \(( A \frac{X}{S} A^\top )^{-1}\) using subspace embedding techniques [<a href="https://ieeexplore.ieee.org/abstract/document/4031351">Sar06</a>,<a href="https://arxiv.org/abs/1207.6365">CW13</a>,<a href="https://arxiv.org/abs/1211.1002">NN13</a>] and leverage score sampling <a href="https://arxiv.org/abs/0803.0929">[SS11]</a>. In the next section, we will discuss how to maintain the projection directly.</p>

<p>The key departure from the central path we present is that we can only maintain</p>

\[0.9 t \leq \mu_i = x_{i}s_{i} \leq 1.1 t\]

<p>for some \(t&gt;0\) instead of \(\mu\) close to \(t\) in \(\ell_2\) norm.</p>

<p>The short step central
path proof maintains an invariant that \(\sum_{i}(x_{i}s_{i}-t)^{2}=O(t^{2})\).
However, since our stochastic step has a stochastic noise with \(\ell_{2}\) norm
as large as \(t\sqrt{\frac{n}{k}}\), one cannot hope to maintain \(x_{i}s_{i}\)
close to \(t\) in \(\ell_{2}\) norm. Instead, we follow an idea in [<a href="https://arxiv.org/abs/1312.6713">LS14</a>, <a href="https://arxiv.org/abs/1508.04874">LSW15</a>] and maintain the following potential</p>

\[\sum_{i=1}^{n}\cosh\left(\lambda\left(\frac{x_{i}s_{i}}{t}-1\right)\right)=n^{O(1)}\]

<p>with \(\lambda=\Theta(\log n)\). Note that the potential bounded by
\(n^{O(1)}\) implies that \(x_{i}s_{i}\) is a multiplicative approximation
of \(t\). We note that our algorithm is one of the very few central path algorithms [<a href="https://link.springer.com/article/10.1007/s101070200296">PRT02</a>,<a href="https://arxiv.org/abs/1307.2205">M13</a>,<a href="https://arxiv.org/abs/1608.06016">M16</a>] that does not maintain \(x_i s_i\) close to some ideal vector in \(\ell_2\) norm. We are hopeful that our stochastic method and our proof will be useful for future research on interior point methods.</p>

<h2 id="projection-maintenance">Projection Maintenance</h2>

<p>The projection matrix we maintain is of the form \(\sqrt{W}A^{\top}\left(AWA^{\top}\right)^{-1}A\sqrt{W}\)
where \(W=\mathrm{diag}(x/s)\). For intuition, we only explain how to maintain the matrix \(M_{w} := A^{\top}(AWA^{\top})^{-1}A\) for the short step central path step here.
In this case, we have \(\sum_{i}\left(\frac{w_{i}^{\mathrm{new}}-w_{i}}{w_{i}}\right)^{2}=O(1)\) for each step.</p>

<p>If the changes of \(w\) is uniformly across all the coordinates, then \(w_{i}^{\mathrm{new}}=(1\pm\frac{1}{\sqrt{n}})w_{i}\) for all \(i\).
Since it takes \(\sqrt{n}\) steps to change all coordinates
by a constant factor and we only need to maintain \(M_{v}\) with
\(v_{i}=\Theta(w_{i})\) for all \(i\), we can update the matrix
every \(\sqrt{n}\) steps. Hence, the average cost of maintaining the
projection matrix is \(n^{\omega-\frac{1}{2}}\), which is exactly what
we desired.</p>

<p>For the other extreme case that the ‘‘adversary’’ puts all of
his \(\ell_{2}\) budget on few coordinates, only \(\sqrt{n}\) coordinates
are changed by a constant factor after \(\sqrt{n}\) iterations. In
this case, instead of updating \(M_{w}\) every step, we can compute
\(M_{w}h\) online by the woodbury matrix identity:</p>

\[( M + U C V )^{-1} = M^{-1} - M^{-1} U ( C^{-1} + V M^{-1} U )^{-1} V M^{-1}.\]

<p>Let \(S \subset [n]\) denote the set of coordinates that is changed by more than a constant factor and \(r = |S|\).
Using the identity above, we have the <a id="mw_update">update rule</a></p>

\[M_{w^{\mathrm{new}}} = M_{w} - (M_w)_S ( \Delta_{S,S}^{-1} + (M_w)_{S,S} )^{-1} ( (M_w)_S)^\top\]

<p>where \(\Delta=\mathrm{diag}(w^{\mathrm{new}}-w)\), \((M_w)_S\in \mathbb{R}^{n \times r}\) is the \(r\) columns from \(S\) of \(M_w\) and \((M_w)_{S,S},\Delta_{S,S} \in \mathbb{R}^{r \times r}\) are the \(r\) rows and columns from \(S\) of \(M_w\) and \(\Delta\).</p>

<p>As long as \(v_{i}=\Theta(w_{i})\) for all \(i\) except not too many
coordinates, the <a href="#mw_update">update rule</a> can be applied online efficiently. In another case, we can use the <a href="#mw_update">update rule</a> instead to
 update the matrix \(M_{w}\) and
the cost is dominated by multiplying a \(n\times n\) matrix with a
\(n\times n^{r}\) matrix.</p>

<p>Since the cost of multiplying \(n\times n\) matrix by a \(n\times1\)
matrix is same as the cost for \(n\times n\) with \(n\times n^{0.31}\) <a href="https://arxiv.org/abs/1708.05622">[GU18]</a>,
the <a href="#mw_update">update rule</a> should be used to update at least \(n^{0.31}\) coordinates. In
the extreme case we are discussing, we only need to update the matrix
\(n^{\frac{1}{2}-0.31}\) times and each takes \(n^{2}\) time, and hence
the total cost is less than \(n^{\omega}\).</p>

<h2 id="epilogue">Epilogue</h2>
<p>Since you have already read \(1/6\) of the paper, we encourage you to finish the whole paper. Finally, we want to say that we are honored and blessed to have collaborated with Michael Cohen. Arguably, this project is a simple corollary for his beautiful algorithm and proof for the inverse maintenance problem, described in the the following figure. We believe that his enthusiasm in finding the proofs from The Book will be remembered.</p>

<p><img src="http://ads-institute.uw.edu/images/faster_lp2.png" alt="The message that starts this paper" /></p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/10/20/faster-lp/</guid>
                <description>
                    
                    Solving Linear Programs in the Current Matrix Multiplication Time
                    
                </description>
                <pubDate>Sat, 20 Oct 2018 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Three new NSF TRIPODS+X awards won by ADSI PIs</title>
                <link>http://localhost:4000/news/2018/09/22/tripodsX/</link>
                <content:encoded>
                    <![CDATA[
                    <p>We are happy to announce that ADSI PIs have won three new <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=296537&amp;WT.mc_id=USNSF_51&amp;WT.mc_ev=click">NSF TRIPODS+X awards</a>! These include two research grants and one educational grant: the first is a research-track endeavor called ‘‘Safe Imitation Learning for Robotics’’ and is led by Zaid Harchaoui; the second is called ‘‘Foundational Training in Neuroscience and Geoscience via Hack Weeks’’, and is led by Maryam Fazel; the third, titled ‘‘Scaling Up Descriptive Epidemiology and Metabolic Network Models via Faster Sampling’’, is led by Yin Tat Lee. The news release from UW can be found <a href="https://www.washington.edu/news/2018/09/12/tripodsx-grants-data-science/">here</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/09/22/tripodsX/</guid>
                <description>
                    
                    ADSI PIs win three new NSF TRIPODS+X awards, including two research grants and one educational grant.
                    
                </description>
                <pubDate>Sat, 22 Sep 2018 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Yin Tat Lee (coPI) wins the Tucker Prize, and Thomas Rothvoss (affiliate member) wins the Fulkerson prize</title>
                <link>http://localhost:4000/news/2018/08/03/ismp-awards/</link>
                <content:encoded>
                    <![CDATA[
                    <p>We are happy to announce that Yin Tat Lee (ADSI coPI) and Thomas Rothvoss, ADSI affiliate member, were recently recognized for their contributions to mathematical optimization. Yin Tat won the Tucker Prize, and Thomas won the Fulkerson Prize. More details can be found in the Allen School news <a href="https://news.cs.washington.edu/2018/08/03/yin-tat-lee-and-thomas-rothvoss-honored-for-significant-contributions-in-mathematical-optimization/">post</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/08/03/ismp-awards/</guid>
                <description>
                    
                    ADSI coPI, Yin Tat Lee, and ADSI affiliate member, Thomas Rothvoss, recognized for their contributions to mathematical optimization
                    
                </description>
                <pubDate>Fri, 03 Aug 2018 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Workshop Announcement</title>
                <link>http://localhost:4000/news/2018/06/30/Workshop/</link>
                <content:encoded>
                    <![CDATA[
                    <p>ADSI is co-organizing a workshop with the University of Wisconsin’s Institute for the Foundations of Data Science. It aims to explore nonconvex problems arising in data science, their key properties, and current bottlenecks in solving them, and develop algorithms to solve them. The target audience includes people from mathematics, statistics, and computer science, with an interest in nonconvex optimization. The workshop will also feature invited presentations, open problem sessions, and panel discussions.  The workshop takes place July 30-August 1, 2018 at the University of Wisconsin, Madison; for more information, please visit the <a href="https://ifds.wisc.edu/workshops/nonconvex-formulations/">website</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/06/30/Workshop/</guid>
                <description>
                    
                    A workshop co-organized by ADSI
                    
                </description>
                <pubDate>Sat, 30 Jun 2018 17:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Talk Announcement</title>
                <link>http://localhost:4000/news/2018/05/14/nesterov-talk/</link>
                <content:encoded>
                    <![CDATA[
                    <p>We are happy to announce that on Monday, May 21 2018, <a href="https://uclouvain.be/fr/repertoires/yurii.nesterov">Prof. Yurii Nesterov</a> will be speaking on ‘‘Relative smoothness condition and its application to third-order methods’’ as part of our <a href="http://blogs.uw.edu/tops/core-seminar/">CORE Seminar series</a>.</p>

<p>The official abstract of the talk is as follows:</p>

<p><em>In this talk, we show that the recently developed relative smoothness condition can be used for constructing implementable third-order methods for Unconstrained Convex Optimization. At each iteration of these methods, we need to solve an auxiliary problem of minimizing a convex multivariate polynomial, which is a sum of the third-order Taylor approximation and a regularization term. It appears that this nontrivial nonlinear optimization problem can be solved very efficiently by a gradient-type minimization method based on the relative smoothness condition. Its linear rate of convergence depends only on absolute constant. This result opens a possibility for practical implementation of the third-order methods.</em></p>

<p>The talk will be at Smith Hall (SMI) 205, at 4:00 PM.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/05/14/nesterov-talk/</guid>
                <description>
                    
                    Upcoming CORE Seminar talk at UW by Prof. Yurii Nesterov
                    
                </description>
                <pubDate>Mon, 14 May 2018 17:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Stochastic subgradient method converges at the rate \(O(k^{-1/4})\) on weakly convex functions</title>
                <link>http://localhost:4000/blog/2018/04/02/sgd-weaklyconvex/</link>
                <content:encoded>
                    <![CDATA[
                    <p>In this blog, we discuss our recent paper, (<a href="https://arxiv.org/abs/1802.02988">Davis and Drusyatskiy, 2018</a>). This work proves that the proximal stochastic subgradient method converges at a rate \(O(k^{-1/4})\) on weakly convex problems. In particular, it resolves the long-standing open question on the rate of convergence of the proximal stochastic gradient method (without batching) for minimizing a sum of a smooth function and a proximable convex function.</p>

<h1 id="introduction">Introduction</h1>

<p>Stochastic optimization is a fundamental task in the statistical sciences, 
underlying all aspects of learning from data. The goal of stochastic optimization in data science 
is to learn a decision rule from a limited data sample, which generalizes well to the entire population.
Learning such a decision rule amounts to  minimizing the <em>population risk</em>:</p>

\[\begin{align}\label{eqn:SO}
	\min_{x \in \mathbb{R}^d}~ \mathbb{E}_{\xi\sim P}[f(x,\xi)].\tag{$\mathcal{SO}$}
\end{align}\]

<p>Here, \(\xi\) encodes the population data, which is assumed 
to follow some fixed but unknown probability distribution \(P\), 
and the function \(f(x,\xi)\) evaluates the loss of the decision rule 
parametrized by \(x\) on a data point \(\xi\).</p>

<p>Robbins-Monro’s pioneering 1951 work gave the first procedure 
for solving (\ref{eqn:SO}) when \(f(\cdot, \xi)\) are smooth and strongly convex, 
inspiring decades of further research. Among such algorithms,
the stochastic (sub)gradient method is the most successful and widely used in practice.
This method constructs a sequence of approximations \(x_t\) of the minimizer of (\ref{eqn:SO})
by traveling in the direction negative to a sample gradient:</p>

\[\begin{equation*}\label{eqn:SG}
	\textrm{Sample } \xi_t \sim P \\
	\textrm{Set } x_{t+1}= x_t - \alpha_t \nabla_x f(x_t, \xi_t),\tag{$\mathcal{SG}$}
\end{equation*}\]

<p>where \(\alpha_t&gt;0\) is an appropriately chosen control sequence. 
Nonsmooth convex problems may be similarly optimized by replacing sample gradients 
by sample subgradients \(v_t\in \partial_x f(x_t,\xi_t)\), where
\(\partial_x f(x_t, \xi_t)\) is the subdifferential
in the sense of convex analysis; see for example, Part V in (Rockafellar, 1970).</p>

<p>Performance of stochastic optimization methods is best judged 
by their <em>sample complexity</em> – the number of i.i.d. realizations
\(\xi_1, \ldots, \xi_N \sim P\) needed to reach a desired accuracy of the decision rule.
Classical results such as by (Nemirovsky and Yudin, 1983) 
stipulate that for convex problems, it suffices to generate
\(O(\varepsilon^{-2})\) samples to reach functional accuracy \(\varepsilon\) in expectation, 
and this complexity is unimprovable without making stronger assumptions.</p>

<p>While the sample complexity of the stochastic subgradient method is well-understood for convex problems,
much less is known for nonconvex problems. In particular, 
the sample complexity of the method is not yet known for any 
reasonably wide class of problems beyond those that are smooth or convex.
This is somewhat concerning as the stochastic subgradient method is 
the simplest and most widely used stochastic optimization algorithm for 
large-scale problems arising in machine learning and is the core optimization subroutine 
in industry backed software libraries, such as Google’s TensorFlow.</p>

<p>In the recent paper (Davis and Drusvyatskiy, 2018), we aim to close the gap 
between theory and practice and provide the first sample complexity bounds 
for the stochastic subgradient method applied to a large class 
of nonsmooth and nonconvex  optimization problems. The problem class we consider 
captures a variety of important computational tasks in data science, as described below.</p>

<p>Our guarantees apply to an even broader setting than population risk minimization (\ref{eqn:SO}). 
Indeed, numerous tasks in machine learning and high dimensional statistics yield problems of the form</p>

\[\begin{equation}\label{eqn:gen_err}
	\min_{x\in\mathbb{R}^d}~ \varphi(x)=g(x)+r(x),\tag{$\mathcal{P}$}
\end{equation}\]

<p>where the functional components \(g\) and \(r\) play qualitatively different roles. 
The function \(g\colon\mathbb{R}^d\to\mathbb{R}\) plays a similar role 
to the population risk in (\ref{eqn:SO}). We will assume 
that the only access to \(g\) is through stochastic estimates of 
its (generalized) gradients. That is,  given a point \(x\), one can
generate a random vector \(v\in\mathbb{R}^d\) satisfying 
\(\mathbb{E}[v]\in \partial g(x)\). A formal definition of the 
nonconvex subdifferential \(\partial g(x)\) is standard in
the optimization literature; see Definition 8.3 in (Rockafellar and Wets, 1998). 
The exact details will not be important for the blog. 
We note however that when \(g\) is differentiable at \(x\), 
the subdifferential \(\partial g(x)\) consists only of the gradient 
\(\nabla g(x)\), while for convex functions, 
it reduces to the subdifferential in the sense of convex analysis.</p>

<p>In contrast, we assume the function \(r\colon\mathbb{R}^d\to\mathbb{R}\cup\{+\infty\}\)
to be explicitly known and simple. It is often used to model constraints on the parameters
 \(x\) or to encourage \(x\) to have some low dimensional structure, such as sparsity or low rank.
 Within a Bayesian framework, the regularizer \(r\) can model prior distributional information on 
 \(x\). One common assumption, which we also make here, is that
 \(r\) is closed and convex and admits a computable proximal map</p>

\[\begin{equation*}
	{\rm prox}_{\alpha r}(x):= \underset{y}{\operatorname{argmin}}\, \left\{r(y)+\tfrac{1}{2\alpha}\|y-x\|^2\right\}.
\end{equation*}\]

<p>In particular, when \(r\) is an indicator function of a closed convex set 
– meaning it equals zero on it and is \(+\infty\) off it – 
the proximal map \({\rm prox}_{\alpha r}(\cdot)\) reduces to the nearest-point projection.</p>

<p>The most widely used algorithm  for (\ref{eqn:gen_err}) is a direct generalization of (\ref{eqn:SG}), 
called the <em>proximal stochastic subgradient method</em>. Given a current iterate \(x_t\), the
method performs the update</p>

\[\begin{equation*}\left\{
	\begin{aligned}
		&amp;\textrm{Generate a stochastic subgradient } v_t\in\mathbb{R}^d \textrm{ of } g \textrm{ at } x_t\\
		&amp; \textrm{Set } x_{t+1}={\rm prox}_{\alpha_t r}\left(x_{t} - \alpha_t v_t\right)
	\end{aligned}\right\},
\end{equation*}\]

<p>where \(\alpha_t&gt;0\) is an appropriately chosen control sequence.</p>

<h1 id="the-search-for-stationary-points">The search for stationary points</h1>

<p>Convex optimization algorithms are judged by the rate at which they decrease the function value along the iterate sequence. Analysis of smooth optimization algorithms  focuses instead on the magnitude of the gradients along the iterates. The situation becomes quite different for problems that are neither smooth nor convex.</p>

<p>The primary goal, akin to smooth minimization, is the search for stationary points. 
A point \(x\in\mathbb{R}^d\) is called <em>stationary</em> for the problem (\ref{eqn:gen_err}) if the inclusion \(0\in \partial \varphi(x)\) holds. In “primal terms”, these are precisely the points where the directional derivative of \(\varphi\) is nonnegative in every direction. Indeed, under mild conditions on \(\varphi\), equality holds; see Proposition 8.32 in (Rockafellar and Wets, 1998):</p>

\[\begin{equation*} %\label{eqn:subdif_direc_der}
	{\rm dist}(0;\partial \varphi(x))=-\inf_{v:\, \|v\|\leq 1} \varphi'(x;v).
\end{equation*}\]

<p>Thus a point \(x\), satisfying \({\rm dist}(0;\partial \varphi(x))\leq \varepsilon\), approximately satisfies first-order necessary conditions for optimality.</p>

<p>An immediate difficulty in analyzing stochastic subgradient methods for nonsmooth and nonconvex problems is that it is not a priori clear how to measure the progress of the algorithm. Neither the functional suboptimality gap, \(\varphi(x_t)-\min \varphi\), nor the stationarity measure, \({\rm dist}(0;\partial \varphi(x_t))\), necessarily tend to zero along the iterate sequence. Indeed, what is missing is a continuous measure of stationarity to monitor, instead of the highly discontinuous function \(x\mapsto{\rm dist}(0;\partial \varphi(x))\).</p>

<h1 id="weak-convexity-and-the-moreau-envelope">Weak convexity and the Moreau envelope</h1>

<p>In the work (Davis and Drusvyatskiy, 2018), we focus on a class of problems that naturally admit a continuous measure of stationarity. We assume that \(g\colon\mathbb{R}^d\to\mathbb{R}\)  is a
<em>\(\rho\)-weakly convex</em> function, meaning that the assignment \(x\mapsto g(x)+\frac{\rho}{2}\|x\|^2\) is convex. The class of weakly convex functions is  broad. It includes all convex functions and smooth functions with Lipschitz continuous gradient.
More generally,  any function of the form \(g = h\circ c,\) with \(h\) convex and \(L\)-Lipschitz and \(c\) a smooth map with \(\beta\)-Lipschitz Jacobian, is weakly convex with constant \(\rho\leq L\beta\) ; see Lemma 4.2 in (Drusvyatskiy and Paquette, 2016). Notice that such composite functions need not be smooth nor convex. Classical literature highlights the importance of weak convexity in optimization (Rockafellar, 1982; Poliquin and Rockafellar, 1992; Poliquin and Rockafellar, 1996), while recent advances in statistical learning and signal processing have further reinvigorated the problem class. Nonlinear least squares, phase retrieval (Eldar and Mendelson, 2014; Duchi and Ruan, 2017; Davis, Drusvyatskiy, and Paquette, 2017), graph synchronization (Bandeira, Boumal, and Voroninski, 2016; Singer, 2011; Abbe, Bandeira, Bracher, and Singer, 2014), and robust principal component analysis (Candès, Li, Ma, and Wright, 2011; Chandrasekaran, Sanghavi, Parrilo, and Willsky, 2011) naturally lead to weakly convex formulations. For a recent discussion on the role of weak convexity in large-scale optimization, see e.g., (Drusvyatskiy, 2018) or the <a href="../../../01/25/proximal-point/">previous blog post</a>.</p>

<p>It has been known since Nurminskii’s work (Nurminskii, 1974; Nurminskii 1973) that when \(g\) is \(\rho\)-weakly convex and \(r=0\), the stochastic subgradient method generates an iterate sequence that subsequentially converges to a stationary point of the problem, almost surely. Nonetheless, the sample complexity of the basic method and of its proximal extension, has remained elusive. Our approach to resolving this open problem relies on the elementary observation: weakly convex problems naturally admit a continuous measure of stationarity through implicit smoothing.
The key construction we use is the <em>Moreau envelope</em>:</p>

\[\varphi_{\lambda}(x):=\min_{y}~ \left\{\varphi(y)+\tfrac{1}{2\lambda}\|y-x\|^2\right\},\]

<p>where \(\lambda &gt; 0\). Standard results such as Theorem 31.5 in (Rockafellar, 1970) show that as long as \(\lambda&lt;\rho^{-1}\), the envelope \(\varphi_{\lambda}\) is \(C^1\)-smooth with the gradient  given by</p>

\[\begin{equation}\label{eqn:grad_form}
	\nabla \varphi_{\lambda}(x)=\lambda^{-1}(x-{\rm prox}_{\lambda \varphi}(x)).
\end{equation}\]

<p>When \(r=0\) and \(g\) is smooth, the norm \(\|\nabla \varphi_{\lambda}(x)\|\) is proportional to the magnitude of the true gradient \(\|\nabla g(x)\|\).
In the broader nonsmooth setting, the norm of the gradient \(\|\nabla \varphi_{\lambda}(x)\|\) has an intuitive interpretation in terms of near-stationarity for the target problem (\ref{eqn:gen_err}). Namely, the definition of the Moreau envelope directly implies that for any point \(x\in\mathbb{R}^d\), the proximal point \(\hat x:={\rm prox}_{\lambda \varphi}(x)\) satisfies</p>

\[\begin{equation*}
	\left\{\begin{array}{cl}
		\|\hat{x}-x\|&amp;=  \lambda\|\nabla \varphi_{\lambda}(x)\|,\\ %F(\hat x)-F(S_t(x))&amp;\leq \frac{t}{2}(L\beta t+1)\|\mathcal{G}_t(x)\|^2,\\
		\varphi(\hat x) &amp;\leq \varphi(x),\\
		{\rm dist}(0;\partial \varphi(\hat{x}))&amp;\leq \|\nabla \varphi_{\lambda}(x)\|.
	\end{array}\right. 
\end{equation*}\]

<p>Thus a small gradient \(\|\nabla \varphi_{\lambda}(x)\|\) implies that \(x\) is <em>near</em> some point \(\hat x\) that is <em>nearly stationary</em> for (\ref{eqn:gen_err}).
For a longer discussion of the near-stationarity concept, see   (Drusvyatskiy, 2018; Section 4.1 in Drusvyatskiy and Paquette, 2016), or the <a href="../../../01/25/proximal-point/">previous blog post</a>.</p>

<h1 id="contributions">Contributions</h1>

<p>In the paper (Davis and Drusvyatskiy, 2018), we show that under an appropriate choice of the  sequence \(\alpha_t\), the proximal stochastic subgradient method will generate a point \(x\) satisfying \(\mathbb{E}\|\nabla \varphi_{1/(2\rho)}(x)\|\leq \varepsilon\) after at most \(O(\varepsilon^{-4})\) iterations.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>This is perhaps surprising, since neither the Moreau envelope \(\varphi_{\lambda}(\cdot)\) nor the proximal map \({\rm prox}_{\lambda \varphi}(\cdot)\) explicitly appear in the definition of the stochastic subgradient method. Our work appears to be the first to recognize the Moreau envelope as a useful potential function for analyzing subgradient methods.</p>

<p>The convergence guarantees we develop are new even in simplified cases. Two such settings are (a) when \(g\) is smooth and \(r\) is the indicator function of a closed convex set, and (b) when \(g\) is nonsmooth, \(r = 0\), and we have explicit access to the exact subgradients of \(g\).</p>

<h1 id="related-literature-and-context">Related Literature and Context</h1>

<p>Analogous convergence guarantees when \(r\) is an indicator function of a closed convex set were recently established for a different algorithm in (Davis and Grimmer, 2017), called the proximally guided projected subgradient method. This scheme proceeds by directly applying the gradient descent method to the Moreau envelope \(\varphi_{\lambda}\), with each proximal point \({\rm prox}_{\lambda \varphi}(x)\) approximately evaluated by a convex subgradient method. In contrast, we showed  that the basic stochastic subgradient method in the fully proximal setting, and without any modification or parameter tuning, already satisfies the desired convergence guarantees.</p>

<p>Our work also improves in two fundamental ways on the results in the seminal papers on the stochastic proximal gradient method for smooth functions (Ghadimi and Lan, 2013; Ghadimi, Lan, and Zhang, 2016; Xu and Yin, 2015): first, we allow \(g\) to be nonsmooth and second, even when \(g\) is smooth we do not require the variance of our stochastic estimator for \(\nabla g(x_t)\) to decrease as a function of \(t\). The second contribution removes the well-known “mini-batching” requirements common to (Ghadimi, Lan, and Zhang, 2016; Xu and Yin, 2015), while the first significantly expands the class of functions for which the rate of convergence of the stochastic proximal subgradient method is known.</p>

<p>The results in this paper are orthogonal to the recent line of work on accelerated rates of convergence for smooth nonconvex finite sum minimization problems, e.g.,(Lei, Ju, Chen, and Jordan, 2017; <em>Katyusha</em>, Allen-Zhu, 2017; Reddi, Sra, Poczos, Smola, 2016; <em>Natasha 2</em>, Allen-Zhu, 2017). These works crucially exploit the finite sum structure and/or (higher order) smoothness of the objective functions to push beyond the \(O(\varepsilon^{-4})\) complexity. We leave it as an intriguing open question whether such improvement is possible for the nonsmooth weakly convex setting we consider here.</p>

<h1 id="references">References</h1>

<p>E. Abbe, A.S. Bandeira, A. Bracher, and A. Singer. Decoding binary node labels from censored edge measurements: phase transition and efficient recovery. <em>IEEE Trans. Network Sci. Eng.,</em> 1(1):10-22, 2014.</p>

<p>Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In <em>STOC,</em> 2017.</p>

<p>Z. Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. <em>arXiv preprint arXiv:1708.08694</em>, 2017.</p>

<p>Z. Allen-Zhu. How to make gradients small stochastically. <em>Preprint arXiv:1801.02982 (version 1),</em> 2018.</p>

<p>A.S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In <em>Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, June 23-26, 2016,</em> pages 361-382, 2016.</p>

<p>E.J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? <em>J. ACM</em>, 58(3):Art. 11, 37, 2011.</p>

<p>V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A.S. Willsky. Rank-sparsity incoherence for matrix decomposition. <em>SIAM J Optim.,</em> 21(2):572-596, 2011.</p>

<p>D. Davis and D. Drusvyatskiy. Complexity of finding near-stationary points of convex functions stochastically. <em>arXiv:1802.08556</em>, 2018.</p>

<p>D. Davis and D. Drusvyatskiy. Stochastic subgradient method converges at the rate $(O(k^{-1/4})$ on weakly convex functions. <em>arXiv: 1802.02988</em>, 2018.</p>

<p>D. Davis, D. Drusvyatskiy, and C. Paquette. The nonsmooth landscape of phase retrieval. <em>Preprint arXiv:1711.03247</em>, 2017.</p>

<p>D. Davis and B. Grimmer. Proximally guided stochastic method for nonsmooth, non-convex problems. <em>Preprint arXiv:1707.03505,</em> 2017.</p>

<p>D. Drusvyatskiy. The proximal point method revisited. <em>To appear in the SIAG/OPT Views and News, arXiv:1712.06038,</em> 2018.</p>

<p>D. Drusvyatskiy and C. Paquette. Efficiency of minimizing compositions of convex functions and smooth maps. <em>Preprint arXiv:1605.00125</em>, 2016.</p>

<p>J.C. Duchi and F. Ruan. Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval. <em>Preprint arXiv:1705.02356</em>, 2017.</p>

<p>Y.C. Eldar and S. Mendelson. Phase retrieval: stability and recovery guarantees. <em>Appl. Comput. Harmon. Anal.,</em> 36(3):473-494, 2014.</p>

<p>S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. <em>SIAM J. Optim.,</em> 23(4):2341-2368, 2013.</p>

<p>S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization, <em>Math. Program.,</em> 155(1):267-305, 2016.</p>

<p>L. Lei, C. Ju, J. Chen, and M.I Jordan. Non-convex finite-sum optimization via scsg methods. In <em>Advances in Neural Information Processing Systems,</em> pages 2345-2355, 2017.</p>

<p>A.S. Nemirovsky and D.B. Yudin. <em>Problem complexity and method efficiency in optimization</em>. A Wiley-Interscience Publication. John Wiley &amp; Sons, Inc., New York, 1983.</p>

<p>E. A. Nurminskii. The quasigradient method for the solving of the nonlinear programming problems. <em>Cybernetics,</em> 9(1):145-150, Jan 1973.</p>

<p>E. A. Nurminskii. Minimization of nondifferentiable functions in the presence of noise. <em>Cybernetics</em>, 10(4):619-621, Jul 1974.</p>

<p>R.A. Poliquin and R.T. Rockafellar. Amenable functions in optimization. In <em>Nonsmooth optimization: methods and applications (Erice, 1991),</em> pages 338-353. Gordon and Breach, Montreaux, 1992.</p>

<p>R.A. Poliquin and R.T. Rockafellar. Prox-regular functions in variational analysis. <em>Trans. Amer. Math. Soc.</em>, 348:1805-1838, 1996.</p>

<p>Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In <em>Advances in Neural Information Processing Systems</em>, pages 1145-1153, 2016.</p>

<p>R.T. Rockafellar. <em>Convex Analysis</em>. Princeton University Press, 1970.</p>

<p>R.T. Rockafellar. Favorable classes of Lipschitz-continuous functions in subgradient optimization. In <em>Progress in nondifferentiable optimization</em>, volume 8 of IIASA <em>Collaborative Proc. Ser. CP-82</em>, pages 125-143. Int. Inst. Appl. Sys. Anal., Laxenburg, 1982.</p>

<p>R.T. Rockafellar and R.J-B. Wets. <em>Variational Analysis</em>. Grundlehren der mathemtischen Wissenschaften, Vol 317, Springer, Berlin, 1998.</p>

<p>A. Singer. Angular synchronization by eigenvectors and semidefinite programming. <em>Appl. Comput. Harmon. Anal.,</em> 30(1):20-36, 2011.</p>

<p>Y. Xu and W. Yin. Block stochastic gradient iteation for convex and nonconvex optimization. <em>SIAM J. Optim.,</em> 25(3):1686-1716, 2015.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In the supplementary text (<a href="https://arxiv.org/abs/1802.08556">Davis and Drusvyatskiy, 2018</a>), we also showed that when \(g\) happens to be convex, this complexity can be improved to \(\widetilde{O}(\varepsilon^{-2})\)  by adapting a gradual regularization technique of (Allen-Zhu, 2018). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/04/02/sgd-weaklyconvex/</guid>
                <description>
                    
                    Recent breakthrough on using proximal stochastic gradient method for weakly convex functions.
                    
                </description>
                <pubDate>Mon, 02 Apr 2018 00:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Acoustic models for music transcription</title>
                <link>http://localhost:4000/blog/2018/03/14/music/</link>
                <content:encoded>
                    <![CDATA[
                    <h1 id="introduction">Introduction</h1>

<p>The production of classical western music is a two stage process. First
a composer writes down a score: written notation that indicates a
particular musical structure. Then a performer reads this score and
manipulates an instrument as indicated by the score to produce audio
waves that a human ear perceives as music. A trained musician is also
capable of transcribing a performance: while listening to a musical
performance, the musician can write down the score that the performer
used to guide the performance.</p>

<figure>
<center>
  <img src="http://localhost:4000/images/performance_loop.jpeg" alt="my alt text" />
    </center>
  <figcaption><p><font size="3">Figure 1. A musician creates a musical performance from a score. A performance can be transcribed by a musician to recover the score.</font></p></figcaption>
</figure>

<p>The closed loop in this figure implies that the musician and
transcription channels are lossless. Indeed, for western classical music
this is mostly the case: performers are expected to render a faithful
performance of the score provided to them, and so it is possible for the
transcriber to precisely recover the original score.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> In this post,
we will consider methods that replace the human transcriber in this loop
with an automated algorithm.</p>

<h1 id="notation">Notation</h1>

<p>Let \(\mathcal{S}\) denote the space of scores and \(\mathcal{P}\) the space
of performances. We will write \(f : \mathcal{S} \to \mathcal{P}\) to
indicate a performance of a score (\(f\) will be random, to account for
variability in the performance of a particular score). We want to find
an inverse function \(f^{-1} : \mathcal{P} \to \mathcal{S}\) such that
\(f^{-1} \circ f (s) = s\) for all \(s \in \mathcal{S}\).</p>

<p>Specifically, we will represent a score as a binary indicator matrix
\(s \in \{0,1\}^{T \times N}\), where \(T\) is the length of the score
(discretized at some rate) and \(N\) is the range of possible notes (e.g.
\(88\) piano keys). We can represent a performance with air pressure
variation measurements captured by a microphone; these measurements are typically sampled at a rate of \(44.1kHz\), so a performance of length \(T\) can be represented by a vector \(p \in \mathbb{R}^{T\times 44100}\). One
way to formulate our problem is to ask for a function
\(f^{-1} : \mathbb{R}^{T\times 44100} \to \{0,1\}^{T \times N}\) such that
\(f^{-1}\circ f(s) = s\) for all \(s \in \mathcal{S}\).</p>

<p>This formulation has some major flaws. First, it is extremely highly
dimensional. Second, it only works for scores of length \(T\); if we
encounter a score of some other length, our function is undefined and we
cannot transcribe it. What we really want is a function
\(f^{-1} : \mathbb{R}^{c \times 44100} \to \{0,1\}^N\) that predicts which
notes are on at a particular time \(t \in [0,T)\) given a local vector of
contextual audio surrounding time \(t\) (in practice \(c = .5\), a half
second of context, is usually reasonable). To transcribe a recording and
construct a full \(T \times N\) dimensional score, we can simply slide our
frame predictor along from the beginning to the end of the audio,
writing down the notes at each time point.</p>

<figure>
<center>
  <img src="http://localhost:4000/images/frame_based_transcription.jpeg" alt="my alt text" />
  </center>
  <figcaption><p><font size="3">Figure 2. (Left) Audio recording, with highlighted half-second frame of context for prediction of notes at time \(500\) ms. (Right) Score with highlighted note-vector that we wish to predict. </font></p></figcaption>
</figure>

<h1 id="historical-highlights">Historical highlights</h1>

<p>Transcription methods based on fourier analysis were proposed as early
as the 1970s by Piszczalski and Galler (1977). The machine learning
community took an interest in this problem beginning with Raphael
(2002). Early work on this problem from a learning perspective was
stymied by the difficulty of obtaining labels on an audio sequence. A
score on its own is insufficient in two ways. First, it must be
digitized: a pdf score cannot be easily converted to a binary matrix
format. Second, it must be aligned to a recording: a score describes the
sequence and relative timings of events in a performance, but the
performer decides:</p>

<ul>
  <li>
    <p>When the performance will begin.</p>
  </li>
  <li>
    <p>The speed of the performance (within bounds set by the composer).</p>
  </li>
  <li>
    <p>Deliberate deviations from this speed for emotional effect.</p>
  </li>
  <li>
    <p>Random fluctuations in the speed due to human fallibility.</p>
  </li>
</ul>

<p>Each of these problems is daunting, so early work focused on
unsupervised methods (Raphael’s work fits an HMM with Baum-Welch).</p>

<p>To the best of my knowledge, music transcription was first considered as
a supervised learning problem by Poliner and Ellis (2006). These
researchers surmounted the digitization problem by leveraging a growing
body of MIDI files, produced by a community of enthusiasts, which encode
much of the information of a score in a parsable digital format. They
surmounted the alignment problem by producing their own performances
using music synthesis software. By constructing artificial performances,
they were able to exert precise control over timings in the resulting
audio and exactly align their MIDIs to the audio.</p>

<h1 id="recent-developments">Recent developments</h1>

<p>Within the supervised transcription framework introduced in Poliner and
Ellis, there are at least three avenues for performance improvements.
First, models trained on synthesized performances may not generalize
well to human performances so we may want to construct a better dataset.
Second, the model introduced by Poliner and Ellis is a standard SVM on
magnitude spectrum features; it may be possible to construct a better
acoustic model, tailored with prior information about the structure of
music. Third, the frame-based framework doesn’t capture the (rich)
time-series structure of the label space: each frame is predicted
independently.</p>

<p><strong>Datasets.</strong> The easiest way to solve the domain adaptation problem is
to eliminate it by finding some way to align human performances to
scores. A solution to this problem was proposed in an earlier paper by
Ellis himself (Turetsky and Ellis, 2003). We discuss MusicNet below,
which was constructed using a variant of Ellis’s technique. As of 2018,
many datasets of music-aligned scores are available for various genres:</p>

<ul>
  <li>
    <p>Sync-RWC. Goto, Hashiguchi, Nishimura, and Oka (2003).</p>
  </li>
  <li>
    <p>MAPS. Emiya, Badeau, and David (2010).</p>
  </li>
  <li>
    <p>Lakh. Raffel (2016).</p>
  </li>
  <li>
    <p>MusicNet. Thickstun, Harchaoui, and Kakade (2017).</p>
  </li>
</ul>

<p><strong>Acoustic models.</strong> Neural acoustic models have become popular in
recent years. Several research teams have proposed deep acoustic models
for transcription: see Nam, Ngiam, Lee, and Slaney (2011) as well as
Trabelsi, Bilaniuk, Serdyuk, Subramanian, Santos, Mehri, Rostamzadeh,
Bengio, and Pal (2018). There has also been interest in applying
convolutional ideas to transcription: Bittner, McFee, Salamon, Li, and
Bello (2017), as well as Pons and Serra (2017). Our own work at
University of Washington (discussed below) also pursues some of these
convolutional ideas: Thickstun, Harchaoui, and Kakade (2017) and
Thickstun, Harchaoui, Foster, and Kakade (2018).</p>

<p><strong>Time series.</strong> Some recent work breaks away from the strict
frame-based task and seeks to incorporate time-series structure in the
label space of scores to improve transcriptions. These methods typically
combine a frame-based acoustic model with a recurrent state-space model.
See Sigtia, Benetos, Boulanger-Lewandowski, Weyde, Garcez, and Dixon
(2015) as well as Sigtia, Benetos, and Dixon (2016). These ideas are
crucially important to achieving the best performance on this task, but
they are somewhat orthogonal to the acoustic modeling concerns and we
won’t consider them in this post.</p>

<h1 id="music-to-score-alignment">Music-to-score alignment</h1>

<p>To align a performance to a score, we need an observation and a key
idea. The observation is that if a performance and score are aligned,
then at each time location \(t\) the vector of notes in the score
\(s_t \in \{0,1\}^{128}\) and the local frame of audio
\(X_t \in \mathbb{R}^{c\times 44100}\) will be “similar,” in the sense of
a cost \(C\) that we will make precise shortly. The key idea is to
minimize the total dissimilarity at each point \(t \in [0,T)\) between
\(X_t\) and \(s_t\). We do this by shrinking or stretching the performance
\(X \in \mathcal{P}\), resulting in a minimal-cost alignment between the
performance and the score.</p>

<p>Mathematically, this shrinking and stretching amounts to solving the
following optimization problem (\(X_{t_i} \in \mathbb{R}^{c\times 44100}\)
indicates the local contextual frame of audio in the performance \(X\)
centered around time \(t_i\)):</p>

\[\begin{array}{ll@{}ll}
\underset{t \in \mathbb{N}^T}{\text{minimize}} &amp; \displaystyle\sum_{i=1}^n C(X_{t_i},s_i)&amp;\\
\text{subject to}&amp; t_0 = 0, &amp; \\
                         &amp; t_n = m, &amp; \\
                         &amp; t_i \leq t_j &amp; \text{if $i &lt; j$}. &amp; 
\end{array}\]

<p>Dynamic time warping gives an exact solution to this
problem in \(\mathcal{O}(mn)\) time and space, where \(m\) is the length of
the performance and \(n\) is the length of the score.</p>

<figure>
<center>
  <img src="http://localhost:4000/images/alignment.jpeg" alt="my alt text" />
  </center>
  <figcaption><p><font size="3">Figure 3. (Left) A heatmap of local alignment costs between the synthesized and recorded spectrograms, with the optimal alignment path in red. The block from \(x = 0\) to \(x = 100\) frames corresponds to silence at the beginning of the recorded performance. The slope of the alignment can be interpreted as an instantaneous tempo ratio between the recorded and synthesized performances. The curvature in the alignment between \(x = 100\) and \(x = 175\) corresponds to an extension of the first notes by the performer. (Right) Annotation of note onsets on the spectrogram of the recorded performance, determined by the alignment shown on the left. </font></p></figcaption>
</figure>

<p>The success of dynamic time warping depends crucially on defining a good
cost metric; i.e. a cost that is small at points where \(X\) and \(s\) are
well-aligned. This is complicated by the fact that points \(X_{t_i}\) and
\(s_i\) do not even live in the same space. The clever idea introduced by
Turetsky and Ellis (2003) is to map the score \(s\) into performance-space
by synthesizing it. Now we have two vectors \(X_t\) and
\(\text{Synth}(s_i)\) in \(\mathbb{R}^{c\times 44100}\) and we can compare
them using standard metrics (e.g. \(L^p\) metrics). Actually, we need to
be a little more clever than this; we really want our comparisons to be
phase-invariant, so we will actually further transform each of our
vectors into the fourier domain and compare their magnitude spectra.</p>

<p>We can and have applied this automated alignment procedure to construct
a large dataset of labeled classical music. MusicNet is available <a href="https://homes.cs.washington.edu/~thickstn/musicnet.html">here</a>.
For further information about the construction and contents of MusicNet,
see Thickstun, Harchaoui, and Kakade (2017).</p>

<h1 id="translation-invariant-networks">Translation-invariant networks</h1>

<p>Prepared with a large dataset of labeled human performances, we can turn
our attention to models that efficiently capture the structure between
the performances and labels. While we can produce a large dataset using
automated alignments, it is necessarily finite. This stands in contrast
with synthesized datasets, which can be configured to generate an
effectively infinite stream of artificial performances. Therefore we
will focus on models that efficiently capture correlations in our data.
Keeping the bias-variance tradeoff in mind, we will bias our model using
prior information about music.</p>

<p>In particular, we will exploit a translation-invariance in music using a
convolutional architecture. Consider, for motivation, a major triad
chord. The concept of a major chord is preserved under translations in
log-frequency space. If we learn convolutional filters over the
log-frequency domain, we can capture the concept of a major triad using
a single filter. Contrast this situation to a fully connected layer over
log-frequencies, which would need to learn this pattern many times over,
once for each root position of the chord. Therefore, our first step will
be to pre-process our data by mapping it into a log-frequency domain. We
will then convolve along the frequency axis of our filterbank to capture
frequency-invariant features of our data.</p>

<figure>
<center>
  <img src="http://localhost:4000/images/MIREX_architecture2.png" alt="my alt text" />
  </center>
  <figcaption><p><font size="3">Figure 4. A translation-invariant network for note classification. Audio input
maps to Layer 1 according to the log-spaced, cosine-windowed filterbank.
Layer 1 maps to Layer 2 by convolving a set of \(128 \times 1\) learned
filters along the log-frequency axis at each fixed time location. Layer
2 maps to Layer 3 by convolving again along the log-frequency axis, this
time with a set of filters of height 1 that fully connect along the time
and channel axes of Layer 2. Notes are predicted at Layer 4 by linear
classification on the Layer 3
representation. </font></p></figcaption>
</figure>

<p>Further exploiting our awareness of frequency-invariance, we augment our
data by stretching or shrinking our input audio with linear
interpolation. This corresponds to a pitch-shift in the frequency
domain. For small shifts (\(\pm 5\) semitones or less) the transformed
audio sounds natural to the human ear. Randomly shifting each data point
in a minibatch by an integral number of semitones in the range \([-5, 5]\)
augments the dataset by an order of magnitude. And the translational
nature of this augmentation reinforces the architectural structure of
the translation-invariant network. In addition to an integral semitone
shift, we also apply a continuous shift to each data point in the range
\([-.1, .1]\). This makes the models more robust to tuning variation
between recordings.</p>

<p>Last year we submitted this translation-invariant model to the MIREX
Multiple Fundamental Frequency Estimation challenge. This is a
frame-based transcription task based on the Poliner and Ellis framework
discussed in this blog post. Table 1 summarizes results for the top 5
participants in 2017. THK1 is the translation-invariant model described
in this section. For further information about the translation-invariant
network architecture see Thickstun, Harchaoui, Foster, and Kakade
(2018).</p>

<figure>
<center>
<font size="4">
<table style="width:80%; border:0 none; font-size:80%; border-collapse: collapse;" rules="none">
  <tr>
    <td>Model</td>
    <td>Precision</td>
    <td>Recall</td>
    <td>Accuracy</td>
    <td>Etot </td>
  </tr>
  <tr>
    <th colspan="5" align="left">MIREX 2009 Dataset:</th>
  </tr>
  <tr>
    <td>THK1</td>
    <td>82.2</td>
    <td>78.9 </td>
    <td><span style="font-weight:bold">72.0</span></td>
    <td><span style="font-weight:bold">.316 </span></td>
  </tr>
    <tr class="no-border">
    <td>KD1</td>
    <td>72.4</td>
    <td> 81.1 </td>
    <td>66.9 </td>
    <td>.419 </td>
  </tr>
      <tr class="no-border">
    <td>MHMTM1</td>
    <td>72.7</td>
    <td>78.2 </td>
    <td>65.5  </td>
    <td>.441</td>
  </tr>
        <tr>
    <td>WCS1 </td>
    <td> 64.0 </td>
    <td>80.6  </td>
    <td>59.3  </td>
    <td>.569</td>
  </tr>
          <tr>
    <td>ZCY2</td>
    <td> 62.7</td>
    <td>56.2  </td>
    <td>50.6  </td>
    <td> .601</td>
  </tr>
    <tr>
    <th colspan="5" align="left">Su Dataset:</th>
  </tr>
  <tr>
    <td>THK1</td> 
    <td>70.1</td>
    <td>54.6 </td>
    <td><span style="font-weight:bold">51.0</span></td>
    <td><span style="font-weight:bold">.529 </span></td>
  </tr>
    <tr style="border:0 none;">
    <td>KD1</td> 
    <td>45.9 </td>
    <td> 45.0  </td>
    <td> 38.1 </td>
    <td> .745 </td>
  </tr>
      <tr style="border:0 none;">
    <td>WCS1</td>
    <td> 63.6</td> 
    <td>39.7</td>
    <td>35.7</td>
    <td> .700</td>
  </tr>
        <tr>
    <td>MHMTM1 </td>
    <td> 61.2  </td> 
    <td>36.8  </td>
    <td>35.2   </td>
    <td>.676</td>
  </tr>
          <tr>
    <td>ZCY2</td> 
    <td> 40.9</td>
    <td> 28.2  </td>
    <td>26.2  </td>
    <td> .799</td>
  </tr>
</table>
  </font>
  </center>
  <figcaption><p><font size="3">
    Table 1. MIREX 2017 transcription results. Models are evaluated on two
  datasets, the MIREX 2009 dataset and the Su dataset. For details, see 
  <a href="http://www.music-ir.org/mirex/wiki/2017:Multiple_Fundamental_Frequency_Estimation_%26_Tracking_Results_-_MIREX_Dataset">here</a>. 
  </font></p></figcaption>
  </figure>

<h1 id="references">References</h1>

<p>Martin Piszczalski and Bernard A. Galler. 1977. “Automatic music transcription.”  <em>Computer Music Journal</em>, Vol. 1, No. 4, pp. 24-31.
 <a href="https://www.jstor.org/stable/40731297">https://www.jstor.org/stable/40731297</a></p>

<p>Christopher Raphael. 2002. “Automatic transcription of piano music.” <em>Proceedings of the International Society of Music Information Retrieval.</em>
<a href="http://www.ismir2002.ismir.net/proceedings/02-FP01-2.pdf">http://www.ismir2002.ismir.net/proceedings/02-FP01-2.pdf</a></p>

<p>Graham E. Poliner and Daniel P. W. Ellis. 2006. “A discriminative model for polyphonic piano transcription.” <em>EURASIP Journal on Applied Signal Processing.</em>
<a href="https://link.springer.com/content/pdf/10.1155/2007/48317.pdf">https://link.springer.com/content/pdf/10.1155/2007/48317.pdf</a></p>

<p>Robert J. Turetsky and Daniel P. W. Ellis. 2003. “Ground-truth transcriptions of real music from force-aligned midi syntheses.”
<a href="https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/20/paper.pdf">https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/20/paper.pdf</a></p>

<p>Masataka Goto, Hiroki Hashiguchi, Takuichi Nishimura, and Ryuichi Oka.2003. “RWC music database: Music genre database and musical instrument sound database.”
<a href="https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/36/paper.pdf">https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/36/paper.pdf</a></p>

<p>Colin Raffel. 2016. “Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching.” PhD Thesis.
<a href="http://colinraffel.com/publications/thesis.pdf">http://colinraffel.com/publications/thesis.pdf</a></p>

<p>Juhan Nam, Jiquan Ngiam, Honglak Lee, and Malcolm Slaney. 2011. “A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations.” <em>International Society of Music Information Retrieval</em>, pp. 175-180.
<a href="https://www-cs.stanford.edu/~jngiam/papers/ismir2011-PolyphonicTranscription.pdf">https://www-cs.stanford.edu/~jngiam/papers/ismir2011-PolyphonicTranscription.pdf</a></p>

<p>Chiheb Trabelsi, Olexa Bilaniuk, Dmitriy Serdyuk, Sandeep Subramanian, João Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J. Pal. 2018. “Deep complex networks.” <em>International Conference on Learning Representations.</em>
<a href="https://arxiv.org/pdf/1705.09792.pdf">https://arxiv.org/pdf/1705.09792.pdf</a></p>

<p>Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, and Juan P. Bello. 2017. “Deep salience representations for f0 estimation in polyphonic music.” <em>International Society for Music Information Retrieval Conference</em>, Suzhou, China, pp. 23-27.
<a href="http://www.justinsalamon.com/uploads/4/3/9/4/4394963/bittner_deepsalience_ismir_2017.pdf">http://www.justinsalamon.com/uploads/4/3/9/4/4394963/bittner_deepsalience_ismir_2017.pdf</a></p>

<p>Jordi Pons and Xavier Serra. 2017. “Designing efficient architectures for modeling temporal features with convolutional neural networks.” In <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em>, pp. 2472-2476.
<a href="http://jordipons.me/media/PonsSerraICASSP2017.pdf">http://jordipons.me/media/PonsSerraICASSP2017.pdf</a></p>

<p>John Thickstun, Zaid Harchaoui, and Sham M. Kakade. 2017. “Learning features of music from scratch.” In <em>International Conference on Learning Representations</em>.
<a href="https://arxiv.org/pdf/1611.09827.pdf">https://arxiv.org/pdf/1611.09827.pdf</a></p>

<p>John Thickstun, Zaid Harchaoui, Dean P. Foster, and Sham M. Kakade. 2018. “Invariances and data augmentation for supervised music transcription.” In <em>IEEE International Conference on Acoustics, Speech, and Signal Processing.</em>
<a href="https://arxiv.org/abs/1711.04845">https://arxiv.org/abs/1711.04845</a></p>

<p>Siddharth Sigtia, Emmanouil Benetos, Nicolas Boulanger-Lewandowski, Tillman Weyde, Artur S. d’Avila Garcez, and Simon Dixon. 2015. “A hybrid recurrent neural network for music transcription.” In <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em>, pp. 2061-2065.
<a href="https://arxiv.org/pdf/1411.1623.pdf">https://arxiv.org/pdf/1411.1623.pdf</a></p>

<p>Siddharth Sigtia, Emmanouil Benetos, and Simon Dixon. 2016. “An end-to-end neural network for polyphonic piano music transcription.” <em>IEEE Transactions on Audio, Speech and Language Processing</em> 24, no. 5.
<a href="http://ieeexplore.ieee.org/abstract/document/7416164/">http://ieeexplore.ieee.org/abstract/document/7416164/</a></p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This would not be true of other genres, jazz for example. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/03/14/music/</guid>
                <description>
                    
                    Recent work on music-to-score alignment and translation-invariant networks for music transcription
                    
                </description>
                <pubDate>Wed, 14 Mar 2018 17:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Summer School Announcement</title>
                <link>http://localhost:4000/news/2018/02/27/SummerSchool/</link>
                <content:encoded>
                    <![CDATA[
                    <p>ADSI is co-organizing a summer school with the University of Wisconsin’s Institute for the Foundations of Data Science. The goal is to introduce participants to foundational concepts in modern data science. Topics will include randomized linear algebra, high-dimensional statistics, interactive machine learning, graphs and networks, continuous optimization, and deep learning. The target audience is graduate students and postdoctoral researchers with a background in statistics, computer science, mathematics or related fields. The school takes place July 24-28, 2018 at the University of Wisconsin-Madison. For more information, please visit the <a href="https://ifds.wisc.edu/workshops/fundamentals/">website</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/02/27/SummerSchool/</guid>
                <description>
                    
                    A summer school co-organized by ADSI
                    
                </description>
                <pubDate>Tue, 27 Feb 2018 16:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Upcoming West Coast Optimization Meeting</title>
                <link>http://localhost:4000/news/2018/02/27/WCOM/</link>
                <content:encoded>
                    <![CDATA[
                    <p><a href="https://sites.math.washington.edu/~ddrusv/WCOM18/index.shtml">West Coast Optimization Meeting</a>, WCOM takes place annually, 
with the Fall meeting hosted by a partner University 
in Canada and the Spring meeting hosted at University of Washington in Seattle. 
This year’s WCOM at UW will take place on May 5th. 
For up-to-date information, please see the <a href="https://sites.math.washington.edu/~ddrusv/WCOM18/index.shtml">WCOM webpage</a>, 
and please stop by if you are in the area!</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/02/27/WCOM/</guid>
                <description>
                    
                    An optimization conference at the UW this Spring
                    
                </description>
                <pubDate>Tue, 27 Feb 2018 09:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Yin Tat Lee wins 2018 NSF CAREER Award</title>
                <link>http://localhost:4000/news/2018/02/26/yintatNsfAward/</link>
                <content:encoded>
                    <![CDATA[
                    <p><a href="http://yintat.com/">Yin Tat Lee</a>, a co-PI of ADSI, was selected for the 
2018 National Science Foundation Faculty Early Career Development 
(<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1749609">CAREER</a>) program. 
He will develop faster, more efficient algorithms for solving convex and other optimization problems. 
The outcome of Lee’s research, which seeks to increase the scientific community’s 
understanding of the relationship between convex geometry and optimization algorithms 
and improve upon current techniques drawn from continuous and discrete optimization, 
will have broad impact across the sciences and beyond.</p>

<p>Recent advances yielding faster algorithms have enabled Lee to break the 
long-standing running time barriers for specific problems, 
such as <a href="https://dl.acm.org/citation.cfm?id=2707413">linear programming</a> and 
<a href="https://dl.acm.org/citation.cfm?id=2634090">maximum flow problems</a>, 
and to apply optimization techniques to a <a href="http://ieeexplore.ieee.org/document/7354442/?reload=true">broader class of problems</a>
than was previously feasible. 
Lee aims to build upon this past work by tackling a set of 
significant problems in convex geometry and optimization in order to push the state of the art even further.</p>

<p>Beyond the proposed topics, Lee has broad research interests in the applications of convexity. 
In the coming STOC 2018 (one of the top 2 conferences in theoretical computer science), 
Lee has a record number of accepted papers (6). 
Besides <a href="https://arxiv.org/abs/1711.01328">convex optimization</a>, these papers are on 
a breadth of topics including <a href="http://ads-institute.uw.edu//blog/2018/01/11/kserver/">online algorithms</a>, 
<a href="https://arxiv.org/abs/1710.06261">algorithmic convex geometry</a>, 
<a href="https://arxiv.org/abs/1712.01791">asymptotic geometric analysis</a>, 
<a href="https://arxiv.org/abs/1710.02587">operator theory</a> and <a href="https://arxiv.org/abs/1704.03864">probability</a>.</p>

<p>See also Lee’s <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1749609">award abstract</a> and 
coverage on the <a href="https://news.cs.washington.edu/2018/02/07/yin-tat-lee-wins-nsf-career-award-to-develop-new-efficient-algorithms-for-convex-optimization/">Allen School news</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2018/02/26/yintatNsfAward/</guid>
                <description>
                    
                    Yin Tat Lee selected for the 2018 National Science Foundation Faculty Early Career Development (CAREER) program
                    
                </description>
                <pubDate>Mon, 26 Feb 2018 16:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Proximal point algorithm revisited, episode 3. Catalyst acceleration</title>
                <link>http://localhost:4000/blog/2018/02/05/catalyst/</link>
                <content:encoded>
                    <![CDATA[
                    <p>This is episode 3 of the three part series that revisits the classical proximal
point algorithm. See the <a href="../../../01/24/proximal-point">first post on the subject</a> for 
introduction and notation.</p>

<h1 id="catalyst-acceleration"><a name="sec3"></a>Catalyst acceleration</h1>

<p>In the previous posts, we looked at the 
<a href="../proximal-subgrad">proximally guided subgradient method</a>
and the <a href="../prox-linear">prox-linear algorithm</a>.
The final example concerns inertial acceleration in convex optimization.
Setting the groundwork, consider a \(\mu\)-strongly convex function \(f\)
with a \(\beta\)-Lipschitz gradient map \(x\mapsto \nabla f(x)\).
Classically, gradient descent will find a point \(x\) satisfying
\(f(x)-\min f&lt;\varepsilon\) after at most</p>

\[O\left(\frac{\beta}{\mu}\ln(1/\varepsilon)\right)\]

<p>iterations.
Accelerated gradient methods, beginning with Nesterov (1983),
equip the gradient descent method with an inertial correction. Such
methods have the much lower complexity guarantee</p>

\[O\left(\sqrt{\frac{\beta}{\mu}}\ln(1/\varepsilon)\right),\]

<p>which is
optimal within the first-order oracle model of computation (Nemirovsky
and Yudin 1983).</p>

<p>It is natural to ask which other methods, aside from gradient descent,
can be “accelerated”. For example, one may wish to accelerate coordinate
descent or so-called variance reduced methods for finite sum problems; I
will comment on the latter problem class shortly.</p>

<p>One appealing strategy relies on the proximal point method. Güler (1992) 
showed that the proximal point method itself can be
equipped with inertial steps leading to improved convergence guarantees.
Building on this work, Lin, Mairal, and Harchaoui (2015; 2017) explained how to derive the <em>total</em> complexity
guarantees for an inexact accelerated proximal point method that take
into account the cost of applying an arbitrary linearly convergent
algorithm \(\mathcal{M}\) to the subproblems. Their <em>Catalyst
acceleration</em> framework is summarized below. The code for 
Catalyst is publicly available <a href="https://github.com/hongzhoulin89/Catalyst-QNing">here</a>.</p>

<h4 id="catalyst-acceleration-1">Catalyst Acceleration</h4>

<ul>
  <li>
    <p><strong>Data</strong>: \(x_0\in {\mathbb R}^d\), \(\kappa&gt;0\), algorithm
\(\mathcal{M}\)</p>
  </li>
  <li>
    <p>Set \(q= \mu/(\mu+\kappa)\), \(\alpha_0=\sqrt{q}\), and \(y_0=x_0\)</p>
  </li>
  <li>
    <p><strong>For</strong> \(t=0,\ldots,T\) <strong>do</strong></p>

    <ul>
      <li>
        <p>Use \(\mathcal{M}\) to approximately solve:</p>

\[x_t\approx \underset{x \in {\mathbb R}^d}{\operatorname{argmin}} \left\{F(x)+\frac{\kappa}{2}\|x-y_{t-1}\|^2\right\}.\;\]
      </li>
      <li>
        <p>Compute \(\alpha_t\in (0,1)\) from the equation</p>

\[\alpha_t^2=(1-\alpha_t)\alpha_{t-1}^2+q\alpha_t.\;\]
      </li>
      <li>
        <p>Compute:</p>

\[\begin{aligned}
\beta_t&amp;=\frac{\alpha_{t-1}(1-\alpha_{t-1})}{\alpha_{t-1}^2+\alpha_t},\\
y_t&amp;=x_t+\beta_t(x_t-x_{t-1}). 
\end{aligned}\]
      </li>
    </ul>
  </li>
</ul>

<p>To state the guarantees of this method, suppose that \(\mathcal{M}\)
converges on the proximal subproblem in function value at a linear rate
\(1-\tau\in (0,1)\). Then a simple termination policy on the subproblems
to solve for \(x_t\) yields an algorithm with overall
complexity</p>

\[\widetilde{O}\left(\frac{\sqrt{\mu+\kappa}}{\tau \sqrt{\mu}}\ln(1/\varepsilon)\right).\]

<p>That is, the expression
above describes the maximal number of iterations of
\(\mathcal{M}\) used by the Catalyst algorithm until it finds a point \(x\)
satisfying \(f(x)-\inf f\leq \varepsilon\). Typically \(\tau\) depends on
\(\kappa\); therefore the best choice of \(\kappa\) is the one that
minimizes the ratio \(\frac{\sqrt{\mu+\kappa}}{\tau \sqrt{\mu}}\).</p>

<p>The main motivation for the Catalyst framework, and its most potent
application, is the regularized Empirical Risk Minimization (ERM)
problem:</p>

\[\min_{x\in {\mathbb R}^d} f(x):=\frac{1}{m}\sum_{i=1}^m f_i(x)+g(x).\]

<p>Such large-finite sum problems are ubiquitous in machine learning and
high-dimensional statistics, where each function \(f_i\) typically models
a misfit between predicted and observed data while \(g\) promotes some low
dimensional structure on \(x\), such as sparsity or low-rank.</p>

<p>Assume that \(f\) is \(\mu\)-strongly convex and each individual \(f_i\) is
\(C^1\)-smooth with \(\beta\)-Lipschitz gradient. Since \(m\) is assumed to be
huge, the complexity of numerical methods is best measured in terms of
the total number of individual gradient evaluations \(\nabla f_i\). In
particular, fast gradient methods have the worst-case complexity</p>

\[O\left(m\sqrt{\frac{\beta}{\mu}}\ln(1/\varepsilon)\right),\]

<p>since
each iteration requires evaluation of all the individual gradients
\(\{\nabla f_i(x)\}_{i=1}^m\). Variance reduced algorithms, such as SAG
(Schmidt, Roux, and Bach 2013), SAGA (Defazio, Bach, and Lacoste-Julien
2014), SDCA (Shalev-Shwartz and Zhang 2012), SMART (Davis 2016), SVRG
(Johnson and Zhang 2013; Xiao and Zhang 2014), FINITO (Defazio, Domke,
and Caetano 2014), and MISO (Mairal 2015; Lin, Mairal, and Harchaoui
2015), aim to improve the dependence on \(m\). In their raw form, all of
these methods exhibit a similar complexity</p>

\[O\left(\left(m+\frac{\beta}{\mu}\right)\ln(1/\varepsilon)\right),\]

<p>in
expectation, and differ only in storage requirements and in whether one
needs to know explicitly the strong convexity constant.</p>

<p>It was a long standing open question to determine if the dependence on
\(\beta/\mu\) can be improved. This is not quite possible in full
generality, and instead one should expect a rate of the form</p>

\[O\left(\left(m+\sqrt{m\frac{\beta}{\mu}}\right)\ln(1/\varepsilon)\right).\]

<p>Indeed, such a rate would be optimal in an appropriate oracle model of
complexity (Woodworth and Srebro 2016; Arjevani 2017; Agarwal and Bottou
2015; Lan 2015). Thus acceleration for ERM problems is only beneficial
in the setting \(m&lt; \beta/\mu\).</p>

<p>Early examples for specific algorithms are the accelerated SDCA
(Shalev-Shwartz and Zhang 2015), APPA (Frostig et al. 2015), and RPDG
(Lan 2015).<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">1</a></sup> The accelerated SDCA and APPA, in particular, use a
specialized proximal-point construction.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">2</a></sup> Catalyst generic
acceleration allows to accelerate all of the variance reduced methods
above in a single conceptually transparent framework. It is worth noting
that the first direct accelerated variance reduced methods for ERM
problems were recently proposed in Allen-Zhu (2016) and Defazio (2016).</p>

<p>In contrast to the convex setting, the role of inertia for nonconvex
problems is not nearly as well understood. In particular, gradient
descent is black-box optimal for \(C^1\)-smooth nonconvex minimization
(Carmon et al. 2017b), and therefore inertia can not help in the worst
case. On the other hand, the recent paper (Carmon et al. 2017a) presents
a first-order method for minimizing \(C^2\) and \(C^3\) smooth functions
that is provably faster than gradient descent. At its core, their
algorithm also combines inertia with the proximal point method. For a
partial extension of the Catalyst framework to weakly convex problems,
see Paquette et al. (2017).</p>

<h1 id="conclusion">Conclusion</h1>

<p>The proximal point method has long been ingrained in the foundations of
optimization. Recent progress in large scale computing has shown that
the proximal point method is not only conceptual, but can guide
methodology. Though direct methods are usually preferable, proximally
guided algorithms can be equally effective and often lead to more easily
interpretable numerical methods. In this blog, I outlined three examples
of this viewpoint, where the proximal-point method guides both the
design and analysis of numerical methods.</p>

<h4 id="acknowledgements">Acknowledgements</h4>
<p>The author thanks Damek Davis, John Duchi, and Zaid Harchaoui for their
helpful comments on an early draft.</p>

<h1 id="references">References<a name="ref"></a></h1>
<p>Abbe, E., A.S. Bandeira, A. Bracher, and A. Singer. 2014. “Decoding
Binary Node Labels from Censored Edge Measurements: Phase Transition and
Efficient Recovery.” <em>IEEE Trans. Network Sci. Eng.</em> 1 (1):10–22.
<a href="https://doi.org/10.1109/TNSE.2014.2368716">https://doi.org/10.1109/TNSE.2014.2368716</a>.</p>

<p>Agarwal, A., and L. Bottou. 2015. “A Lower Bound for the Optimization of
Finite Sums.” In <em>Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>, 78–86.
<a href="http://leon.bottou.org/papers/agarwal-bottou-2015">http://leon.bottou.org/papers/agarwal-bottou-2015</a>.</p>

<p>Allen-Zhu, Z. 2016. “Katyusha: The First Direct Acceleration of
Stochastic Gradient Methods.” <em>Preprint arXiv:1603.05953 (Version 5)</em>.</p>

<p>Arjevani, Y. 2017. “Limitations on Variance-Reduction and Acceleration
Schemes for Finite Sums Optimization.” In <em>Advances in Neural
Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
3543–52. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf">http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf</a>.</p>

<p>Bandeira, A.S., N. Boumal, and V. Voroninski. 2016. “On the Low-Rank
Approach for Semidefinite Programs Arising in Synchronization and
Community Detection.” In <em>Proceedings of the 29th Conference on Learning
Theory, COLT 2016, New York, Usa, June 23-26, 2016</em>, 361–82.
<a href="http://jmlr.org/proceedings/papers/v49/bandeira16.html">http://jmlr.org/proceedings/papers/v49/bandeira16.html</a>.</p>

<p>Bartlett, P.L., M.I. Jordan, and J.D. McAuliffe. 2006. “Convexity,
Classification, and Risk Bounds.” <em>J. Amer. Statist. Assoc.</em> 101
(473):138–56.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907">https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907</a>.</p>

<p>Beck, A., and M. Teboulle. 2012. “Smoothing and First Order Methods: A
Unified Framework.” <em>SIAM J. Optim.</em> 22 (2):557–80.
<a href="https://doi.org/10.1137/100818327">https://doi.org/10.1137/100818327</a>.</p>

<p>Bottou, L., and O. Bousquet. 2008. “The Tradeoffs of Large Scale
Learning.” In <em>Advances in Neural Information Processing Systems</em>,
161–68. <a href="http://leon.bottou.org/publications/pdf/nips-2007.pdf">http://leon.bottou.org/publications/pdf/nips-2007.pdf</a>.</p>

<p>Burke, J.V., and M.C. Ferris. 1995. “A Gauss-Newton Method for Convex
Composite Optimization.” <em>Math. Programming</em> 71 (2, Ser. A):179–94.
<a href="https://doi.org/10.1007/BF01585997">https://doi.org/10.1007/BF01585997</a>.</p>

<p>Candès, E.J., X. Li, Y. Ma, and J. Wright. 2011. “Robust Principal
Component Analysis?” <em>J. ACM</em> 58 (3):Art. 11, 37.
<a href="https://doi.org/10.1145/1970392.1970395">https://doi.org/10.1145/1970392.1970395</a>.</p>

<p>Candès, E.J., X. Li, and M. Soltanolkotabi. 2015. “Phase Retrieval via
Wirtinger Flow: Theory and Algorithms.” <em>IEEE Trans. Inform. Theory</em> 61
(4):1985–2007. <a href="https://doi.org/10.1109/TIT.2015.2399924">https://doi.org/10.1109/TIT.2015.2399924</a>.</p>

<p>Carmon, Y., J.C. Duchi, O. Hinder, and A. Sidford. 2017a. “‘Convex Until
Proven Guilty’: Dimension-Free Acceleration of Gradient Descent on
Non-Convex Functions.” In <em>Proceedings of the 34th International
Conference on Machine Learning</em>, 70:654–63.</p>

<p>Y. Carmon, J.C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I.
<em>Preprint arXiv:1710.11606</em>.</p>

<p>Cartis, C., N.I.M. Gould, and P.L. Toint. 2011. “On the Evaluation
Complexity of Composite Function Minimization with Applications to
Nonconvex Nonlinear Programming.” <em>SIAM J. Optim.</em> 21 (4):1721–39.
<a href="https://doi.org/10.1137/11082381X">https://doi.org/10.1137/11082381X</a>.</p>

<p>Chambolle, A., and T. Pock. 2011. “A First-Order Primal-Dual Algorithm
for Convex Problems with Applications to Imaging.” <em>J. Math. Imaging
Vision</em> 40 (1):120–45. <a href="https://doi.org/10.1007/s10851-010-0251-1">https://doi.org/10.1007/s10851-010-0251-1</a>.</p>

<p>Chandrasekaran, V., S. Sanghavi, P. A. Parrilo, and A.S. Willsky. 2011.
“Rank-Sparsity Incoherence for Matrix Decomposition.” <em>SIAM J. Optim.</em>
21 (2):572–96. <a href="https://doi.org/10.1137/090761793">https://doi.org/10.1137/090761793</a>.</p>

<p>Chen, Y., and E.J. Candès. 2017. “Solving Random Quadratic Systems of
Equations Is Nearly as Easy as Solving Linear Systems.” <em>Comm. Pure
Appl. Math.</em> 70 (5):822–83.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638">https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638</a>.</p>

<p>Davis, D. 2016. “SMART: The Stochastic Monotone Aggregated Root-Finding
Algorithm.” <em>Preprint arXiv:1601.00698</em>.</p>

<p>Davis, D., D. Drusvyatskiy, and C. Paquette. 2017. “The Nonsmooth
Landscape of Phase Retrieval.” <em>Preprint arXiv:1711.03247</em>.</p>

<p>Davis, D., and B. Grimmer. 2017. “Proximally Guided Stochastic
Sbgradient Method for Nonsmooth, Nonconvex Problems.” <em>Preprint,
arXiv:1707.03505</em>.</p>

<p>Defazio, A. 2016. “A Simple Practical Accelerated Method for Finite
Sums.” In <em>Advances in Neural Information Processing Systems 29</em>, edited
by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
676–84. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf">http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf</a>.</p>

<p>Defazio, A., F. Bach, and S. Lacoste-Julien. 2014. “SAGA: A Fast
Incremental Gradient Method with Support for Non-Strongly Convex
Composite Objectives.” In <em>Advances in Neural Information Processing
Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, 1646–54. Curran Associates, Inc.</p>

<p>Defazio, A., J. Domke, and T.S. Caetano. 2014. “Finito: A Faster,
Permutable Incremental Gradient Method for Big Data Problems.” In
<em>ICML</em>, 1125–33.</p>

<p>Drusvyatskiy, D., and A.S. Lewis. 2013. “Tilt Stability, Uniform
Quadratic Growth, and Strong Metric Regularity of the Subdifferential.”
<em>SIAM J. Optim.</em> 23 (1):256–67. <a href="https://doi.org/10.1137/120876551">https://doi.org/10.1137/120876551</a>.</p>

<p>D. Drusvyatskiy and A.S. Lewis. 2016. “Error Bounds, Quadratic Growth, and Linear Convergence
of Proximal Methods.” <em>To Appear in Math. Oper. Res., arXiv:1602.06661</em>.</p>

<p>Drusvyatskiy, D., B.S. Mordukhovich, and T.T.A. Nghia. 2014.
“Second-Order Growth, Tilt-Stability, and Metric Regularity of the
Subdifferential.” <em>J. Convex Anal.</em> 21 (4):1165–92.</p>

<p>Drusvyatskiy, D., and C. Paquette. 2016. “Efficiency of Minimizing
Compositions of Convex Functions and Smooth Maps.” <em>Preprint,
arXiv:1605.00125</em>.</p>

<p>Duchi, J.C., and F. Ruan. 2017a. “Solving (Most) of a Set of Quadratic
Equalities: Composite Optimization for Robust Phase Retrieval.”
<em>Preprint arXiv:1705.02356</em>.</p>

<p>J.C. Duchi and F. Ruan. 2017b. “Stochastic Methods for Composite Optimization
Problems.” <em>Preprint arXiv:1703.08570</em>.</p>

<p>Eldar, Y.C., and S. Mendelson. 2014. “Phase Retrieval: Stability and
Recovery Guarantees.” <em>Appl. Comput. Harmon. Anal.</em> 36 (3):473–94.
<a href="https://doi.org/10.1016/j.acha.2013.08.003">https://doi.org/10.1016/j.acha.2013.08.003</a>.</p>

<p>Frostig, R., R. Ge, S.M. Kakade, and A. Sidford. 2015. “Un-Regularizing:
Approximate Proximal Point and Faster Stochastic Algorithms for
Empirical Risk Minimization.” In <em>Proceedings of the 32nd International
Conference on Machine Learning (ICML)</em>.</p>

<p>Ghadimi, S., and G. Lan. 2013. “Stochastic First- and Zeroth-Order
Methods for Nonconvex Stochastic Programming.” <em>SIAM J. Optim.</em> 23
(4):2341–68. <a href="https://doi.org/10.1137/120880811">https://doi.org/10.1137/120880811</a>.</p>

<p>Güler, O. 1992. “New Proximal Point Algorithms for Convex Minimization.”
<em>SIAM J. Optim.</em> 2 (4):649–64.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/0802032">https://doi-org.offcampus.lib.washington.edu/10.1137/0802032</a>.</p>

<p>Hazan, E., and S. Kale. 2011. “Beyond the Regret Minimization Barrier:
An Optimal Algorithm for Stochastic Strongly-Convex Optimization.” In
<em>Proceedings of the 24th Annual Conference on Learning Theory</em>, edited
by Sham M. Kakade and Ulrike von Luxburg, 19:421–36. Proceedings of
Machine Learning Research. Budapest, Hungary: PMLR.</p>

<p>Johnson, R., and T. Zhang. 2013. “Accelerating Stochastic Gradient
Descent Using Predictive Variance Reduction.” In <em>Proceedings of the
26th International Conference on Neural Information Processing Systems</em>,
315–23. NIPS’13. USA: Curran Associates Inc.
<a href="http://dl.acm.org/citation.cfm?id=2999611.2999647">http://dl.acm.org/citation.cfm?id=2999611.2999647</a>.</p>

<p>Juditsky, A., and Y. Nesterov. 2014. “Deterministic and Stochastic
Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization.”
<em>Stoch. Syst.</em> 4 (1):44–80.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010">https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010</a>.</p>

<p>Lacoste-Julien, S., M. Schmidt, and F. Bach. 2012. “A Simpler Approach
to Obtaining an \({O}(1/t)\) Convergence Rate for the Projected Stochastic
Subgradient Method.” <em>Arxiv arXiv:1212.2002</em>.</p>

<p>Lan, G. 2015. “An Optimal Randomized Incremental Gradient Method.”
<em>arXiv:1507.02000</em>.</p>

<p>Lemarechal, C., J.-J. Strodiot, and A. Bihain. 1981. “On a Bundle
Algorithm for Nonsmooth Optimization.” In <em>Nonlinear Programming, 4
(Madison, Wis., 1980)</em>, 245–82. Academic Press, New York-London.</p>

<p>Lewis, A.S., and S.J. Wright. 2015. “A Proximal Method for Composite
Minimization.” <em>Math. Program.</em> Springer Berlin Heidelberg, 1–46.
<a href="https://doi.org/10.1007/s10107-015-0943-9">https://doi.org/10.1007/s10107-015-0943-9</a>.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. 2015. “A Universal Catalyst for
First-Order Optimization.” In <em>Advances in Neural Information Processing
Systems</em>, 3366–74.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. “Catalyst Acceleration for 
First-order Convex Optimization: 
from Theory to Practice.” <em>arXiv preprint arXiv:1712.05654</em> (2017).</p>

<p>Luke, R. 2017. “Phase Retrieval, What’s New?” <em>SIAG/OPT Views and News</em>
25 (1).</p>

<p>Mairal, J. 2015. “Incremental Majorization-Minimization Optimization
with Application to Large-Scale Machine Learning.” <em>SIAM Journal on
Optimization</em> 25 (2):829–55.</p>

<p>Nemirovski, A. 2004. “Prox-Method with Rate of Convergence \(O(1/t)\) for
Variational Inequalities with Lipschitz Continuous Monotone Operators
and Smooth Convex-Concave Saddle Point Problems.” <em>SIAM J. Optim.</em> 15
(1):229–51. <a href="https://doi.org/10.1137/S1052623403425629">https://doi.org/10.1137/S1052623403425629</a>.</p>

<p>Nemirovski, A., A. Juditsky, G. Lan, and A. Shapiro. 2008. “Robust
Stochastic Approximation Approach to Stochastic Programming.” <em>SIAM J.
Optim.</em> 19 (4):1574–1609.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/070704277">https://doi-org.offcampus.lib.washington.edu/10.1137/070704277</a>.</p>

<p>Nemirovsky, A.S., and D.B. Yudin. 1983. <em>Problem Complexity and Method
Efficiency in Optimization</em>. A Wiley-Interscience Publication. John
Wiley &amp; Sons, Inc., New York.</p>

<p>Nesterov, Y., and A. Nemirovskii. 1994. <em>Interior-Point Polynomial
Algorithms in Convex Programming</em>. Vol. 13. SIAM Studies in Applied
Mathematics. Society for Industrial; Applied Mathematics (SIAM),
Philadelphia, PA. <a href="https://doi.org/10.1137/1.9781611970791">https://doi.org/10.1137/1.9781611970791</a>.</p>

<p>Nesterov, Yu. 1983. “A Method for Solving the Convex Programming Problem
with Convergence Rate \(O(1/k^{2})\).” <em>Dokl. Akad. Nauk SSSR</em> 269
(3):543–47.</p>

<p>Nesterov, Yu. 2005. “Smooth Minimization of Non-Smooth Functions.” <em>Math.
Program.</em> 103 (1, Ser. A):127–52.
<a href="https://doi.org/10.1007/s10107-004-0552-5">https://doi.org/10.1007/s10107-004-0552-5</a>.</p>

<p>Nesterov, Yu. 2007. “Modified Gauss-Newton Scheme with Worst Case
Guarantees for Global Performance.” <em>Optim. Methods Softw.</em> 22
(3):469–83. <a href="https://doi.org/10.1080/08927020600643812">https://doi.org/10.1080/08927020600643812</a>.</p>

<p>Nesterov, Yu. 2013. “Gradient Methods for Minimizing Composite Functions.”
<em>Math. Program.</em> 140 (1, Ser. B):125–61.
<a href="https://doi.org/10.1007/s10107-012-0629-5">https://doi.org/10.1007/s10107-012-0629-5</a>.</p>

<p>Paquette, C., H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. 2017. 
“Catalyst Acceleration for Gradient-Based Non-Convex
Optimization.” <em>Preprint arXiv:1703.10993</em>.</p>

<p>Polyak, B.T., and A.B. Juditsky. 1992. “Acceleration of Stochastic
Approximation by Averaging.” <em>SIAM J. Control Optim.</em> 30 (4):838–55.
<a href="https://doi.org/10.1137/0330046">https://doi.org/10.1137/0330046</a>.</p>

<p>Rakhlin, A., O. Shamir, and K. Sridharan. 2012. “Making Gradient Descent
Optimal for Strongly Convex Stochastic Optimization.” In <em>Proceedings of
the 29th International Coference on International Conference on Machine
Learning</em>, 1571–8. ICML’12. USA: Omnipress.
<a href="http://dl.acm.org/citation.cfm?id=3042573.3042774">http://dl.acm.org/citation.cfm?id=3042573.3042774</a>.</p>

<p>Robbins, H., and S. Monro. 1951. “A Stochastic Approximation Method.”
<em>Ann. Math. Statistics</em> 22:400–407.</p>

<p>Schmidt, M., N. Le Roux, and F. Bach. 2013. “Minimizing Finite Sums with
the Stochastic Average Gradient.” <em>arXiv:1309.2388</em>.</p>

<p>Shalev-Shwartz, S., and T. Zhang. 2012. “Proximal Stochastic Dual
Coordinate Ascent.” <em>arXiv:1211.2717</em>.</p>

<p>S. Shalev-Shwartz and T. Zhang. 2015. “Accelerated Proximal Stochastic Dual Coordinate Ascent
for Regularized Loss Minimization.” <em>Mathematical Programming</em>.</p>

<p>Singer, A. 2011. “Angular Synchronization by Eigenvectors and
Semidefinite Programming.” <em>Appl. Comput. Harmon. Anal.</em> 30 (1):20–36.
<a href="https://doi.org/10.1016/j.acha.2010.02.001">https://doi.org/10.1016/j.acha.2010.02.001</a>.</p>

<p>Sun, J., Q. Qu, and J. Wright. 2017. “A Geometric Analysis of Phase
Retrieval.” <em>To Appear in Found. Comp. Math., arXiv:1602.06664</em>.</p>

<p>Woodworth, B.E., and N. Srebro. 2016. “Tight Complexity Bounds for
Optimizing Composite Objectives.” In <em>Advances in Neural Information
Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, 3639–47. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf">http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf</a>.</p>

<p>Wright, S.J. 1997. <em>Primal-Dual Interior-Point Methods</em>. Society for
Industrial; Applied Mathematics (SIAM), Philadelphia, PA.
<a href="https://doi.org/10.1137/1.9781611971453">https://doi.org/10.1137/1.9781611971453</a>.</p>

<p>Xiao, L., and T. Zhang. 2014. “A Proximal Stochastic Gradient Method
with Progressive Variance Reduction.” <em>SIAM J. Optim.</em> 24 (4):2057–75.
<a href="https://doi.org/10.1137/140961791">https://doi.org/10.1137/140961791</a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:4" role="doc-endnote">
      <p>Here, I am ignoring logarithmic terms in the convergence rate. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>The accelerated SDCA was the motivation for the Catalyst
framework, while APPA appeared concurrently with Catalyst. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/02/05/catalyst/</guid>
                <description>
                    
                    Revisiting the proximal point method, and catalyst generic acceleration for regularized Empirical Risk Minimization.
                    
                </description>
                <pubDate>Mon, 05 Feb 2018 16:03:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Proximal point algorithm revisited, episode 2. The prox-linear algorithm</title>
                <link>http://localhost:4000/blog/2018/01/30/prox-linear/</link>
                <content:encoded>
                    <![CDATA[
                    <p>This is episode 2 of the three-part series that revisits the classical proximal
point algorithm. See the <a href="../../24/proximal-point/">first post on this topic</a>
for an introduction and notation.</p>

<h1 id="the-prox-linear-algorithm"><a name="sec2"></a>The prox-linear algorithm</h1>

<p>For well-structured weakly convex problems, one can hope for faster
numerical methods than the subgradient scheme. In this episode, I will
focus on the composite problem class \(\mathcal{C}\).
To simplify the exposition, I will assume \(L=1\), which can always be
arranged by rescaling.</p>

<p>Since composite functions are weakly convex, one could apply the
proximal point method directly, while setting the parameter
\(\nu\leq\beta^{-1}\). Even though the proximal subproblems are strongly
convex, they are not in a form that is most amenable to convex
optimization techniques. Indeed, most convex optimization algorithms are
designed for minimizing a sum of a convex function and a composition of
a convex function with a <em>linear</em> map. This observation suggests
introducing the following modification to the proximal-point algorithm.
Given a current iterate \(x_t\), the <em>prox-linear method</em> sets</p>

\[\begin{aligned}
x_{t+1}=\underset{x}{\operatorname{argmin}} \{F(x;x_t)+\tfrac{\beta}{2}\|x-x_t\|^2\},
\end{aligned}\]

<p>where \(F(x;y)\) is the local convex model</p>

\[F(x;y):=g(x)+h\left(c(y)+\nabla c(y)(x-y)\right).\]

<p>In other words,
each proximal subproblem is approximated by linearizing the smooth map
\(c\) at the current iterate \(x_t\).</p>

<p>The main advantage is that each subproblem is now a sum of a strongly
convex function and a composition of a Lipschitz convex function with a
linear map. A variety of methods utilizing this structure can be
formally applied; e.g. smoothing (Nesterov 2005), saddle-point
(Nemirovski 2004; Chambolle and Pock 2011), and interior point
algorithms (Nesterov and Nemirovskii 1994; Wright 1997). Which of these
methods is practical depends on the specifics of the problem, such as
the size and the cost of vector-matrix multiplications.</p>

<p>It is instructive to note that in the simplest setting of additive
composite problems
(Example 1), the prox-linear method reduces to the
popular proximal-gradient algorithm or ISTA (Beck and Teboulle 2012).
For nonlinear least squares, the prox-linear method is a close variant
of Gauss-Newton.</p>

<p>Recall that the step-size of the proximal point method provides a
convenient stopping criteria, since it directly relates to the gradient
of the Moreau envelope – a smooth approximation of the objective
function. Is there such an interpretation for the prox-linear method?
This question is central, since termination criteria is not only used to
stop the method but also to judge its efficiency and to compare against
competing methods.</p>

<p>The answer is yes. Even though one can not evaluate the gradient
\(\|\nabla F_{\frac{1}{2\beta}}\|\) directly, the scaled step-size of the
prox-linear method</p>

\[\mathcal{G}(x):=\beta(x_{t+1}-x_t)\]

<p>is a good
surrogate (Drusvyatskiy and Paquette 2016 Theorem 4.5):</p>

\[\tfrac{1}{4} \|\nabla F_{\frac{1}{2\beta}}(x)\| \leq \|\mathcal{G}(x)\|\leq 3\|\nabla F_{\frac{1}{2\beta}}(x)\|.\]

<p>In particular, the prox-linear method will find a point \(x\) satisfying
\(\|\nabla F_{\frac{1}{2\beta}}(x)\|^2\leq\varepsilon\) after at most
\(O\left(\frac{\beta(F(x_0)-\inf F)}{\varepsilon}\right)\) iterations. In
the simplest setting when \(g=0\) and \(h(t)=t\), this rate reduces to the
well-known convergence guarantee of gradient descent, which is black-box
optimal for \(C^1\)-smooth nonconvex optimization (Carmon et al. 2017b).</p>

<p>It is worthwhile to note that a number of improvements to the basic
prox-linear method were recently proposed. Cartis,
Gould, and Toint (2011) discuss trust region variants and their
complexity guarantees, while Duchi and Ruan (2017b) propose stochastic
extensions of the scheme and prove almost sure convergence.
Drusvyatskiy and Paquette (2016) discuss overall complexity guarantees
when the convex subproblems can only be solved by first-order methods,
and proposes an inertial variant of the scheme whose convergence
guarantees automatically adapt to the near-convexity of the problem.</p>

<h2 id="local-rapid-convergence">Local rapid convergence</h2>

<p>Under typical regularity conditions, the prox-linear method exhibits the
same types of rapid convergence guarantees as the proximal point method.
I will illustrate with two intuitive and widely used regularity
conditions, yielding local linear and quadratic convergence,
respectively.</p>

<p>A local minimizer \(\bar x\) of \(F\) is <em>\(\alpha\)-tilt-stable</em> if there
exists \(r&gt;0\) such that the solution map</p>

\[M: v\mapsto \underset{x\in B_r(\bar x)}{\operatorname{argmin}} \left\{ F(x)-\langle v,x \rangle\right\}\]

<p>is \(1/\alpha\)-Lipschitz around \(0\) with \(M(0)=\bar x\).</p>

<p>This condition might seem unfamiliar to convex optimization specialist.
Though not obvious, tilt-stability is equivalent to a uniform quadratic
growth property and a subtle localization of strong convexity of \(F\).
See Drusvyatskiy and Lewis (2013) or Drusvyatskiy, Mordukhovich, and
Nghia (2014) for more details on these equivalences. Under the
tilt-stability assumption, the prox-linear method initialized
sufficiently close to \(\bar x\) produces iterates that converge at a
linear rate \(1-\alpha/\beta\).</p>

<p>The second regularity condition models sharp growth of the function
around the minimizer. Let \(S\) be the set of all stationary points of
\(F\), meaning \(x\) lies in \(S\) if and only if the directional derivative
\(F'(x;v)\) is nonnegative in every direction \(v\in {\mathbb R}^d\).</p>

<p>A local minimizer \(\bar x\) of \(F\) is <em>sharp</em> if there exists \(\alpha&gt;0\)
and a neighborhood \(\mathcal{X}\) of \(\bar x\) such that</p>

\[F(x)\geq  F({\rm proj}_S(x))+c\cdot {\rm dist}(x,S)\qquad\forall x\in \mathcal{X}.\]

<p>Under the sharpness condition, the prox-linear method initialized
sufficiently close to \(\bar x\) produces iterates that converge
quadratically.</p>

<p>For well-structured problems, one can hope to justify the two regularity
conditions above under statistical assumptions. The recent work of Duchi
and Ruan (2017a) on the phase retrieval problem is an
interesting recent example. Under mild statistical assumptions on the
data generating mechanism, sharpness is assured with high probability.
Therefore the prox-linear method (and even subgradient methods (Davis,
Drusvyatskiy, and Paquette 2017)) converge rapidly, when initialized
within a constant relative distance of an optimal solution.</p>

<h1 id="references">References</h1>
<p>Abbe, E., A.S. Bandeira, A. Bracher, and A. Singer. 2014. “Decoding
Binary Node Labels from Censored Edge Measurements: Phase Transition and
Efficient Recovery.” <em>IEEE Trans. Network Sci. Eng.</em> 1 (1):10–22.
<a href="https://doi.org/10.1109/TNSE.2014.2368716">https://doi.org/10.1109/TNSE.2014.2368716</a>.</p>

<p>Agarwal, A., and L. Bottou. 2015. “A Lower Bound for the Optimization of
Finite Sums.” In <em>Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>, 78–86.
<a href="http://leon.bottou.org/papers/agarwal-bottou-2015">http://leon.bottou.org/papers/agarwal-bottou-2015</a>.</p>

<p>Allen-Zhu, Z. 2016. “Katyusha: The First Direct Acceleration of
Stochastic Gradient Methods.” <em>Preprint arXiv:1603.05953 (Version 5)</em>.</p>

<p>Arjevani, Y. 2017. “Limitations on Variance-Reduction and Acceleration
Schemes for Finite Sums Optimization.” In <em>Advances in Neural
Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
3543–52. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf">http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf</a>.</p>

<p>Bandeira, A.S., N. Boumal, and V. Voroninski. 2016. “On the Low-Rank
Approach for Semidefinite Programs Arising in Synchronization and
Community Detection.” In <em>Proceedings of the 29th Conference on Learning
Theory, COLT 2016, New York, Usa, June 23-26, 2016</em>, 361–82.
<a href="http://jmlr.org/proceedings/papers/v49/bandeira16.html">http://jmlr.org/proceedings/papers/v49/bandeira16.html</a>.</p>

<p>Bartlett, P.L., M.I. Jordan, and J.D. McAuliffe. 2006. “Convexity,
Classification, and Risk Bounds.” <em>J. Amer. Statist. Assoc.</em> 101
(473):138–56.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907">https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907</a>.</p>

<p>Beck, A., and M. Teboulle. 2012. “Smoothing and First Order Methods: A
Unified Framework.” <em>SIAM J. Optim.</em> 22 (2):557–80.
<a href="https://doi.org/10.1137/100818327">https://doi.org/10.1137/100818327</a>.</p>

<p>Bottou, L., and O. Bousquet. 2008. “The Tradeoffs of Large Scale
Learning.” In <em>Advances in Neural Information Processing Systems</em>,
161–68. <a href="http://leon.bottou.org/publications/pdf/nips-2007.pdf">http://leon.bottou.org/publications/pdf/nips-2007.pdf</a>.</p>

<p>Burke, J.V., and M.C. Ferris. 1995. “A Gauss-Newton Method for Convex
Composite Optimization.” <em>Math. Programming</em> 71 (2, Ser. A):179–94.
<a href="https://doi.org/10.1007/BF01585997">https://doi.org/10.1007/BF01585997</a>.</p>

<p>Candès, E.J., X. Li, Y. Ma, and J. Wright. 2011. “Robust Principal
Component Analysis?” <em>J. ACM</em> 58 (3):Art. 11, 37.
<a href="https://doi.org/10.1145/1970392.1970395">https://doi.org/10.1145/1970392.1970395</a>.</p>

<p>Candès, E.J., X. Li, and M. Soltanolkotabi. 2015. “Phase Retrieval via
Wirtinger Flow: Theory and Algorithms.” <em>IEEE Trans. Inform. Theory</em> 61
(4):1985–2007. <a href="https://doi.org/10.1109/TIT.2015.2399924">https://doi.org/10.1109/TIT.2015.2399924</a>.</p>

<p>Carmon, Y., J.C. Duchi, O. Hinder, and A. Sidford. 2017a. “‘Convex Until
Proven Guilty’: Dimension-Free Acceleration of Gradient Descent on
Non-Convex Functions.” In <em>Proceedings of the 34th International
Conference on Machine Learning</em>, 70:654–63.</p>

<p>Y. Carmon, J.C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I.
<em>Preprint arXiv:1710.11606</em>.</p>

<p>Cartis, C., N.I.M. Gould, and P.L. Toint. 2011. “On the Evaluation
Complexity of Composite Function Minimization with Applications to
Nonconvex Nonlinear Programming.” <em>SIAM J. Optim.</em> 21 (4):1721–39.
<a href="https://doi.org/10.1137/11082381X">https://doi.org/10.1137/11082381X</a>.</p>

<p>Chambolle, A., and T. Pock. 2011. “A First-Order Primal-Dual Algorithm
for Convex Problems with Applications to Imaging.” <em>J. Math. Imaging
Vision</em> 40 (1):120–45. <a href="https://doi.org/10.1007/s10851-010-0251-1">https://doi.org/10.1007/s10851-010-0251-1</a>.</p>

<p>Chandrasekaran, V., S. Sanghavi, P. A. Parrilo, and A.S. Willsky. 2011.
“Rank-Sparsity Incoherence for Matrix Decomposition.” <em>SIAM J. Optim.</em>
21 (2):572–96. <a href="https://doi.org/10.1137/090761793">https://doi.org/10.1137/090761793</a>.</p>

<p>Chen, Y., and E.J. Candès. 2017. “Solving Random Quadratic Systems of
Equations Is Nearly as Easy as Solving Linear Systems.” <em>Comm. Pure
Appl. Math.</em> 70 (5):822–83.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638">https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638</a>.</p>

<p>Davis, D. 2016. “SMART: The Stochastic Monotone Aggregated Root-Finding
Algorithm.” <em>Preprint arXiv:1601.00698</em>.</p>

<p>Davis, D., D. Drusvyatskiy, and C. Paquette. 2017. “The Nonsmooth
Landscape of Phase Retrieval.” <em>Preprint arXiv:1711.03247</em>.</p>

<p>Davis, D., and B. Grimmer. 2017. “Proximally Guided Stochastic
Sbgradient Method for Nonsmooth, Nonconvex Problems.” <em>Preprint,
arXiv:1707.03505</em>.</p>

<p>Defazio, A. 2016. “A Simple Practical Accelerated Method for Finite
Sums.” In <em>Advances in Neural Information Processing Systems 29</em>, edited
by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
676–84. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf">http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf</a>.</p>

<p>Defazio, A., F. Bach, and S. Lacoste-Julien. 2014. “SAGA: A Fast
Incremental Gradient Method with Support for Non-Strongly Convex
Composite Objectives.” In <em>Advances in Neural Information Processing
Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, 1646–54. Curran Associates, Inc.</p>

<p>Defazio, A., J. Domke, and T.S. Caetano. 2014. “Finito: A Faster,
Permutable Incremental Gradient Method for Big Data Problems.” In
<em>ICML</em>, 1125–33.</p>

<p>Drusvyatskiy, D., and A.S. Lewis. 2013. “Tilt Stability, Uniform
Quadratic Growth, and Strong Metric Regularity of the Subdifferential.”
<em>SIAM J. Optim.</em> 23 (1):256–67. <a href="https://doi.org/10.1137/120876551">https://doi.org/10.1137/120876551</a>.</p>

<p>D. Drusvyatskiy and A.S. Lewis. 2016. “Error Bounds, Quadratic Growth, and Linear Convergence
of Proximal Methods.” <em>To Appear in Math. Oper. Res., arXiv:1602.06661</em>.</p>

<p>Drusvyatskiy, D., B.S. Mordukhovich, and T.T.A. Nghia. 2014.
“Second-Order Growth, Tilt-Stability, and Metric Regularity of the
Subdifferential.” <em>J. Convex Anal.</em> 21 (4):1165–92.</p>

<p>Drusvyatskiy, D., and C. Paquette. 2016. “Efficiency of Minimizing
Compositions of Convex Functions and Smooth Maps.” <em>Preprint,
arXiv:1605.00125</em>.</p>

<p>Duchi, J.C., and F. Ruan. 2017a. “Solving (Most) of a Set of Quadratic
Equalities: Composite Optimization for Robust Phase Retrieval.”
<em>Preprint arXiv:1705.02356</em>.</p>

<p>J.C. Duchi and F. Ruan. 2017b. “Stochastic Methods for Composite Optimization
Problems.” <em>Preprint arXiv:1703.08570</em>.</p>

<p>Eldar, Y.C., and S. Mendelson. 2014. “Phase Retrieval: Stability and
Recovery Guarantees.” <em>Appl. Comput. Harmon. Anal.</em> 36 (3):473–94.
<a href="https://doi.org/10.1016/j.acha.2013.08.003">https://doi.org/10.1016/j.acha.2013.08.003</a>.</p>

<p>Frostig, R., R. Ge, S.M. Kakade, and A. Sidford. 2015. “Un-Regularizing:
Approximate Proximal Point and Faster Stochastic Algorithms for
Empirical Risk Minimization.” In <em>Proceedings of the 32nd International
Conference on Machine Learning (ICML)</em>.</p>

<p>Ghadimi, S., and G. Lan. 2013. “Stochastic First- and Zeroth-Order
Methods for Nonconvex Stochastic Programming.” <em>SIAM J. Optim.</em> 23
(4):2341–68. <a href="https://doi.org/10.1137/120880811">https://doi.org/10.1137/120880811</a>.</p>

<p>Güler, O. 1992. “New Proximal Point Algorithms for Convex Minimization.”
<em>SIAM J. Optim.</em> 2 (4):649–64.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/0802032">https://doi-org.offcampus.lib.washington.edu/10.1137/0802032</a>.</p>

<p>Hazan, E., and S. Kale. 2011. “Beyond the Regret Minimization Barrier:
An Optimal Algorithm for Stochastic Strongly-Convex Optimization.” In
<em>Proceedings of the 24th Annual Conference on Learning Theory</em>, edited
by Sham M. Kakade and Ulrike von Luxburg, 19:421–36. Proceedings of
Machine Learning Research. Budapest, Hungary: PMLR.</p>

<p>Johnson, R., and T. Zhang. 2013. “Accelerating Stochastic Gradient
Descent Using Predictive Variance Reduction.” In <em>Proceedings of the
26th International Conference on Neural Information Processing Systems</em>,
315–23. NIPS’13. USA: Curran Associates Inc.
<a href="http://dl.acm.org/citation.cfm?id=2999611.2999647">http://dl.acm.org/citation.cfm?id=2999611.2999647</a>.</p>

<p>Juditsky, A., and Y. Nesterov. 2014. “Deterministic and Stochastic
Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization.”
<em>Stoch. Syst.</em> 4 (1):44–80.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010">https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010</a>.</p>

<p>Lacoste-Julien, S., M. Schmidt, and F. Bach. 2012. “A Simpler Approach
to Obtaining an \({O}(1/t)\) Convergence Rate for the Projected Stochastic
Subgradient Method.” <em>Arxiv arXiv:1212.2002</em>.</p>

<p>Lan, G. 2015. “An Optimal Randomized Incremental Gradient Method.”
<em>arXiv:1507.02000</em>.</p>

<p>Lemarechal, C., J.-J. Strodiot, and A. Bihain. 1981. “On a Bundle
Algorithm for Nonsmooth Optimization.” In <em>Nonlinear Programming, 4
(Madison, Wis., 1980)</em>, 245–82. Academic Press, New York-London.</p>

<p>Lewis, A.S., and S.J. Wright. 2015. “A Proximal Method for Composite
Minimization.” <em>Math. Program.</em> Springer Berlin Heidelberg, 1–46.
<a href="https://doi.org/10.1007/s10107-015-0943-9">https://doi.org/10.1007/s10107-015-0943-9</a>.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. 2015. “A Universal Catalyst for
First-Order Optimization.” In <em>Advances in Neural Information Processing
Systems</em>, 3366–74.</p>

<p>Luke, R. 2017. “Phase Retrieval, What’s New?” <em>SIAG/OPT Views and News</em>
25 (1).</p>

<p>Mairal, J. 2015. “Incremental Majorization-Minimization Optimization
with Application to Large-Scale Machine Learning.” <em>SIAM Journal on
Optimization</em> 25 (2):829–55.</p>

<p>Nemirovski, A. 2004. “Prox-Method with Rate of Convergence \(O(1/t)\) for
Variational Inequalities with Lipschitz Continuous Monotone Operators
and Smooth Convex-Concave Saddle Point Problems.” <em>SIAM J. Optim.</em> 15
(1):229–51. <a href="https://doi.org/10.1137/S1052623403425629">https://doi.org/10.1137/S1052623403425629</a>.</p>

<p>Nemirovski, A., A. Juditsky, G. Lan, and A. Shapiro. 2008. “Robust
Stochastic Approximation Approach to Stochastic Programming.” <em>SIAM J.
Optim.</em> 19 (4):1574–1609.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/070704277">https://doi-org.offcampus.lib.washington.edu/10.1137/070704277</a>.</p>

<p>Nemirovsky, A.S., and D.B. Yudin. 1983. <em>Problem Complexity and Method
Efficiency in Optimization</em>. A Wiley-Interscience Publication. John
Wiley &amp; Sons, Inc., New York.</p>

<p>Nesterov, Y., and A. Nemirovskii. 1994. <em>Interior-Point Polynomial
Algorithms in Convex Programming</em>. Vol. 13. SIAM Studies in Applied
Mathematics. Society for Industrial; Applied Mathematics (SIAM),
Philadelphia, PA. <a href="https://doi.org/10.1137/1.9781611970791">https://doi.org/10.1137/1.9781611970791</a>.</p>

<p>Nesterov, Yu. 1983. “A Method for Solving the Convex Programming Problem
with Convergence Rate \(O(1/k^{2})\).” <em>Dokl. Akad. Nauk SSSR</em> 269
(3):543–47.</p>

<p>Nesterov, Yu. 2005. “Smooth Minimization of Non-Smooth Functions.” <em>Math.
Program.</em> 103 (1, Ser. A):127–52.
<a href="https://doi.org/10.1007/s10107-004-0552-5">https://doi.org/10.1007/s10107-004-0552-5</a>.</p>

<p>Nesterov, Yu. 2007. “Modified Gauss-Newton Scheme with Worst Case
Guarantees for Global Performance.” <em>Optim. Methods Softw.</em> 22
(3):469–83. <a href="https://doi.org/10.1080/08927020600643812">https://doi.org/10.1080/08927020600643812</a>.</p>

<p>Nesterov, Yu. 2013. “Gradient Methods for Minimizing Composite Functions.”
<em>Math. Program.</em> 140 (1, Ser. B):125–61.
<a href="https://doi.org/10.1007/s10107-012-0629-5">https://doi.org/10.1007/s10107-012-0629-5</a>.</p>

<p>Paquette, C., H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. 2017. 
“Catalyst Acceleration for Gradient-Based Non-Convex
Optimization.” <em>Preprint arXiv:1703.10993</em>.</p>

<p>Polyak, B.T., and A.B. Juditsky. 1992. “Acceleration of Stochastic
Approximation by Averaging.” <em>SIAM J. Control Optim.</em> 30 (4):838–55.
<a href="https://doi.org/10.1137/0330046">https://doi.org/10.1137/0330046</a>.</p>

<p>Rakhlin, A., O. Shamir, and K. Sridharan. 2012. “Making Gradient Descent
Optimal for Strongly Convex Stochastic Optimization.” In <em>Proceedings of
the 29th International Coference on International Conference on Machine
Learning</em>, 1571–8. ICML’12. USA: Omnipress.
<a href="http://dl.acm.org/citation.cfm?id=3042573.3042774">http://dl.acm.org/citation.cfm?id=3042573.3042774</a>.</p>

<p>Robbins, H., and S. Monro. 1951. “A Stochastic Approximation Method.”
<em>Ann. Math. Statistics</em> 22:400–407.</p>

<p>Schmidt, M., N. Le Roux, and F. Bach. 2013. “Minimizing Finite Sums with
the Stochastic Average Gradient.” <em>arXiv:1309.2388</em>.</p>

<p>Shalev-Shwartz, S., and T. Zhang. 2012. “Proximal Stochastic Dual
Coordinate Ascent.” <em>arXiv:1211.2717</em>.</p>

<p>S. Shalev-Shwartz and T. Zhang. 2015. “Accelerated Proximal Stochastic Dual Coordinate Ascent
for Regularized Loss Minimization.” <em>Mathematical Programming</em>.</p>

<p>Singer, A. 2011. “Angular Synchronization by Eigenvectors and
Semidefinite Programming.” <em>Appl. Comput. Harmon. Anal.</em> 30 (1):20–36.
<a href="https://doi.org/10.1016/j.acha.2010.02.001">https://doi.org/10.1016/j.acha.2010.02.001</a>.</p>

<p>Sun, J., Q. Qu, and J. Wright. 2017. “A Geometric Analysis of Phase
Retrieval.” <em>To Appear in Found. Comp. Math., arXiv:1602.06664</em>.</p>

<p>Woodworth, B.E., and N. Srebro. 2016. “Tight Complexity Bounds for
Optimizing Composite Objectives.” In <em>Advances in Neural Information
Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, 3639–47. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf">http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf</a>.</p>

<p>Wright, S.J. 1997. <em>Primal-Dual Interior-Point Methods</em>. Society for
Industrial; Applied Mathematics (SIAM), Philadelphia, PA.
<a href="https://doi.org/10.1137/1.9781611971453">https://doi.org/10.1137/1.9781611971453</a>.</p>

<p>Xiao, L., and T. Zhang. 2014. “A Proximal Stochastic Gradient Method
with Progressive Variance Reduction.” <em>SIAM J. Optim.</em> 24 (4):2057–75.
<a href="https://doi.org/10.1137/140961791">https://doi.org/10.1137/140961791</a>.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/01/30/prox-linear/</guid>
                <description>
                    
                    Revisiting the proximal point method. Composite models and the prox-linear algorithm.
                    
                </description>
                <pubDate>Tue, 30 Jan 2018 16:02:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Proximal point algorithm revisited, episode 1. The proximally guided subgradient method</title>
                <link>http://localhost:4000/blog/2018/01/24/proximal-subgrad/</link>
                <content:encoded>
                    <![CDATA[
                    <p>This is episode 1 of the three part series that revisits the classical proximal
point algorithm. See the <a href="../proximal-point">previous post</a> for 
introduction and notation.</p>

<h1 id="the-proximally-guided-subgradient-method"><a name="sec1"></a>The proximally guided subgradient method</h1>

<p>As the first example of contemporary applications of the proximal point
method, consider the problem of minimizing the expectation:<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">1</a></sup></p>

\[\min_{x\in {\mathbb R}^d}~ F(x)=\mathbb{E}_{\zeta} f(x,\zeta).\]

<p>Here,
\(\zeta\) is a random variable, and the only access to \(F\) is by sampling
\(\zeta\). It is difficult to overstate the importance of this problem
class (often called <em>stochastic approximation</em>) in large-scale
optimization; see e.g. Bottou and Bousquet (2008); Bartlett, Jordan, and
McAuliffe (2006).</p>

<p>When the problem is convex, the stochastic subgradient method (Polyak
and Juditsky 1992; Robbins and Monro 1951; Nemirovski et al. 2008) has
strong theoretical guarantees and is often the method of choice. In
contrast, when applied to nonsmooth and nonconvex problems, the behavior
of the method is poorly understood. The recent paper (Davis and Grimmer
2017) shows how to use the proximal point method to guide the
subgradient iterates in this broader setting, with rigorous guarantees.</p>

<p>Henceforth, assume that the function \(x\mapsto f(x,\zeta)\) is
\(\rho\)-weakly convex and \(L\)-Lipschitz for each \(\zeta\). Davis and
Grimmer (2017) proposed the scheme outlined below.</p>

<h4 id="proximally-guided-stochastic-subgradient-method">Proximally guided stochastic subgradient method</h4>

<ul>
  <li><strong>Data</strong>: \(x_0\in {\mathbb R}^d\), \(\{j_t\}\subset\mathbb{N}\),
\(\{\alpha_j\}\subset{\mathbb R}_{++}\)</li>
  <li><strong>For</strong> \(t=0,\ldots,T\) <strong>do</strong>
    <ul>
      <li>Set \(y_0=x_t\)</li>
      <li><strong>For</strong> \(j=0,\ldots,j_t-2\) <strong>do</strong>
        <ul>
          <li>Sample \(\zeta\) and choose \(v_j\in\partial (f(\cdot,\zeta)+\rho\|\cdot-x_t\|^2)(y_j)\)</li>
          <li>Set \(y_{j+1}= y_j-\alpha_jv_j\)</li>
        </ul>
      </li>
      <li>Set \(x_{t+1}= \frac{1}{j_t}\sum_{j=0}^{j_t-1}y_j\)</li>
    </ul>
  </li>
</ul>

<p>The method proceeds by applying a proximal point method with each
subproblem approximately solved by a stochastic subgradient method. The
intuition is that each proximal subproblem is \(\rho/2\)-strongly convex
and therefore according to well-known results (e.g. Lacoste-Julien,
Schmidt, and Bach (2012); Rakhlin, Shamir, and Sridharan (2012); Hazan and
Kale (2011); Juditsky and Nesterov (2014)), the stochastic subgradient
method should converge at the rate \(O(\frac{1}{T})\) on the subproblem,
in expectation. This intuition is not quite correct because the
objective function of the subproblem is not globally Lipschitz – a key
assumption for the \(O(\frac{1}{T})\) rate. Nonetheless, the authors show
that warm-starting the subgradient method for each proximal subproblem
with the current proximal iterate corrects this issue, yielding a
favorable guarantees (Davis and Grimmer 2017 Theorem 1).</p>

<p>To describe the rate of convergence, set
\(j_t=t+\lceil 648\log(648)\rceil\) and \(\alpha_j=\tfrac{2}{\rho(j+49)}\)
in the Proximally guided stochastic subgradient method. Then the scheme
will generate an iterate \(x\) satisfying</p>

\[\mathbb{E}_{\zeta}[\|\nabla F_{2\rho}(x)\|^2]\leq \varepsilon\]

<p>after
at most</p>

\[O\left(\frac{\rho^2(F(x_0)-\inf  F)^2}{\varepsilon^2}+\frac{L^4 \log^{4}(\varepsilon^{-1})}{\varepsilon^2}\right)\]

<p>subgradient evaluations. This rate agrees with analogous guarantees for
stochastic gradient methods for smooth nonconvex functions (Ghadimi and
Lan 2013). It is also worth noting that convex constraints on \(x\) can be
easily incorporated into the Proximally guided stochastic subgradient
method by introducing a nearest-point projection in the definition of
\(y_{j+1}\).</p>

<h1 id="references">References<a name="ref"></a></h1>
<p>Abbe, E., A.S. Bandeira, A. Bracher, and A. Singer. 2014. “Decoding
Binary Node Labels from Censored Edge Measurements: Phase Transition and
Efficient Recovery.” <em>IEEE Trans. Network Sci. Eng.</em> 1 (1):10–22.
<a href="https://doi.org/10.1109/TNSE.2014.2368716">https://doi.org/10.1109/TNSE.2014.2368716</a>.</p>

<p>Agarwal, A., and L. Bottou. 2015. “A Lower Bound for the Optimization of
Finite Sums.” In <em>Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>, 78–86.
<a href="http://leon.bottou.org/papers/agarwal-bottou-2015">http://leon.bottou.org/papers/agarwal-bottou-2015</a>.</p>

<p>Allen-Zhu, Z. 2016. “Katyusha: The First Direct Acceleration of
Stochastic Gradient Methods.” <em>Preprint arXiv:1603.05953 (Version 5)</em>.</p>

<p>Arjevani, Y. 2017. “Limitations on Variance-Reduction and Acceleration
Schemes for Finite Sums Optimization.” In <em>Advances in Neural
Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
3543–52. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf">http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf</a>.</p>

<p>Bandeira, A.S., N. Boumal, and V. Voroninski. 2016. “On the Low-Rank
Approach for Semidefinite Programs Arising in Synchronization and
Community Detection.” In <em>Proceedings of the 29th Conference on Learning
Theory, COLT 2016, New York, Usa, June 23-26, 2016</em>, 361–82.
<a href="http://jmlr.org/proceedings/papers/v49/bandeira16.html">http://jmlr.org/proceedings/papers/v49/bandeira16.html</a>.</p>

<p>Bartlett, P.L., M.I. Jordan, and J.D. McAuliffe. 2006. “Convexity,
Classification, and Risk Bounds.” <em>J. Amer. Statist. Assoc.</em> 101
(473):138–56.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907">https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907</a>.</p>

<p>Beck, A., and M. Teboulle. 2012. “Smoothing and First Order Methods: A
Unified Framework.” <em>SIAM J. Optim.</em> 22 (2):557–80.
<a href="https://doi.org/10.1137/100818327">https://doi.org/10.1137/100818327</a>.</p>

<p>Bottou, L., and O. Bousquet. 2008. “The Tradeoffs of Large Scale
Learning.” In <em>Advances in Neural Information Processing Systems</em>,
161–68. <a href="http://leon.bottou.org/publications/pdf/nips-2007.pdf">http://leon.bottou.org/publications/pdf/nips-2007.pdf</a>.</p>

<p>Burke, J.V., and M.C. Ferris. 1995. “A Gauss-Newton Method for Convex
Composite Optimization.” <em>Math. Programming</em> 71 (2, Ser. A):179–94.
<a href="https://doi.org/10.1007/BF01585997">https://doi.org/10.1007/BF01585997</a>.</p>

<p>Candès, E.J., X. Li, Y. Ma, and J. Wright. 2011. “Robust Principal
Component Analysis?” <em>J. ACM</em> 58 (3):Art. 11, 37.
<a href="https://doi.org/10.1145/1970392.1970395">https://doi.org/10.1145/1970392.1970395</a>.</p>

<p>Candès, E.J., X. Li, and M. Soltanolkotabi. 2015. “Phase Retrieval via
Wirtinger Flow: Theory and Algorithms.” <em>IEEE Trans. Inform. Theory</em> 61
(4):1985–2007. <a href="https://doi.org/10.1109/TIT.2015.2399924">https://doi.org/10.1109/TIT.2015.2399924</a>.</p>

<p>Carmon, Y., J.C. Duchi, O. Hinder, and A. Sidford. 2017a. “‘Convex Until
Proven Guilty’: Dimension-Free Acceleration of Gradient Descent on
Non-Convex Functions.” In <em>Proceedings of the 34th International
Conference on Machine Learning</em>, 70:654–63.</p>

<p>Y. Carmon, J.C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I.
<em>Preprint arXiv:1710.11606</em>.</p>

<p>Cartis, C., N.I.M. Gould, and P.L. Toint. 2011. “On the Evaluation
Complexity of Composite Function Minimization with Applications to
Nonconvex Nonlinear Programming.” <em>SIAM J. Optim.</em> 21 (4):1721–39.
<a href="https://doi.org/10.1137/11082381X">https://doi.org/10.1137/11082381X</a>.</p>

<p>Chambolle, A., and T. Pock. 2011. “A First-Order Primal-Dual Algorithm
for Convex Problems with Applications to Imaging.” <em>J. Math. Imaging
Vision</em> 40 (1):120–45. <a href="https://doi.org/10.1007/s10851-010-0251-1">https://doi.org/10.1007/s10851-010-0251-1</a>.</p>

<p>Chandrasekaran, V., S. Sanghavi, P. A. Parrilo, and A.S. Willsky. 2011.
“Rank-Sparsity Incoherence for Matrix Decomposition.” <em>SIAM J. Optim.</em>
21 (2):572–96. <a href="https://doi.org/10.1137/090761793">https://doi.org/10.1137/090761793</a>.</p>

<p>Chen, Y., and E.J. Candès. 2017. “Solving Random Quadratic Systems of
Equations Is Nearly as Easy as Solving Linear Systems.” <em>Comm. Pure
Appl. Math.</em> 70 (5):822–83.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638">https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638</a>.</p>

<p>Davis, D. 2016. “SMART: The Stochastic Monotone Aggregated Root-Finding
Algorithm.” <em>Preprint arXiv:1601.00698</em>.</p>

<p>Davis, D., D. Drusvyatskiy, and C. Paquette. 2017. “The Nonsmooth
Landscape of Phase Retrieval.” <em>Preprint arXiv:1711.03247</em>.</p>

<p>Davis, D., and B. Grimmer. 2017. “Proximally Guided Stochastic
Sbgradient Method for Nonsmooth, Nonconvex Problems.” <em>Preprint,
arXiv:1707.03505</em>.</p>

<p>Defazio, A. 2016. “A Simple Practical Accelerated Method for Finite
Sums.” In <em>Advances in Neural Information Processing Systems 29</em>, edited
by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
676–84. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf">http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf</a>.</p>

<p>Defazio, A., F. Bach, and S. Lacoste-Julien. 2014. “SAGA: A Fast
Incremental Gradient Method with Support for Non-Strongly Convex
Composite Objectives.” In <em>Advances in Neural Information Processing
Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, 1646–54. Curran Associates, Inc.</p>

<p>Defazio, A., J. Domke, and T.S. Caetano. 2014. “Finito: A Faster,
Permutable Incremental Gradient Method for Big Data Problems.” In
<em>ICML</em>, 1125–33.</p>

<p>Drusvyatskiy, D., and A.S. Lewis. 2013. “Tilt Stability, Uniform
Quadratic Growth, and Strong Metric Regularity of the Subdifferential.”
<em>SIAM J. Optim.</em> 23 (1):256–67. <a href="https://doi.org/10.1137/120876551">https://doi.org/10.1137/120876551</a>.</p>

<p>D. Drusvyatskiy and A.S. Lewis. 2016. “Error Bounds, Quadratic Growth, and Linear Convergence
of Proximal Methods.” <em>To Appear in Math. Oper. Res., arXiv:1602.06661</em>.</p>

<p>Drusvyatskiy, D., B.S. Mordukhovich, and T.T.A. Nghia. 2014.
“Second-Order Growth, Tilt-Stability, and Metric Regularity of the
Subdifferential.” <em>J. Convex Anal.</em> 21 (4):1165–92.</p>

<p>Drusvyatskiy, D., and C. Paquette. 2016. “Efficiency of Minimizing
Compositions of Convex Functions and Smooth Maps.” <em>Preprint,
arXiv:1605.00125</em>.</p>

<p>Duchi, J.C., and F. Ruan. 2017a. “Solving (Most) of a Set of Quadratic
Equalities: Composite Optimization for Robust Phase Retrieval.”
<em>Preprint arXiv:1705.02356</em>.</p>

<p>J.C. Duchi and F. Ruan. 2017b. “Stochastic Methods for Composite Optimization
Problems.” <em>Preprint arXiv:1703.08570</em>.</p>

<p>Eldar, Y.C., and S. Mendelson. 2014. “Phase Retrieval: Stability and
Recovery Guarantees.” <em>Appl. Comput. Harmon. Anal.</em> 36 (3):473–94.
<a href="https://doi.org/10.1016/j.acha.2013.08.003">https://doi.org/10.1016/j.acha.2013.08.003</a>.</p>

<p>Frostig, R., R. Ge, S.M. Kakade, and A. Sidford. 2015. “Un-Regularizing:
Approximate Proximal Point and Faster Stochastic Algorithms for
Empirical Risk Minimization.” In <em>Proceedings of the 32nd International
Conference on Machine Learning (ICML)</em>.</p>

<p>Ghadimi, S., and G. Lan. 2013. “Stochastic First- and Zeroth-Order
Methods for Nonconvex Stochastic Programming.” <em>SIAM J. Optim.</em> 23
(4):2341–68. <a href="https://doi.org/10.1137/120880811">https://doi.org/10.1137/120880811</a>.</p>

<p>Güler, O. 1992. “New Proximal Point Algorithms for Convex Minimization.”
<em>SIAM J. Optim.</em> 2 (4):649–64.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/0802032">https://doi-org.offcampus.lib.washington.edu/10.1137/0802032</a>.</p>

<p>Hazan, E., and S. Kale. 2011. “Beyond the Regret Minimization Barrier:
An Optimal Algorithm for Stochastic Strongly-Convex Optimization.” In
<em>Proceedings of the 24th Annual Conference on Learning Theory</em>, edited
by Sham M. Kakade and Ulrike von Luxburg, 19:421–36. Proceedings of
Machine Learning Research. Budapest, Hungary: PMLR.</p>

<p>Johnson, R., and T. Zhang. 2013. “Accelerating Stochastic Gradient
Descent Using Predictive Variance Reduction.” In <em>Proceedings of the
26th International Conference on Neural Information Processing Systems</em>,
315–23. NIPS’13. USA: Curran Associates Inc.
<a href="http://dl.acm.org/citation.cfm?id=2999611.2999647">http://dl.acm.org/citation.cfm?id=2999611.2999647</a>.</p>

<p>Juditsky, A., and Y. Nesterov. 2014. “Deterministic and Stochastic
Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization.”
<em>Stoch. Syst.</em> 4 (1):44–80.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010">https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010</a>.</p>

<p>Lacoste-Julien, S., M. Schmidt, and F. Bach. 2012. “A Simpler Approach
to Obtaining an \({O}(1/t)\) Convergence Rate for the Projected Stochastic
Subgradient Method.” <em>Arxiv arXiv:1212.2002</em>.</p>

<p>Lan, G. 2015. “An Optimal Randomized Incremental Gradient Method.”
<em>arXiv:1507.02000</em>.</p>

<p>Lemarechal, C., J.-J. Strodiot, and A. Bihain. 1981. “On a Bundle
Algorithm for Nonsmooth Optimization.” In <em>Nonlinear Programming, 4
(Madison, Wis., 1980)</em>, 245–82. Academic Press, New York-London.</p>

<p>Lewis, A.S., and S.J. Wright. 2015. “A Proximal Method for Composite
Minimization.” <em>Math. Program.</em> Springer Berlin Heidelberg, 1–46.
<a href="https://doi.org/10.1007/s10107-015-0943-9">https://doi.org/10.1007/s10107-015-0943-9</a>.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. 2015. “A Universal Catalyst for
First-Order Optimization.” In <em>Advances in Neural Information Processing
Systems</em>, 3366–74.</p>

<p>Luke, R. 2017. “Phase Retrieval, What’s New?” <em>SIAG/OPT Views and News</em>
25 (1).</p>

<p>Mairal, J. 2015. “Incremental Majorization-Minimization Optimization
with Application to Large-Scale Machine Learning.” <em>SIAM Journal on
Optimization</em> 25 (2):829–55.</p>

<p>Nemirovski, A. 2004. “Prox-Method with Rate of Convergence \(O(1/t)\) for
Variational Inequalities with Lipschitz Continuous Monotone Operators
and Smooth Convex-Concave Saddle Point Problems.” <em>SIAM J. Optim.</em> 15
(1):229–51. <a href="https://doi.org/10.1137/S1052623403425629">https://doi.org/10.1137/S1052623403425629</a>.</p>

<p>Nemirovski, A., A. Juditsky, G. Lan, and A. Shapiro. 2008. “Robust
Stochastic Approximation Approach to Stochastic Programming.” <em>SIAM J.
Optim.</em> 19 (4):1574–1609.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/070704277">https://doi-org.offcampus.lib.washington.edu/10.1137/070704277</a>.</p>

<p>Nemirovsky, A.S., and D.B. Yudin. 1983. <em>Problem Complexity and Method
Efficiency in Optimization</em>. A Wiley-Interscience Publication. John
Wiley &amp; Sons, Inc., New York.</p>

<p>Nesterov, Y., and A. Nemirovskii. 1994. <em>Interior-Point Polynomial
Algorithms in Convex Programming</em>. Vol. 13. SIAM Studies in Applied
Mathematics. Society for Industrial; Applied Mathematics (SIAM),
Philadelphia, PA. <a href="https://doi.org/10.1137/1.9781611970791">https://doi.org/10.1137/1.9781611970791</a>.</p>

<p>Nesterov, Yu. 1983. “A Method for Solving the Convex Programming Problem
with Convergence Rate \(O(1/k^{2})\).” <em>Dokl. Akad. Nauk SSSR</em> 269
(3):543–47.</p>

<p>Nesterov, Yu. 2005. “Smooth Minimization of Non-Smooth Functions.” <em>Math.
Program.</em> 103 (1, Ser. A):127–52.
<a href="https://doi.org/10.1007/s10107-004-0552-5">https://doi.org/10.1007/s10107-004-0552-5</a>.</p>

<p>Nesterov, Yu. 2007. “Modified Gauss-Newton Scheme with Worst Case
Guarantees for Global Performance.” <em>Optim. Methods Softw.</em> 22
(3):469–83. <a href="https://doi.org/10.1080/08927020600643812">https://doi.org/10.1080/08927020600643812</a>.</p>

<p>Nesterov, Yu. 2013. “Gradient Methods for Minimizing Composite Functions.”
<em>Math. Program.</em> 140 (1, Ser. B):125–61.
<a href="https://doi.org/10.1007/s10107-012-0629-5">https://doi.org/10.1007/s10107-012-0629-5</a>.</p>

<p>Paquette, C., H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. 2017. 
“Catalyst Acceleration for Gradient-Based Non-Convex
Optimization.” <em>Preprint arXiv:1703.10993</em>.</p>

<p>Polyak, B.T., and A.B. Juditsky. 1992. “Acceleration of Stochastic
Approximation by Averaging.” <em>SIAM J. Control Optim.</em> 30 (4):838–55.
<a href="https://doi.org/10.1137/0330046">https://doi.org/10.1137/0330046</a>.</p>

<p>Rakhlin, A., O. Shamir, and K. Sridharan. 2012. “Making Gradient Descent
Optimal for Strongly Convex Stochastic Optimization.” In <em>Proceedings of
the 29th International Coference on International Conference on Machine
Learning</em>, 1571–8. ICML’12. USA: Omnipress.
<a href="http://dl.acm.org/citation.cfm?id=3042573.3042774">http://dl.acm.org/citation.cfm?id=3042573.3042774</a>.</p>

<p>Robbins, H., and S. Monro. 1951. “A Stochastic Approximation Method.”
<em>Ann. Math. Statistics</em> 22:400–407.</p>

<p>Schmidt, M., N. Le Roux, and F. Bach. 2013. “Minimizing Finite Sums with
the Stochastic Average Gradient.” <em>arXiv:1309.2388</em>.</p>

<p>Shalev-Shwartz, S., and T. Zhang. 2012. “Proximal Stochastic Dual
Coordinate Ascent.” <em>arXiv:1211.2717</em>.</p>

<p>S. Shalev-Shwartz and T. Zhang. 2015. “Accelerated Proximal Stochastic Dual Coordinate Ascent
for Regularized Loss Minimization.” <em>Mathematical Programming</em>.</p>

<p>Singer, A. 2011. “Angular Synchronization by Eigenvectors and
Semidefinite Programming.” <em>Appl. Comput. Harmon. Anal.</em> 30 (1):20–36.
<a href="https://doi.org/10.1016/j.acha.2010.02.001">https://doi.org/10.1016/j.acha.2010.02.001</a>.</p>

<p>Sun, J., Q. Qu, and J. Wright. 2017. “A Geometric Analysis of Phase
Retrieval.” <em>To Appear in Found. Comp. Math., arXiv:1602.06664</em>.</p>

<p>Woodworth, B.E., and N. Srebro. 2016. “Tight Complexity Bounds for
Optimizing Composite Objectives.” In <em>Advances in Neural Information
Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, 3639–47. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf">http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf</a>.</p>

<p>Wright, S.J. 1997. <em>Primal-Dual Interior-Point Methods</em>. Society for
Industrial; Applied Mathematics (SIAM), Philadelphia, PA.
<a href="https://doi.org/10.1137/1.9781611971453">https://doi.org/10.1137/1.9781611971453</a>.</p>

<p>Xiao, L., and T. Zhang. 2014. “A Proximal Stochastic Gradient Method
with Progressive Variance Reduction.” <em>SIAM J. Optim.</em> 24 (4):2057–75.
<a href="https://doi.org/10.1137/140961791">https://doi.org/10.1137/140961791</a>.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:3" role="doc-endnote">
      <p>For simplicity of the exposition, the minimization problem is
unconstrained. Simple constraints can be accommodated using a
projection operation. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/01/24/proximal-subgrad/</guid>
                <description>
                    
                    Revisiting the proximal point method, with the proximally guided subgradient method for stochastic optimization.
                    
                </description>
                <pubDate>Wed, 24 Jan 2018 16:01:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>The proximal point method revisited, episode 0. Introduction</title>
                <link>http://localhost:4000/blog/2018/01/24/proximal-point/</link>
                <content:encoded>
                    <![CDATA[
                    <h1 id="introduction">Introduction</h1>

<p>The proximal point method is a conceptually simple algorithm for
minimizing a function \(f\) on \({\mathbb R}^d\). Given an iterate \(x_t\),
the method defines \(x_{t+1}\) to be any minimizer of the proximal
subproblem</p>

\[\underset{x}{\operatorname{argmin}}~\left\{f(x)+\tfrac{1}{2\nu}\|x-x_t\|^2\right\},\]

<p>for an appropriately chosen parameter \(\nu&gt;0\). At first glance, each proximal
subproblem seems no easier than minimizing \(f\) in the first place. On
the contrary, the addition of the quadratic penalty term often
regularizes the proximal subproblems and makes them well conditioned.
Case in point, the subproblem may become convex despite \(f\) not being
convex; and even if \(f\) were convex, the subproblem has a larger strong
convexity parameter thereby facilitating faster numerical methods.</p>

<p>Despite the improved conditioning, each proximal subproblem still
requires invoking an iterative solver. For this reason, the proximal
point method has predominantly been thought of as a
theoretical/conceptual algorithm, only guiding algorithm design and
analysis rather than being implemented directly. One good example is the
proximal bundle method (Lemarechal, Strodiot, and Bihain 1981), which
approximates each proximal subproblem by a cutting plane model. In the
past few years, this viewpoint has undergone a major revision. In a
variety of circumstances, the proximal point method (or a close variant)
with a judicious choice of the control parameter \(\nu&gt;0\) and an
appropriate iterative method for the subproblems can lead to practical
and theoretically sound numerical methods. In this blog, I will briefly
describe three recent examples of this trend:</p>

<ul>
  <li>
    <p><a href="../proximal-subgrad">Episode 1</a>: a subgradient method for weakly convex stochastic approximation
problems (Davis and Grimmer 2017),</p>
  </li>
  <li>
    <p><a href="../../30/prox-linear/">Episode 2</a>: the prox-linear algorithm for minimizing compositions of convex
functions and smooth maps (Drusvyatskiy and Lewis 2016; Drusvyatskiy
and Paquette 2016; Burke and Ferris 1995; Nesterov 2007; Lewis and
Wright 2015; Cartis, Gould, and Toint 2011),</p>
  </li>
  <li>
    <p><a href="../../../02/05/catalyst">Episode 3</a>: Catalyst generic acceleration schema (Lin, Mairal, and
Harchaoui 2015; 2017) for regularized Empirical Risk Minimization.</p>
  </li>
</ul>

<!---
-   [Episode 2](../prox-linear): The prox-linear algorithm for minimizing compositions of convex
    functions and smooth maps (Drusvyatskiy and Lewis 2016; Drusvyatskiy
    and Paquette 2016; Burke and Ferris 1995; Nesterov 2007; Lewis and
    Wright 2015; Cartis, Gould, and Toint 2011),
-   [Episode 3](../catalyst): Catalyst generic acceleration schema (Lin, Mairal, and
    Harchaoui 2015) for regularized Empirical Risk Minimization.
--->

<p>Each epsiode, discussing the examples above, is self-contained and can
be read independently of the others. A version of this blog series will
appear in SIAG/OPT Views and News 2018.</p>

<h1 id="notation"><a name="notation"></a>Notation</h1>

<p>The following two constructions will play a basic role in the blog. For
any closed function \(f\) on \({\mathbb R}^d\), the <em>Moreau envelope</em> and
the <em>proximal map</em> are</p>

\[\begin{aligned}
f_{\nu}(z)&amp;:=\inf_{x}~\left\{f(x)+\tfrac{1}{2\nu}\|x-z\|^2\right\},\\
{\rm prox}_{\nu f}(z)&amp;:=\underset{x}{\operatorname{argmin}}~\left\{f(x)+\tfrac{1}{2\nu}\|x-z\|^2\right\},
\end{aligned}\]

<p>respectively. In this notation, the proximal point method is simply the
fixed-point recurrence on the proximal map:<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">1</a></sup></p>

\[{\bf Step\, }t: \qquad \textrm{choose }x_{t+1}\in {\rm prox}_{\nu f}(x_t).\]

<p>Clearly, in order to have any hope of solving the proximal subproblems,
one must ensure that they are convex. Consequently, the class of weakly
convex functions forms the natural setting for the proximal point
method.</p>

<p>A function \(f\) is called <em>\(\rho\)-weakly convex</em> if the assignment
\(x\mapsto f(x)+\frac{\rho}{2}\|x\|^2\) is a convex function.</p>

<p>For example, a \(C^1\)-smooth function with \(\rho\)-Lipschitz gradient is
\(\rho\)-weakly convex, while a \(C^2\)-smooth function \(f\) is \(\rho\)-weakly
convex precisely when the minimal eigenvalue of its Hessian is uniformly
bounded below by \(-\rho\). In essence, weak convexity precludes functions
that have downward kinks. For instance, \(f(x):=-\|x\|\) is not weakly
convex since no addition of a quadratic makes the resulting function
convex.</p>

<p>Whenever \(f\) is \(\rho\)-weakly convex and the proximal parameter \(\nu\)
satisfies \(\nu&lt;\rho^{-1}\), each proximal subproblem is itself convex and
therefore globally tractable. Moreover, in this setting, the Moreau
envelope is \(C^1\)-smooth with the gradient</p>

\[\nabla f_{\nu}(x)=\nu^{-1}(x-{\rm prox}_{\nu f}(x)).\]

<p>Rearranging the
gradient formula yields the useful interpretation of the proximal point
method as gradient descent on the Moreau envelope</p>

\[x_{t+1}=x_t-\nu\nabla f_{\nu}(x_t).\]

<p>In summary, the Moreau envelope \(f_{\nu}\) serves as a \(C^1\)-smooth
approximation of \(f\) for all small \(\nu\). Moreover, the two conditions</p>

\[\|\nabla f_{\nu}(x_{t})\|&lt; \varepsilon\]

<p>and</p>

\[\|\nu^{-1}(x_t-x_{t+1})\|&lt;\varepsilon,\]

<p>are equivalent for the
proximal point sequence \(\{x_t\}\). Hence, the step-size
\(\|x_t-x_{t+1}\|\) of the proximal point method serves as a convenient
termination criteria.</p>

<h2 id="examples-of-weakly-convex-functions">Examples of weakly convex functions</h2>

<p>Weakly convex functions are widespread in applications and are typically
easy to recognize. One common source of weakly convex functions is the
composite problem class \(\mathcal{C}\):</p>

\[\min_{x}~ F(x):=g(x)+h(c(x)),\]

<p>where
\(g\colon {\mathbb R}^d\to{\mathbb R}\cup\{+\infty\}\) is a closed convex
function, \(h\colon{\mathbb R}^m\to{\mathbb R}\) is convex and
\(L\)-Lipschitz, and \(c\colon{\mathbb R}^d\to{\mathbb R}^m\) is a
\(C^1\)-smooth map with \(\beta\)-Lipschitz gradient. An easy argument shows
that \(F\) is \(L\beta\)-weakly convex. This is a worst case estimate. In
concrete circumstances, the composite function \(F\) may have a much more
favorable weak convexity constant (e.g., phase retrieval (Duchi and Ruan
2017a, Section 3.2)).</p>

<ol>
  <li>
    <p>(Additive composite)
 The most prevalent example is additive composite minimization. In this
 case, the map \(c\) maps to the real line and \(h\) is the identity
 function:</p>

\[\label{eqn:add_comp}
 \min_{x}~ c(x)+g(x).\]

    <p>Such problems appear often in statistical
 learning and imaging. A variety of specialized algorithms are available;
 see for example Beck and Teboulle (2012) or Nesterov
 (2013).</p>
  </li>
  <li>
    <p>(Nonlinear least squares)</p>

    <p>The composite problem class also captures nonlinear least squares
 problems with bound constraints:</p>

\[\begin{aligned}
         \min_x~ \|c(x)\|_2\quad \textrm{subject to}\quad l_i\leq x_i\leq u_i ~\forall i.
 \end{aligned}\]

    <p>Such problems pervade engineering and scientific
 applications.</p>
  </li>
  <li>
    <p>(Exact penalty formulations)
 Consider a nonlinear optimization
 problem:</p>

\[\begin{aligned}
         \min_x~ \{f(x): G(x)\in \mathcal{K}\},
 \end{aligned}\]

    <p>where \(f\) and \(G\) are smooth maps and
 \(\mathcal{K}\) is a closed convex cone. An accompanying <em>penalty
 formulation</em> – ubiquitous in nonlinear optimization – takes the form</p>

\[\min_x~ f(x)+\lambda \cdot {\rm dist}_{\mathcal{K}}(G(x)),\]

    <p>where
 \({\rm dist}_{\mathcal{K}}(\cdot)\) is the distance to \(\mathcal{K}\) in
 some norm. Historically, exact penalty formulations served as the early
 motivation for the composite class \(\mathcal{C}\).</p>
  </li>
  <li>
    <p>(Robust phase retrieval)
 Phase retrieval is a common computational problem, with applications in
 diverse areas, such as imaging, X-ray crystallography, and speech
 processing. For simplicity, I will focus on the version of the problem
 over the reals. The (real) phase retrieval problem seeks to determine a
 point \(x\) satisfying the magnitude conditions,</p>

\[|\langle a_i,x\rangle|\approx b_i\quad \textrm{for }i=1,\ldots,m,\]

    <p>where \(a_i\in {\mathbb R}^d\) and \(b_i\in{\mathbb R}\) are given. Whenever
 there are gross outliers in the measurements \(b_i\), the following robust
 formulation of the problem is appealing (Eldar and Mendelson 2014; Duchi
 and Ruan 2017a; Davis, Drusvyatskiy, and Paquette 2017):</p>

\[\min_x ~\tfrac{1}{m}\sum_{i=1}^m |\langle a_i,x\rangle^2-b_i^2|.\]

    <p>Clearly, this is an instance of the composite class \(\mathcal{C}\).
 For some recent perspectives on phase retrieval, see the survey (Luke
 2017). There are numerous recent nonconvex approaches to phase
 retrieval, which rely on alternate problem formulations; e.g., (Candès,
 Li, and Soltanolkotabi 2015; Chen and Candès 2017; Sun, Qu, and Wright
 2017).</p>
  </li>
  <li>
    <p>(Robust PCA)
 In robust principal component analysis, one seeks to identify sparse
 corruptions of a low-rank matrix (Candès et al. 2011; Chandrasekaran et
 al. 2011). One typical example is image deconvolution, where the
 low-rank structure models the background of an image while the sparse
 corruption models the foreground. Formally, given a \(m\times n\) matrix
 \(M\), the goal is to find a decomposition \(M=L+S\), where \(L\) is low-rank
 and \(S\) is sparse. A common formulation of the problem reads:</p>

\[\min_{U\in {\mathbb R}^{m\times r},V\in {\mathbb R}^{n\times r}}~ \|UV^T-M\|_1,\]

    <p>where \(r\) is the target rank.</p>
  </li>
  <li>
    <p>(Censored \(\mathbb{Z}_2\) synchronization)
 A synchronization problem over a graph is to estimate group elements
 \(g_1,\ldots, g_n\) from pairwise products \(g_ig_j^{-1}\) over a set of
 edges \(ij\in E\). For a list of application of such problem see
 (Bandeira, Boumal, and Voroninski 2016; Singer 2011; Abbe et al. 2014),
 and references therein. A simple instance is \(\mathbb{Z}_2\)
 synchronization, corresponding to the group on two elements \(\{-1,+1\}\).
 The popular problem of detecting communities in a network, within the
 Binary Stochastic Block Model (SBM), can be modeled using \(\mathbb{Z}_2\)
 synchronization.</p>

    <p>Formally, given a partially observed matrix \(M\), the goal is to recover
 a vector \(\theta\in \{\pm 1\}^d\), satisfying
 \(M_{ij}\approx \theta_i \theta_j\) for all \(ij\in E\). When the entries of
 \(M\) are corrupted by adversarial sign flips, one can postulate the
 following formulation</p>

\[\min_{\theta\in {\mathbb R}^{d}}~ \|P_{E}(\theta\theta^T-M)\|_1,\]

    <p>where the operator \(P_E\) records the entries indexed by the edge set
 \(E\). Clearly, this is again an instance of the composite problem class
 \(\mathcal{C}\).</p>
  </li>
</ol>

<h1 id="references">References<a name="ref"></a></h1>
<p>Abbe, E., A.S. Bandeira, A. Bracher, and A. Singer. 2014. “Decoding
Binary Node Labels from Censored Edge Measurements: Phase Transition and
Efficient Recovery.” <em>IEEE Trans. Network Sci. Eng.</em> 1 (1):10–22.
<a href="https://doi.org/10.1109/TNSE.2014.2368716">https://doi.org/10.1109/TNSE.2014.2368716</a>.</p>

<p>Agarwal, A., and L. Bottou. 2015. “A Lower Bound for the Optimization of
Finite Sums.” In <em>Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>, 78–86.
<a href="http://leon.bottou.org/papers/agarwal-bottou-2015">http://leon.bottou.org/papers/agarwal-bottou-2015</a>.</p>

<p>Allen-Zhu, Z. 2016. “Katyusha: The First Direct Acceleration of
Stochastic Gradient Methods.” <em>Preprint arXiv:1603.05953 (Version 5)</em>.</p>

<p>Arjevani, Y. 2017. “Limitations on Variance-Reduction and Acceleration
Schemes for Finite Sums Optimization.” In <em>Advances in Neural
Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
3543–52. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf">http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf</a>.</p>

<p>Bandeira, A.S., N. Boumal, and V. Voroninski. 2016. “On the Low-Rank
Approach for Semidefinite Programs Arising in Synchronization and
Community Detection.” In <em>Proceedings of the 29th Conference on Learning
Theory, COLT 2016, New York, Usa, June 23-26, 2016</em>, 361–82.
<a href="http://jmlr.org/proceedings/papers/v49/bandeira16.html">http://jmlr.org/proceedings/papers/v49/bandeira16.html</a>.</p>

<p>Bartlett, P.L., M.I. Jordan, and J.D. McAuliffe. 2006. “Convexity,
Classification, and Risk Bounds.” <em>J. Amer. Statist. Assoc.</em> 101
(473):138–56.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907">https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907</a>.</p>

<p>Beck, A., and M. Teboulle. 2012. “Smoothing and First Order Methods: A
Unified Framework.” <em>SIAM J. Optim.</em> 22 (2):557–80.
<a href="https://doi.org/10.1137/100818327">https://doi.org/10.1137/100818327</a>.</p>

<p>Bottou, L., and O. Bousquet. 2008. “The Tradeoffs of Large Scale
Learning.” In <em>Advances in Neural Information Processing Systems</em>,
161–68. <a href="http://leon.bottou.org/publications/pdf/nips-2007.pdf">http://leon.bottou.org/publications/pdf/nips-2007.pdf</a>.</p>

<p>Burke, J.V., and M.C. Ferris. 1995. “A Gauss-Newton Method for Convex
Composite Optimization.” <em>Math. Programming</em> 71 (2, Ser. A):179–94.
<a href="https://doi.org/10.1007/BF01585997">https://doi.org/10.1007/BF01585997</a>.</p>

<p>Candès, E.J., X. Li, Y. Ma, and J. Wright. 2011. “Robust Principal
Component Analysis?” <em>J. ACM</em> 58 (3):Art. 11, 37.
<a href="https://doi.org/10.1145/1970392.1970395">https://doi.org/10.1145/1970392.1970395</a>.</p>

<p>Candès, E.J., X. Li, and M. Soltanolkotabi. 2015. “Phase Retrieval via
Wirtinger Flow: Theory and Algorithms.” <em>IEEE Trans. Inform. Theory</em> 61
(4):1985–2007. <a href="https://doi.org/10.1109/TIT.2015.2399924">https://doi.org/10.1109/TIT.2015.2399924</a>.</p>

<p>Carmon, Y., J.C. Duchi, O. Hinder, and A. Sidford. 2017a. “‘Convex Until
Proven Guilty’: Dimension-Free Acceleration of Gradient Descent on
Non-Convex Functions.” In <em>Proceedings of the 34th International
Conference on Machine Learning</em>, 70:654–63.</p>

<p>Y. Carmon, J.C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I.
<em>Preprint arXiv:1710.11606</em>.</p>

<p>Cartis, C., N.I.M. Gould, and P.L. Toint. 2011. “On the Evaluation
Complexity of Composite Function Minimization with Applications to
Nonconvex Nonlinear Programming.” <em>SIAM J. Optim.</em> 21 (4):1721–39.
<a href="https://doi.org/10.1137/11082381X">https://doi.org/10.1137/11082381X</a>.</p>

<p>Chambolle, A., and T. Pock. 2011. “A First-Order Primal-Dual Algorithm
for Convex Problems with Applications to Imaging.” <em>J. Math. Imaging
Vision</em> 40 (1):120–45. <a href="https://doi.org/10.1007/s10851-010-0251-1">https://doi.org/10.1007/s10851-010-0251-1</a>.</p>

<p>Chandrasekaran, V., S. Sanghavi, P. A. Parrilo, and A.S. Willsky. 2011.
“Rank-Sparsity Incoherence for Matrix Decomposition.” <em>SIAM J. Optim.</em>
21 (2):572–96. <a href="https://doi.org/10.1137/090761793">https://doi.org/10.1137/090761793</a>.</p>

<p>Chen, Y., and E.J. Candès. 2017. “Solving Random Quadratic Systems of
Equations Is Nearly as Easy as Solving Linear Systems.” <em>Comm. Pure
Appl. Math.</em> 70 (5):822–83.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638">https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638</a>.</p>

<p>Davis, D. 2016. “SMART: The Stochastic Monotone Aggregated Root-Finding
Algorithm.” <em>Preprint arXiv:1601.00698</em>.</p>

<p>Davis, D., D. Drusvyatskiy, and C. Paquette. 2017. “The Nonsmooth
Landscape of Phase Retrieval.” <em>Preprint arXiv:1711.03247</em>.</p>

<p>Davis, D., and B. Grimmer. 2017. “Proximally Guided Stochastic
Sbgradient Method for Nonsmooth, Nonconvex Problems.” <em>Preprint,
arXiv:1707.03505</em>.</p>

<p>Defazio, A. 2016. “A Simple Practical Accelerated Method for Finite
Sums.” In <em>Advances in Neural Information Processing Systems 29</em>, edited
by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
676–84. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf">http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf</a>.</p>

<p>Defazio, A., F. Bach, and S. Lacoste-Julien. 2014. “SAGA: A Fast
Incremental Gradient Method with Support for Non-Strongly Convex
Composite Objectives.” In <em>Advances in Neural Information Processing
Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, 1646–54. Curran Associates, Inc.</p>

<p>Defazio, A., J. Domke, and T.S. Caetano. 2014. “Finito: A Faster,
Permutable Incremental Gradient Method for Big Data Problems.” In
<em>ICML</em>, 1125–33.</p>

<p>Drusvyatskiy, D., and A.S. Lewis. 2013. “Tilt Stability, Uniform
Quadratic Growth, and Strong Metric Regularity of the Subdifferential.”
<em>SIAM J. Optim.</em> 23 (1):256–67. <a href="https://doi.org/10.1137/120876551">https://doi.org/10.1137/120876551</a>.</p>

<p>D. Drusvyatskiy and A.S. Lewis. 2016. “Error Bounds, Quadratic Growth, and Linear Convergence
of Proximal Methods.” <em>To Appear in Math. Oper. Res., arXiv:1602.06661</em>.</p>

<p>Drusvyatskiy, D., B.S. Mordukhovich, and T.T.A. Nghia. 2014.
“Second-Order Growth, Tilt-Stability, and Metric Regularity of the
Subdifferential.” <em>J. Convex Anal.</em> 21 (4):1165–92.</p>

<p>Drusvyatskiy, D., and C. Paquette. 2016. “Efficiency of Minimizing
Compositions of Convex Functions and Smooth Maps.” <em>Preprint,
arXiv:1605.00125</em>.</p>

<p>Duchi, J.C., and F. Ruan. 2017a. “Solving (Most) of a Set of Quadratic
Equalities: Composite Optimization for Robust Phase Retrieval.”
<em>Preprint arXiv:1705.02356</em>.</p>

<p>J.C. Duchi and F. Ruan. 2017b. “Stochastic Methods for Composite Optimization
Problems.” <em>Preprint arXiv:1703.08570</em>.</p>

<p>Eldar, Y.C., and S. Mendelson. 2014. “Phase Retrieval: Stability and
Recovery Guarantees.” <em>Appl. Comput. Harmon. Anal.</em> 36 (3):473–94.
<a href="https://doi.org/10.1016/j.acha.2013.08.003">https://doi.org/10.1016/j.acha.2013.08.003</a>.</p>

<p>Frostig, R., R. Ge, S.M. Kakade, and A. Sidford. 2015. “Un-Regularizing:
Approximate Proximal Point and Faster Stochastic Algorithms for
Empirical Risk Minimization.” In <em>Proceedings of the 32nd International
Conference on Machine Learning (ICML)</em>.</p>

<p>Ghadimi, S., and G. Lan. 2013. “Stochastic First- and Zeroth-Order
Methods for Nonconvex Stochastic Programming.” <em>SIAM J. Optim.</em> 23
(4):2341–68. <a href="https://doi.org/10.1137/120880811">https://doi.org/10.1137/120880811</a>.</p>

<p>Güler, O. 1992. “New Proximal Point Algorithms for Convex Minimization.”
<em>SIAM J. Optim.</em> 2 (4):649–64.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/0802032">https://doi-org.offcampus.lib.washington.edu/10.1137/0802032</a>.</p>

<p>Hazan, E., and S. Kale. 2011. “Beyond the Regret Minimization Barrier:
An Optimal Algorithm for Stochastic Strongly-Convex Optimization.” In
<em>Proceedings of the 24th Annual Conference on Learning Theory</em>, edited
by Sham M. Kakade and Ulrike von Luxburg, 19:421–36. Proceedings of
Machine Learning Research. Budapest, Hungary: PMLR.</p>

<p>Johnson, R., and T. Zhang. 2013. “Accelerating Stochastic Gradient
Descent Using Predictive Variance Reduction.” In <em>Proceedings of the
26th International Conference on Neural Information Processing Systems</em>,
315–23. NIPS’13. USA: Curran Associates Inc.
<a href="http://dl.acm.org/citation.cfm?id=2999611.2999647">http://dl.acm.org/citation.cfm?id=2999611.2999647</a>.</p>

<p>Juditsky, A., and Y. Nesterov. 2014. “Deterministic and Stochastic
Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization.”
<em>Stoch. Syst.</em> 4 (1):44–80.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010">https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010</a>.</p>

<p>Lacoste-Julien, S., M. Schmidt, and F. Bach. 2012. “A Simpler Approach
to Obtaining an \({O}(1/t)\) Convergence Rate for the Projected Stochastic
Subgradient Method.” <em>Arxiv arXiv:1212.2002</em>.</p>

<p>Lan, G. 2015. “An Optimal Randomized Incremental Gradient Method.”
<em>arXiv:1507.02000</em>.</p>

<p>Lemarechal, C., J.-J. Strodiot, and A. Bihain. 1981. “On a Bundle
Algorithm for Nonsmooth Optimization.” In <em>Nonlinear Programming, 4
(Madison, Wis., 1980)</em>, 245–82. Academic Press, New York-London.</p>

<p>Lewis, A.S., and S.J. Wright. 2015. “A Proximal Method for Composite
Minimization.” <em>Math. Program.</em> Springer Berlin Heidelberg, 1–46.
<a href="https://doi.org/10.1007/s10107-015-0943-9">https://doi.org/10.1007/s10107-015-0943-9</a>.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. 2015. “A Universal Catalyst for
First-Order Optimization.” In <em>Advances in Neural Information Processing
Systems</em>, 3366–74.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. “Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice.” <em>arXiv preprint arXiv:1712.05654</em> (2017).</p>

<p>Luke, R. 2017. “Phase Retrieval, What’s New?” <em>SIAG/OPT Views and News</em>
25 (1).</p>

<p>Mairal, J. 2015. “Incremental Majorization-Minimization Optimization
with Application to Large-Scale Machine Learning.” <em>SIAM Journal on
Optimization</em> 25 (2):829–55.</p>

<p>Nemirovski, A. 2004. “Prox-Method with Rate of Convergence \(O(1/t)\) for
Variational Inequalities with Lipschitz Continuous Monotone Operators
and Smooth Convex-Concave Saddle Point Problems.” <em>SIAM J. Optim.</em> 15
(1):229–51. <a href="https://doi.org/10.1137/S1052623403425629">https://doi.org/10.1137/S1052623403425629</a>.</p>

<p>Nemirovski, A., A. Juditsky, G. Lan, and A. Shapiro. 2008. “Robust
Stochastic Approximation Approach to Stochastic Programming.” <em>SIAM J.
Optim.</em> 19 (4):1574–1609.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/070704277">https://doi-org.offcampus.lib.washington.edu/10.1137/070704277</a>.</p>

<p>Nemirovsky, A.S., and D.B. Yudin. 1983. <em>Problem Complexity and Method
Efficiency in Optimization</em>. A Wiley-Interscience Publication. John
Wiley &amp; Sons, Inc., New York.</p>

<p>Nesterov, Y., and A. Nemirovskii. 1994. <em>Interior-Point Polynomial
Algorithms in Convex Programming</em>. Vol. 13. SIAM Studies in Applied
Mathematics. Society for Industrial; Applied Mathematics (SIAM),
Philadelphia, PA. <a href="https://doi.org/10.1137/1.9781611970791">https://doi.org/10.1137/1.9781611970791</a>.</p>

<p>Nesterov, Yu. 1983. “A Method for Solving the Convex Programming Problem
with Convergence Rate \(O(1/k^{2})\).” <em>Dokl. Akad. Nauk SSSR</em> 269
(3):543–47.</p>

<p>Nesterov, Yu. 2005. “Smooth Minimization of Non-Smooth Functions.” <em>Math.
Program.</em> 103 (1, Ser. A):127–52.
<a href="https://doi.org/10.1007/s10107-004-0552-5">https://doi.org/10.1007/s10107-004-0552-5</a>.</p>

<p>Nesterov, Yu. 2007. “Modified Gauss-Newton Scheme with Worst Case
Guarantees for Global Performance.” <em>Optim. Methods Softw.</em> 22
(3):469–83. <a href="https://doi.org/10.1080/08927020600643812">https://doi.org/10.1080/08927020600643812</a>.</p>

<p>Nesterov, Yu. 2013. “Gradient Methods for Minimizing Composite Functions.”
<em>Math. Program.</em> 140 (1, Ser. B):125–61.
<a href="https://doi.org/10.1007/s10107-012-0629-5">https://doi.org/10.1007/s10107-012-0629-5</a>.</p>

<p>Paquette, C., H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. 2017. 
“Catalyst Acceleration for Gradient-Based Non-Convex
Optimization.” <em>Preprint arXiv:1703.10993</em>.</p>

<p>Polyak, B.T., and A.B. Juditsky. 1992. “Acceleration of Stochastic
Approximation by Averaging.” <em>SIAM J. Control Optim.</em> 30 (4):838–55.
<a href="https://doi.org/10.1137/0330046">https://doi.org/10.1137/0330046</a>.</p>

<p>Rakhlin, A., O. Shamir, and K. Sridharan. 2012. “Making Gradient Descent
Optimal for Strongly Convex Stochastic Optimization.” In <em>Proceedings of
the 29th International Coference on International Conference on Machine
Learning</em>, 1571–8. ICML’12. USA: Omnipress.
<a href="http://dl.acm.org/citation.cfm?id=3042573.3042774">http://dl.acm.org/citation.cfm?id=3042573.3042774</a>.</p>

<p>Robbins, H., and S. Monro. 1951. “A Stochastic Approximation Method.”
<em>Ann. Math. Statistics</em> 22:400–407.</p>

<p>Schmidt, M., N. Le Roux, and F. Bach. 2013. “Minimizing Finite Sums with
the Stochastic Average Gradient.” <em>arXiv:1309.2388</em>.</p>

<p>Shalev-Shwartz, S., and T. Zhang. 2012. “Proximal Stochastic Dual
Coordinate Ascent.” <em>arXiv:1211.2717</em>.</p>

<p>S. Shalev-Shwartz and T. Zhang. 2015. “Accelerated Proximal Stochastic Dual Coordinate Ascent
for Regularized Loss Minimization.” <em>Mathematical Programming</em>.</p>

<p>Singer, A. 2011. “Angular Synchronization by Eigenvectors and
Semidefinite Programming.” <em>Appl. Comput. Harmon. Anal.</em> 30 (1):20–36.
<a href="https://doi.org/10.1016/j.acha.2010.02.001">https://doi.org/10.1016/j.acha.2010.02.001</a>.</p>

<p>Sun, J., Q. Qu, and J. Wright. 2017. “A Geometric Analysis of Phase
Retrieval.” <em>To Appear in Found. Comp. Math., arXiv:1602.06664</em>.</p>

<p>Woodworth, B.E., and N. Srebro. 2016. “Tight Complexity Bounds for
Optimizing Composite Objectives.” In <em>Advances in Neural Information
Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, 3639–47. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf">http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf</a>.</p>

<p>Wright, S.J. 1997. <em>Primal-Dual Interior-Point Methods</em>. Society for
Industrial; Applied Mathematics (SIAM), Philadelphia, PA.
<a href="https://doi.org/10.1137/1.9781611971453">https://doi.org/10.1137/1.9781611971453</a>.</p>

<p>Xiao, L., and T. Zhang. 2014. “A Proximal Stochastic Gradient Method
with Progressive Variance Reduction.” <em>SIAM J. Optim.</em> 24 (4):2057–75.
<a href="https://doi.org/10.1137/140961791">https://doi.org/10.1137/140961791</a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>To ensure that \({\rm prox}_{\nu f}(\cdot)\) is nonempty, it
suffices to assume that \(f\) is bounded from below. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/01/24/proximal-point/</guid>
                <description>
                    
                    Revisiting the proximal point method. Introduction and Notation.
                    
                </description>
                <pubDate>Wed, 24 Jan 2018 16:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>k-server problem</title>
                <link>http://localhost:4000/blog/2018/01/10/kserver/</link>
                <content:encoded>
                    <![CDATA[
                    <p>The \(k\)-server problem is one of the most important and most well-studied problems in the field of online algorithms. The goal of this blog is to describe the problem to you and give you some flavor of the underlying techniques used in the recent breakthrough by <a href="https://arxiv.org/abs/1711.01085">Sebastien Bubeck, Michael Cohen, James Lee, Aleksander Madry and me</a>. (Note that three of them are members of ADSI!) For a detailed blog with proofs, please see <a href="https://blogs.princeton.edu/imabandit/2017/12/16/k-server-part-1-online-learning-and-online-algorithms/">here</a>.</p>

<h2 id="background">Background</h2>

<p>In this problem, we control the movement of a set of \(k\) servers on a fixed metric space of \(n\) vertices. In each iteration, we are
given a request \(r\). If there is no server at that location, we must choose a server, move it there and pay the movement cost of this server. Our goal is to minimize the total distance of all servers’ moves. This problem is very general and is originally proposed to model problems related to cache management.</p>

<p>We call an algorithm \(f(k,n)\)-competitive if for any request sequence, its total movement is at most \(f(k,n)\cdot\text{OPT}\) where \(\text{OPT}\) is the optimal movement cost if the whole request sequence is given in the beginning. A priori, it is unclear that any competitive algorithm exists because we are comparing against a powerful algorithm that knows the whole request sequence and is free to make any moves.</p>

<p>To illustrate the learning aspect, consider a \(2\)-server problem on a graph with 3 vertices \(\{a_{1},a_{2},b\}\). Suppose that \(d(a_{1},a_{2})=1\), \(d(a_{i},b)=2017\) for \(i=1,2\) and the request is</p>

\[a_{1},b,a_{2},b,a_{1},b,a_{2},b,\cdots\cdots.\]

<p>Since \(a_{i}\) and \(b\) are so far away, the optimal strategy for this sequence is to put a server on \(a_{i}\) and a server on \(b\). However, if \(b\) appears less frequently than once per \(2017\) iterations, a better strategy would put both servers on \(a_{i}\) most of the time. Therefore, any memoryless algorithm is not competitive and that we indeed need to learn something.</p>

<p>One main focus for the \(k\)-server problem is to achieve competitive ratio independent of \(n\) because of the common case \(k\ll n\). Two of the major open problems is to find a <a href="https://en.wikipedia.org/wiki/K-server_problem">\(k\)-competitive deterministic algorithm</a> and a \(O(\log k)\)-competitive randomized algorithm for the \(k\)-server problem. For the deterministic problem, <a href="https://doi.org/10.1145/210118.210128">Koutsoupias and Papadimitriou</a> gave a deterministic algorithm that achieves a competitive ratio of \(2k-1\) on any metric space. However, there has not been too much progress on the randomize problem. In particular, there is no \(o(k)\) competitive algorithm except for simple classes of graphs such as weighted complete graphs. Due to its importance and the gap between the lower and upper bound, the following “easier” problem had been a major target of the field. And in my opinion, this was the most important conjecture in online algorithms.</p>

<blockquote>
  <p><strong>(Weak randomized \(k\)-server conjecture)</strong> There is a randomized algorithm for the \(k\)-server problem on any graph with competitive ratio \(\log^{O(1)}(k)\).</p>
</blockquote>

<p>The previous best competitive ratio is \(O(\log^{3}(n)\log^{2}(k))\) (by <a href="https://arxiv.org/abs/1110.1580v1">Bansal, Buchbinder, Naor and Madry in 2011</a>). In our paper, we give an algorithm with competitive ratio \(O(\log^{2}(k))\) on hierarchically separated trees (HST, a much larger classes of graphs) and as a corollary \(O(\log^{2}(k)\log(n))\) on general graph. Soon after our paper,
James Lee developed upon our paper and gave an algorithm with competitive ratio \(O(\log^{6}(k))\), finally resolving the weak randomized \(k\)-server conjecture!</p>

<h2 id="online-learning-and-mirror-descent">Online Learning and Mirror Descent</h2>

<p>Our algorithm is based on mirror descent with a multiscale entropy. So, let me describe an online learning problem and the mirror descent algorithm for it.</p>

<p>In this problem, we are given a convex set \(K\). In the \(k^{th}\) iteration, we select a vector \(x^{(k)}\in K\), then the adversary selects a vector \(v^{(k)}\) in \(K\) and we receive a loss \(v^{(k)\top}x^{(k)}\) for that iteration. Our goal is to minimize the regret (the difference between your loss and the loss of the optimal fixed strategy)</p>

\[\sum_{k=1}^{T}v^{(k)\top}x^{(k)}-\min_{x\in K}\sum_{k=1}^{T}v^{(k)\top}x.\]

<p>This problem can be solved by mirror descent:</p>

\[x^{(k+1)}=\text{argmin}_{x\in K}\eta\cdot v^{(k)\top}x^{(k)}+D_{\Phi}(x;x^{(k)})\]

<p>where \(\eta\) is the step size, \(\Phi\) is a convex function on \(K\) called mirror map and the Bregman divergence associated to \(\Phi\)
defined by</p>

\[D_{\Phi}(y;x):=\Phi(y)-\Phi(x)-\nabla\Phi(x)^{\top}(y-x).\]

<p>When \(x\) is very close to \(y\), \(D_{\Phi}(y;x)\sim(y-x)^{\top}\nabla^{2}\Phi(x)(y-x)\) and hence mirror descent is simply moving \(x\) towards \(-v\) direction while making sure the point is in \(K\) and it is not too far from the previous point in \(\nabla^{2}\Phi(x)\) norm.</p>

<p>For me, a general wisdom, when facing a new learning problem, is to check if mirror descent or some of its variant is good. See my favorite example <a href="https://arxiv.org/abs/1607.03084">here</a>.</p>

<h2 id="weighted-complete-graph">Weighted Complete Graph</h2>

<p>Unfortunately, applying mirror descent to the \(k\)-server problem is not as easy as picking a good mirror map as I wished. Let me first describe the algorithm for the complete graphs with the metric of the form \(d(i,j)=\omega_{i}+\omega_{j}\). For this and many other graphs, it is known how to turn a fractional solution \(x(t)\in\mathbb{R}^{n}\) that is feasible</p>

\[0\leq x_{i}(t)\leq1 \text{ and } \sum x_{i}(t)=k\]

<p>to an integral solution \(\widetilde{x}\in\{0,1\}^{n}\) with the movement cost bounded by \(O(1)\cdot\int_{0}^{T}\omega_{i}\left|\frac{dx_{i}}{dt}\right|dt\) (a natural continuous definition of movement cost). Therefore, it
suffices to propose a fractional algorithm (the first prerequisite for applying mirror descent).</p>

<p>Our algorithm is motivated by the \(O(\log k)\) competitive-algorithm by <a href="http://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf">Bansal, Buchbinder and Naor in 2007</a>. To make its relation to mirror descent clear, I describe our process
here as a discrete process with an infinitesimally small step size \(\eta\). Instead of working on the fractional server \(x(t)\), our algorithm is defined on the fractional anti-server \(y(t):=1-x(t)\):</p>

<ul>
  <li>Let \(K=\{0\leq y\leq1,\sum_{i=1}^{n} y_{i}=n-k\}\).</li>
  <li>Let \(\Phi(y)=\sum_{i=1}^{n}\omega_{i}(y_{i}+\frac{1}{2k})\log(y_{i}+\frac{1}{2k})\).</li>
  <li>When the request for the vertex \(\ell\) arrives
    <ul>
      <li>While \(y_{\ell}^{(k)}&gt;0\).
        <ul>
          <li>\(y^{(k+1)}=\text{argmin}_{y\in K}\eta\cdot e_{\ell}^{\top}y+D_{\Phi}(y;y^{(k)})\) where \(e_{\ell}\) is the coordinate vector at \(\ell\).</li>
          <li>\(k \leftarrow k+1\).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>In short, when the request arrives at \(\ell\), we run the mirror descent with the cost \(e_{\ell}\) until all anti-mass \(y\) leaves the coordinate \(\ell\).</p>

<p>The reason of using \(e_{\ell}\) is to put a “cost” at coordinate \(\ell\) that forces all anti-mass at \(\ell\) leaves (equivalently, attracting servers to move to \(\ell\)). Since \(K\) is a simplex, the standard choice of the mirror map is \(\sum_{i}\omega_{i}y_{i}\log y_{i}\). However, this mirror map is not suitable because its gradient \((1+\log y_{i})_{i}\)
blows up on the boundary. Following the idea in <a href="http://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf">(BBN07)</a>, we shift all variables by \(\Theta(1/k)\) in the mirror map.</p>

<p>Since the step size is infinitesimally small,</p>

\[D_{\Phi}(y;y^{(k)}) = (y-y^{(k)})^{\top}\nabla^{2}\Phi(y^{(k)})(y-y^{(k)}) =\sum_{i}\frac{\omega_{i}}{y_{i}^{(k)}+\frac{1}{2k}}(y-y^{(k)})^{2}.\]

<p>Using this, one can show that the algorithm is moving the anti-mass \(y\) from coordinate \(\ell\) to coordinate \(i\) with a rate proportionally to \((y_{i}^{(k)}+\frac{1}{2k})/\omega_{i}\). Namely, the algorithm tends to move the server from vertices with smaller weight and less server mass to the request. One can show this algorithm has a \(O(\log k)\) competitive ratio.</p>

<h2 id="hierarchically-separated-tree">Hierarchically Separated Tree</h2>

<p>In general, it suffices to solve the \(k\)-server problem on HST metrics <a href="https://arxiv.org/abs/1110.1580">(BBNM11)</a>. This reduction would cost a \(\log{n}\) factor in the competitive ratio. Given a tree \(T=(V,E)\) with vertex weights \(\omega&gt;0\), we define the metric on the leaf \(\mathcal{L}\) by \(d_{\omega}(\ell,\ell')=\omega_{\text{lca}(\ell,\ell')}\) where \(\text{lca}(\ell,\ell')\) is the least common ancestor of \(\ell\) and \(\ell'\). We call this tree is a HST if \(\omega_{v}\leq\omega_{u}/6\) whenever \(v\) is a child of \(u\) and we call the metric space \((\mathcal{L},d_{\omega})\) a HST metric.</p>

<p>Two main questions to be decided are:</p>
<ol>
  <li>How do we represent the solution?</li>
  <li>What is the mirror map we use?</li>
</ol>

<p>One natural choose is to represent anti-mass \(y\) using</p>

\[B:=\{y\in[0,1]^{V}:\ y_{r}=n-k\text{ and } y_{u}=\sum_{p(v)=u}y_{v}\}\]

<p>where \(r\) is the root of the tree and \(p(v)\) is the parent of \(v\). The main issue of this representation is that it cannot distinguish between the case we need exactly 1 server in a subtree or we need 2 servers with 50% probability. Another small issue is that some constraint is never active. Using the fact that the algorithm is always moving servers to the request until \(y_{\ell}=0\), one can show that \(1\geq y\geq0\) and \(y_{u} \geq \sum_{p(v)=u}y_{v}\) are never active.</p>

<p>Fixing these issues, we have a new representation</p>

\[K:=\{y:\ y_{i,r}=1_{i&gt;k},  \qquad \qquad \qquad \qquad
\\
\qquad \qquad \sum_{i\leq\left|S\right|}x_{u,i}\leq\sum_{(v,j)\in S}x_{v,j} \  \forall S  \}.\]

<p>where \(S\) are sets of pairs \((v,j)\) with \(p(v)=u\).
The first set of constraints ensures that there are only \(k\) servers in total and the second set of constraints ensures that the number of servers in
the children of a vertex \(u\) is at most the number of server at \(u\).</p>

<p>The mirror map we pick is a generalization of the mirror map on a simplex</p>

\[\Phi(y):=\sum_{u\in V}\omega_{u}\sum_{i\geq1}(y_{u,i}+\delta)\log(y_{u,i}+\delta)\]

<p>where the shift \(\delta=\frac{1}{2017k}\).</p>

<p>Except for some technical issues, our algorithm for HST is the same as the algorithm described for the complete graph except this new \(K\) and mirror map \(\Phi\).</p>

<h2 id="open-problems">Open Problems</h2>

<p>The proof of the classical mirror descent is clean, short and optimal. Unfortunately, our proof is slightly longer (i.e. few pages) and the bound we get for HST does not sound optimal. We hoped to get the ultimate algorithm for \(k\)-server at least for HST, however, our algorithm seems to not be the one from the BOOK. So, what is the algorithm from the BOOK for the \(k\)-server problem (at least for HST)?</p>

<p>On the other hand, it is interesting to see if HST is necessary for an \(\log^{O(1)}k\)-competitive algorithm on general graphs (or for a path). Maybe I am too naive on this and HST is indeed the right tool?</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/blog/2018/01/10/kserver/</guid>
                <description>
                    
                    Recent breakthrough of the k-server problem on HST
                    
                </description>
                <pubDate>Wed, 10 Jan 2018 16:00:00 -0800</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>Simons Workshop on Fast Iterative Methods in Optimization</title>
                <link>http://localhost:4000/news/2017/10/01/Simons/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Fazel was a co-organizer of <a href="https://simons.berkeley.edu/workshops/optimization2017-2">the Simons Workshop on Fast Iterative Methods in Optimization</a>, held on Oct 2-6, 2017 at the Simons Institute, UC Berkeley.  This workshop was part of the semester-long program on Bridging Continuous and Discrete Optimization held Aug-Dec 2017, and Fazel was also a long-term participant of the program, which aimed to bring together experts from these two sides of the broader optimization community.</p>

<p>Recently, a confluence of ideas from optimization and theoretical computer science led to breakthroughs in terms of new understanding and running time bound improvements for some classic iterative continuous optimization primitives. This workshop explored these advances as well as new directions that they have opened up. Some of the specific topics were advanced first-order methods (non-smooth optimization, regularization, preconditioning), structured optimization, fast LP/SDP solvers, advances in interior point methods and fast streaming/sketching techniques. One highlighted theme was how combining the continuous and discrete points of view can often achieve near-optimal running time bounds.</p>

<p>Lee gave a talk at this workshop, Drusvyatskiy was one of the attendants, and several ADSI affiliated students also attended.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2017/10/01/Simons/</guid>
                <description>
                    
                    An optimization workshop co-organized by ADSI PI
                    
                </description>
                <pubDate>Sun, 01 Oct 2017 17:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
        
            <item>
                <title>NSF Tripods Award</title>
                <link>http://localhost:4000/news/2017/08/23/TRIPODS/</link>
                <content:encoded>
                    <![CDATA[
                    <p>A team of University of Washington researchers co-led by <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a>, a professor in the Allen School and Department of Statistics, and Electrical Engineering Professor <a href="http://faculty.washington.edu/mfazel/">Maryam Fazel</a> have secured a <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=242888">$1.5 million</a> award from the National Science Foundation (NSF) to develop new algorithmic tools that will advance the state of the art in data science. They are joined on the project by three co-principal investigators: Mathematics Professor <a href="https://sites.math.washington.edu/~ddrusv/">Dmitriy Drusvyatskiy</a>, Statistics Professor <a href="http://faculty.washington.edu/zaid/">Zaid Harchaoui</a>, and Allen School Professor <a href="http://yintat.com/">Yin Tat Lee</a>.</p>

<p>The funding will support ADSI (Algorithmic Foundations of Data Science Institute) at UW, as part of the agency’s Transdisciplinary Research in Principles of Data Science (TRIPODS) program. <a href="https://www.nsf.gov/funding/pgm_summ.jsp?pims_id=505347">TRIPODS</a> was designed to engage members of the theoretical computer science, mathematics and statistics communities in developing the theoretical foundations of data science to promote data-driven discovery. ADSI aims to produce a common language and set of design principles to guide the development of new algorithmic tools that will automate the process of extracting robust insights from vast troves of data.</p>

<p>Link to the <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=242888">NSF announcement</a>, <a href="https://news.cs.washington.edu/2017/08/24/uws-sham-kakade-and-maryam-fazel-earn-nsf-tripods-award-to-advance-the-state-of-the-art-in-data-science/">UW Press release</a>.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/news/2017/08/23/TRIPODS/</guid>
                <description>
                    
                    UW faculty win NSF Tripods Award to establish a new institute advancing Algorithms for Data Science
                    
                </description>
                <pubDate>Wed, 23 Aug 2017 17:00:00 -0700</pubDate>
                <author>Sham Kakade</author>
            </item>
        
    
  </channel>
</rss>
