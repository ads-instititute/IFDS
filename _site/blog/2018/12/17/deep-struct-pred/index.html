<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Training Deep Structured Prediction Models at Scale &#8211; UW Institute on the Algorithmic Foundations of Data Science</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This post discusses the use of smoothing and accelerated incremental algorithms for faster training of structured prediction models">
    <meta name="robots" content="all">
    <meta name="author" content="Sham Kakade">
    
    <meta name="keywords" content="blog">
    <link rel="canonical" href="http://localhost:4000/blog/2018/12/17/deep-struct-pred/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for UW Institute on the Algorithmic Foundations of Data Science" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202011120026" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    
    <script type="text/javascript" async
        src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Training Deep Structured Prediction Models at Scale">
    <meta property="og:description" content="">
    <meta property="og:url" content="http://localhost:4000/blog/2018/12/17/deep-struct-pred/">
    <meta property="og:site_name" content="UW Institute on the Algorithmic Foundations of Data Science">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Training Deep Structured Prediction Models at Scale" />
    <meta name="twitter:description" content="This post discusses the use of smoothing and accelerated incremental algorithms for faster training of structured prediction models" />
    <meta name="twitter:url" content="http://localhost:4000/blog/2018/12/17/deep-struct-pred/" />
    

    <!-- Icons -->
    <!-- <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> -->

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#ffffff">

    
</head>

<body class="site">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <div style="width:100%">
      <a href="/"><img src="/images/ifds.png" style="width:30%;float:left;"></a>
      <a href="https://nsf-tripods.org/"><img src="/images/NSF.gif" style="width:15%;margin-left:12.5%;float:left;"></a>
      <a href="http://escience.washington.edu/"><img src="/images/eScience.png" style="width:30%;float:right;"></a>
        <div style="clear: both;"></div>
      </div>
      
      <nav class="site-nav">
        



    
    
    
    

    

    
    
    
    
        <a href="/about.html">About</a>
    

    

    
    
    
    
        <a href="/index.html">News</a>
    

    

    
    
    
    
        <a href="/publications.html">Publications</a>
    

    

    
    
    
    
        <a href="/seminars.html">Seminars</a>
    

    

    
    
    
    
        <a href="/courses.html">Courses</a>
    

    

    
    
    
    
        <a href="/members.html">Members</a>
    

    

    
    
    
    
        <a href="/blog.html">Blog</a>
    

    



<div class="dropdown">
  <button class="dropbtn">Workshops</button>
  <div class="dropdown-content">
    <a href="https://alecgt.github.io/adsi_summer/" style="margin-left:0em;"><font size=2px>ADSI Summer School</font></a>
    <a href="https://ajwagen.github.io/adsi_learning_and_control/" style="margin-left:0em;"><font size=2px>ADSI Workshop</font></a>
  </div>
</div>


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
 
  
  <a href="https://papers.nips.cc/paper/7726-a-smoother-way-to-train-structured-prediction-models" style="text-decoration: none" class="post-link">
  
    <img src="/images/201812_smoother/viterbi-K.png" style="width:30%;float:right;">
  
  </a>
  
 
  
  
  <h1>Training Deep Structured Prediction Models at Scale</h1>
  <span class="post-meta">Krishna Pillutla, Vincent Roulet. Dec 17, 2018.</span><br>
  
  <span class="post-meta small">
  
    6 minute read
  
  </span>
</div>

<article class="post-content">
  <p>This blog post decribes the 
<a href="https://papers.nips.cc/paper/7726-a-smoother-way-to-train-structured-prediction-models">recent NeurIPS 2018 paper</a> 
and <a href="https://github.com/krishnap25/casimir">companion code</a> on smooth training of max-margin structured prediction models. Training structured prediction models consists in optimizing a non-smooth objective using an inference combinatorial optimization algorithm.</p>

<p>We propose a framework called <a href="https://github.com/krishnap25/casimir">Casimir</a> based on the Catalyst acceleration and infimal-convolution smoothing allowing us to break the non-smoothness barrier and obtain fast incremental algorithms for large-scale training of deep structured prediction models.</p>

<h2 id="setting">Setting</h2>
<p>Structured prediction consists in predicting complex outputs such as sequences, trees or lattices. For instance, named entity recognition can be cast as the task of predicting a sequence of tags, one for each word which identifies the word as a named entity.</p>

<!-- ![NER](http://ads-institute.uw.edu/images/201812_smoother/ner.png) -->
<p align="center">
  <img src="http://ads-institute.uw.edu/images/201812_smoother/ner.png" />
</p>

<p>In this example, an output is a chain of tags, where each tag can take values from a dictionary. In general, the set \(\mathcal{Y}\) of all outputs is finite but too large to enumerate.
To overcome this difficulty, a <em>score function</em> \(\phi(\cdot, \cdot; w)\), parameterized by \(w\), is defined to measure the compatibility of the input-output pair \((x, y)\) as \(\phi(x,y;w)\). This score function decomposes over the structure of the outputs (e.g., a chain) so that predictions can be made by an inference procedure which finds</p>

\[y^*(x ; w) \in \operatorname*{arg max}_{y \in \mathcal{Y}} \phi(x, y ; w) \,.\]

<p>The inference problem can be solved in various settings of interest by efficient combinatorial algorithms such as the Viterbi algorithm for named entity recognition.</p>

<p>The goal of the <em>learning problem</em> is to find the best parameter \(w\) so that inference \(y^*(x ; w)\) produces the correct output. Given a loss \(\ell\) such as the Hamming loss, max-margin structured prediction aims to minimize a surrogate called the structural hinge loss, which is defined for an input-output pair \((x_i, y_i)\) as</p>

\[f_i(w) = \max_{y \in \mathcal{Y}} \psi_i(y; w)  \,,\]

<p>where \(\psi_i\) is defined as a generalization of the margin used in classical support vector machine,</p>

\[\psi_i(y; w) = \phi(x_i, y ; w) + \ell(y_i, y) - \phi(x_i, y_i ; w) \,.\]

<p>The <em>optimization problem</em> is defined for samples \(\{(x_i, y_i)\}_{i=1}^n\) as the regularized empirical surrogate risk minimization</p>

\[\min_w \left[ F(w) = \frac{1}{n}\sum_{i=1}^n f_i(w) + \frac{\lambda}{2}\|w\|_2^2 \right] \,.\]

<p>The subgradient of \(f_i\) is computed by running the inference procedure as</p>

\[\partial f_i(w) \in \operatorname*{arg max}_{y \in \mathcal{Y}} \psi_i(y ; w) \,.\]

<p>Though the above formulation allows one to use tractable first-order information through combinatorial procedures, its non-smoothness prevents us from using fast incremental optimization algorithms. We overcome this challenge by blending an extrapolation scheme for acceleration and an adaptive smoothing scheme.</p>

<h2 id="smoothing">Smoothing</h2>
<p>We now wish to smooth the objective function \(F\) in order to apply incremental algorithms for smooth optimization. This is not straightforward because each \(f_i\) is computed by a discrete inference algorithms.</p>

<p>To smooth \(f_i\), we first note that it can be written as the composition \(f_i = h \circ g_i\), where</p>

\[g_i(w) =  \big(\psi_i(y;w) \big)_{y \in \mathcal{Y}} \,,
\quad \text{and} \quad 
h(z) =  \max_{i \in |\mathcal{Y}|} z_i.\]

<p>The non-smooth max function \(h\) is simply smoothed by adding a strongly convex function \(\omega\) to its dual formulation as</p>

\[h_{\mu\omega}(w) = \max_{u \in \Delta^{|\mathcal{Y}|}} \{
	z^\top u - \mu \omega(u) \} \,,\]

<p>where \(\Delta^m\) is the simplex in \(\mathbb{R}^m\).
It can be shown that \(h_{\mu\omega}\) is a smooth approximation of \(h\) upto \(O(\mu)\) [<a href="https://link.springer.com/article/10.1007/s10107-004-0552-5">N05</a>,<a href="https://epubs.siam.org/doi/abs/10.1137/100818327">BT12</a>]. Common choices of \(\omega\) are given below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Smoothing type</strong></th>
      <th style="text-align: center">\(\omega(u)\)</th>
      <th style="text-align: center"><strong>Smoothing computation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">entropy</td>
      <td style="text-align: center">\(H(u) = \langle u, \log u \rangle\)</td>
      <td style="text-align: center">log-sum-exp</td>
    </tr>
    <tr>
      <td style="text-align: center">\(\ell_2^2\)</td>
      <td style="text-align: center">\(\ell_2^2(u) = \tfrac{1}{2}|u|^2_2\)</td>
      <td style="text-align: center">projection on simplex</td>
    </tr>
  </tbody>
</table>

<p>The smooth structural hinge loss is obtained by replacing the non-smooth \(h\) with its smooth counterpart as</p>

\[f_{i, \mu \omega} = h_{\mu\omega} \circ g_i.\]

<p>In the structured prediction setting, entropy smoothing is equivalent to a conditional random field
[<a href="https://dl.acm.org/citation.cfm?id=655813">LMP01</a>], which is only tractable for tree structured outputs.
On the other hand, the sparse outputs of \(\ell_2^2\) smoothing can be well approximated by picking a small integer \(K\) and considering the top-\(K\) highest scoring outputs. This makes \(\ell_2^2\) smoothing more feasible for tree structured outputs as well as select loopy output structures. See the illustration below for the example of named entity recognition.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://ads-institute.uw.edu/images/201812_smoother/viterbi-max.png" alt="nonsmooth" /></th>
      <th style="text-align: center"><img src="http://ads-institute.uw.edu/images/201812_smoother/viterbi-K.png" alt="l2 smoothing" /></th>
      <th style="text-align: center"><img src="http://ads-institute.uw.edu/images/201812_smoother/viterbi-exp.png" alt="entropy" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Non-smooth</em></td>
      <td style="text-align: center"><em>\(\ell_2^2\) smoothing</em></td>
      <td style="text-align: center"><em>entropy smoothing</em></td>
    </tr>
  </tbody>
</table>

<p>Formally, we define inference oracles, namely the max, top-\(K\) and exp oracles as first order oracles for the structural hinge loss and its smoothed variants with \(\ell_2^2\) and entropy smoothing respectively. 
This allows us to measure the complexity of optimization algorithms, which we discuss next.
The table below shows how the smooth inference oracles are implemented for a given max oracle.
Their computational complexity is given in terms of \(\mathcal{T}\), the cost of max oracle and \(p\), the size of each output \(y\).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Max oracle</strong></th>
      <th style="text-align: center"><strong>Top-\(K\) oracle</strong></th>
      <th style="text-align: center"><strong>Exp oracle</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Max-product</td>
      <td style="text-align: center">Top-\(K\)  max-product, \(\widetilde O(K\mathcal{T})\) time</td>
      <td style="text-align: center">Sum-product, \(O(\mathcal{T})\) time</td>
    </tr>
    <tr>
      <td style="text-align: center">Graph cut</td>
      <td style="text-align: center">BMMF, \(O(pK \mathcal{T})\) time</td>
      <td style="text-align: center">Intractable</td>
    </tr>
    <tr>
      <td style="text-align: center">Graph matching</td>
      <td style="text-align: center">BMMF, \(O(K \mathcal{T})\) time</td>
      <td style="text-align: center">Intractable</td>
    </tr>
    <tr>
      <td style="text-align: center">Branch and bound search</td>
      <td style="text-align: center">Top-\(K\) search</td>
      <td style="text-align: center">Intractable</td>
    </tr>
  </tbody>
</table>

<p>Here, BMMF is the Best max marginal first algorithm of [<a href="https://www.semanticscholar.org/paper/Finding-the-M-Most-Probable-Configurations-Using-Yanover-Weiss/1c38cd37dc563b5182aa49f3f4a735a10caa5daa">YW03</a>].</p>

<h2 id="optimization-algorithms">Optimization algorithms</h2>
<p>The optimization algorithms depend on the nature of the map \(w \mapsto \phi(x, y ; w)\). The structural hinge loss, and thus \(F\), are convex if this map is linear. Otherwise, \(F\) could be nonconvex in general.</p>

<h3 id="convex-structured-prediction">Convex structured prediction</h3>
<p>Classical structured prediction [<a href="https://www.aaai.org/Papers/ICML/2003/ICML03-004.pdf">ATH03</a>, <a href="https://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf">TGK04</a>] uses a linear score \(\phi(x, y;w)=w^\top \Phi(x, y)\) where \(\Phi(x, y)\) is a hand-engineered feature map.
The objective \(F\) is convex here and we extend the generic Catalyst acceleration scheme [<a href="http://www.jmlr.org/papers/volume18/17-748/17-748.pdf">LMH18</a>] to optimize nonsmooth objectives by using smooth surrogates. See also <a href="http://ads-institute.uw.edu//blog/2018/02/06/catalyst/">this blog post</a> for a review. In particular, each iteration considers smoothed and regularized surrogates of the form</p>

\[F_{\mu, \kappa}(w ; z) = \frac{1}{n}\sum_{i=1}^n f_{i, \mu}(w) + \frac{\lambda}{2}\|w\|^2_2 + \frac{\kappa}{2}\|w-z\|^2_2 \,.\]

<h4 id="algorithm">Algorithm</h4>
<p>Starting at \(z_0 = w_0\), at each step \(k\), do</p>

<ul>
  <li>Approximately solve using a linearly convergent algorithm \(\mathcal{M}\):</li>
</ul>

\[w_{k+1} \approx \operatorname*{arg min}_w F_{\mu_k, \kappa_k}(w;z_k)\]

<ul>
  <li>Extrapolate to get</li>
</ul>

\[z_{k+1} = w_k + \beta_k (w_{k+1} - w_{k})\]

<p>When using, for instance, SVRG [<a href="https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf">JZ15</a>] as the linearly convergent algorithm \(\mathcal{M}\), we are guaranteed to get approximate solution \(F(w_k)-F^* \leq \epsilon\) after \(N\) iterations where</p>

\[\mathbb{E}(N) = 
\begin{cases} O \left(n + \sqrt{\frac{n}{\lambda \epsilon}} \right), \quad  \mbox{if fixed smoothing}\\
O \left(n + {\frac{1}{\lambda \epsilon}} \right), \quad  \mbox{if adaptive smoothing}
\end{cases}\]

<h3 id="deep-structured-prediction">Deep structured prediction</h3>
<p>More broadly, deep structured prediction attempts to learn the feature map \(\Phi\). The score function \(\phi(x, y ; w) = w_2^\top \Phi(x, y; w_1)\) is nonlinear in \(w = (w_1, w_2)\) so that \(F\) is nonconvex. In this case, the prox-linear algorithm [<a href="https://link.springer.com/article/10.1007/BF01584377">B85</a>,<a href="https://link.springer.com/article/10.1007/s10107-018-1311-3">DP18</a>] is applicable. It was described previously <a href="http://ads-institute.uw.edu//blog/2018/01/31/prox-linear/">here</a>. This amounts to considering a linear approximation to \(\psi\) as</p>

\[\psi_i(y ; w, z) = \psi_i(y ; z) + \nabla_z \psi_i(y ; z)(w-z) 
\quad \text{and} \quad f_i(w ; z) = \max_{y \in \mathcal{Y}} \psi_i(y ; w, z) \,,\]

<p>to get a regularized convex model</p>

\[F_\gamma(w ; z) = \frac{1}{n}\sum_{i=1}^n f_i(w ; z) + \frac{\lambda}{2}\|w\|_2^2  + \frac{1}{2\gamma} \|w-z\|_2^2 \,.\]

<p>The convex sub-problem above can then be optimized by the convex optimization algorithm described earlier.</p>

<h2 id="numerical-experiments">Numerical experiments</h2>
<p>Given below are results of numerical experiments for named entity recognition on CoNLL-2003 dataset and visual object localization on 
the PASCAL VOC dataset. 
We first show the performance of the proposed algorithm for the convex case, where the feature maps are predefined.
Casimir-SVRG-const and Casimir-SVRG-adapt are the two variants with constant and adaptive smoothing respectively.</p>

<p><img src="http://ads-institute.uw.edu/images/201812_smoother/ner_cvx.png" alt="ner" />
<img src="http://ads-institute.uw.edu/images/201812_smoother/loc_cvx.png" alt="localization" /></p>

<p>Next, we consider deep structured prediction where the feature map is learnt using a convolutional neural network.</p>

<p><img src="http://ads-institute.uw.edu/images/201812_smoother/loc_ncvx.png" alt="localization" /></p>

<p>A <em>software package</em> called <a href="https://github.com/krishnap25/casimir"><strong>Casimir</strong></a> implementing all these algorithms and more is available 
<a href="https://homes.cs.washington.edu/~pillutla/documentation/casimir/">here</a>.</p>


</article>













      </div>
    </div>
  </div>

  <footer class="center">
</footer>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112084371-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112084371-1');
</script>

</body>
</html>
