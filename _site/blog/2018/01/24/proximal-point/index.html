<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>The proximal point method revisited, episode 0. Introduction &#8211; UW Institute on the Algorithmic Foundations of Data Science</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Revisiting the proximal point method. Introduction and Notation.">
    <meta name="robots" content="all">
    <meta name="author" content="Sham Kakade">
    
    <meta name="keywords" content="blog">
    <link rel="canonical" href="http://localhost:4000/blog/2018/01/24/proximal-point/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for UW Institute on the Algorithmic Foundations of Data Science" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202011120026" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    
    <script type="text/javascript" async
        src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="The proximal point method revisited, episode 0. Introduction">
    <meta property="og:description" content="">
    <meta property="og:url" content="http://localhost:4000/blog/2018/01/24/proximal-point/">
    <meta property="og:site_name" content="UW Institute on the Algorithmic Foundations of Data Science">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="The proximal point method revisited, episode 0. Introduction" />
    <meta name="twitter:description" content="Revisiting the proximal point method. Introduction and Notation." />
    <meta name="twitter:url" content="http://localhost:4000/blog/2018/01/24/proximal-point/" />
    

    <!-- Icons -->
    <!-- <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> -->

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#ffffff">

    
</head>

<body class="site">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <div style="width:100%">
      <a href="/"><img src="/images/ifds.png" style="width:30%;float:left;"></a>
      <a href="https://nsf-tripods.org/"><img src="/images/NSF.gif" style="width:15%;margin-left:12.5%;float:left;"></a>
      <a href="http://escience.washington.edu/"><img src="/images/eScience.png" style="width:30%;float:right;"></a>
        <div style="clear: both;"></div>
      </div>
      
      <nav class="site-nav">
        



    
    
    
    

    

    
    
    
    
        <a href="/about.html">About</a>
    

    

    
    
    
    
        <a href="/index.html">News</a>
    

    

    
    
    
    
        <a href="/publications.html">Publications</a>
    

    

    
    
    
    
        <a href="/seminars.html">Seminars</a>
    

    

    
    
    
    
        <a href="/courses.html">Courses</a>
    

    

    
    
    
    
        <a href="/members.html">Members</a>
    

    

    
    
    
    
        <a href="/blog.html">Blog</a>
    

    



<div class="dropdown">
  <button class="dropbtn">Workshops</button>
  <div class="dropdown-content">
    <a href="https://alecgt.github.io/adsi_summer/" style="margin-left:0em;"><font size=2px>ADSI Summer School</font></a>
    <a href="https://ajwagen.github.io/adsi_learning_and_control/" style="margin-left:0em;"><font size=2px>ADSI Workshop</font></a>
  </div>
</div>


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
 
  
  <a href="http://sites.math.washington.edu/~ddrusv/" style="text-decoration: none" class="post-link">
  
    <img src="/images/prox.png" style="width:30%;float:right;">
  
  </a>
  
 
  
  
  <h1>The proximal point method revisited, episode 0. Introduction</h1>
  <span class="post-meta">Dmitriy Drusvyatskiy. Jan 24, 2018.</span><br>
  
  <span class="post-meta small">
  
    15 minute read
  
  </span>
</div>

<article class="post-content">
  <h1 id="introduction">Introduction</h1>

<p>The proximal point method is a conceptually simple algorithm for
minimizing a function \(f\) on \({\mathbb R}^d\). Given an iterate \(x_t\),
the method defines \(x_{t+1}\) to be any minimizer of the proximal
subproblem</p>

\[\underset{x}{\operatorname{argmin}}~\left\{f(x)+\tfrac{1}{2\nu}\|x-x_t\|^2\right\},\]

<p>for an appropriately chosen parameter \(\nu&gt;0\). At first glance, each proximal
subproblem seems no easier than minimizing \(f\) in the first place. On
the contrary, the addition of the quadratic penalty term often
regularizes the proximal subproblems and makes them well conditioned.
Case in point, the subproblem may become convex despite \(f\) not being
convex; and even if \(f\) were convex, the subproblem has a larger strong
convexity parameter thereby facilitating faster numerical methods.</p>

<p>Despite the improved conditioning, each proximal subproblem still
requires invoking an iterative solver. For this reason, the proximal
point method has predominantly been thought of as a
theoretical/conceptual algorithm, only guiding algorithm design and
analysis rather than being implemented directly. One good example is the
proximal bundle method (Lemarechal, Strodiot, and Bihain 1981), which
approximates each proximal subproblem by a cutting plane model. In the
past few years, this viewpoint has undergone a major revision. In a
variety of circumstances, the proximal point method (or a close variant)
with a judicious choice of the control parameter \(\nu&gt;0\) and an
appropriate iterative method for the subproblems can lead to practical
and theoretically sound numerical methods. In this blog, I will briefly
describe three recent examples of this trend:</p>

<ul>
  <li>
    <p><a href="../proximal-subgrad">Episode 1</a>: a subgradient method for weakly convex stochastic approximation
problems (Davis and Grimmer 2017),</p>
  </li>
  <li>
    <p><a href="../../30/prox-linear/">Episode 2</a>: the prox-linear algorithm for minimizing compositions of convex
functions and smooth maps (Drusvyatskiy and Lewis 2016; Drusvyatskiy
and Paquette 2016; Burke and Ferris 1995; Nesterov 2007; Lewis and
Wright 2015; Cartis, Gould, and Toint 2011),</p>
  </li>
  <li>
    <p><a href="../../../02/05/catalyst">Episode 3</a>: Catalyst generic acceleration schema (Lin, Mairal, and
Harchaoui 2015; 2017) for regularized Empirical Risk Minimization.</p>
  </li>
</ul>

<!---
-   [Episode 2](../prox-linear): The prox-linear algorithm for minimizing compositions of convex
    functions and smooth maps (Drusvyatskiy and Lewis 2016; Drusvyatskiy
    and Paquette 2016; Burke and Ferris 1995; Nesterov 2007; Lewis and
    Wright 2015; Cartis, Gould, and Toint 2011),
-   [Episode 3](../catalyst): Catalyst generic acceleration schema (Lin, Mairal, and
    Harchaoui 2015) for regularized Empirical Risk Minimization.
--->

<p>Each epsiode, discussing the examples above, is self-contained and can
be read independently of the others. A version of this blog series will
appear in SIAG/OPT Views and News 2018.</p>

<h1 id="notation"><a name="notation"></a>Notation</h1>

<p>The following two constructions will play a basic role in the blog. For
any closed function \(f\) on \({\mathbb R}^d\), the <em>Moreau envelope</em> and
the <em>proximal map</em> are</p>

\[\begin{aligned}
f_{\nu}(z)&amp;:=\inf_{x}~\left\{f(x)+\tfrac{1}{2\nu}\|x-z\|^2\right\},\\
{\rm prox}_{\nu f}(z)&amp;:=\underset{x}{\operatorname{argmin}}~\left\{f(x)+\tfrac{1}{2\nu}\|x-z\|^2\right\},
\end{aligned}\]

<p>respectively. In this notation, the proximal point method is simply the
fixed-point recurrence on the proximal map:<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">1</a></sup></p>

\[{\bf Step\, }t: \qquad \textrm{choose }x_{t+1}\in {\rm prox}_{\nu f}(x_t).\]

<p>Clearly, in order to have any hope of solving the proximal subproblems,
one must ensure that they are convex. Consequently, the class of weakly
convex functions forms the natural setting for the proximal point
method.</p>

<p>A function \(f\) is called <em>\(\rho\)-weakly convex</em> if the assignment
\(x\mapsto f(x)+\frac{\rho}{2}\|x\|^2\) is a convex function.</p>

<p>For example, a \(C^1\)-smooth function with \(\rho\)-Lipschitz gradient is
\(\rho\)-weakly convex, while a \(C^2\)-smooth function \(f\) is \(\rho\)-weakly
convex precisely when the minimal eigenvalue of its Hessian is uniformly
bounded below by \(-\rho\). In essence, weak convexity precludes functions
that have downward kinks. For instance, \(f(x):=-\|x\|\) is not weakly
convex since no addition of a quadratic makes the resulting function
convex.</p>

<p>Whenever \(f\) is \(\rho\)-weakly convex and the proximal parameter \(\nu\)
satisfies \(\nu&lt;\rho^{-1}\), each proximal subproblem is itself convex and
therefore globally tractable. Moreover, in this setting, the Moreau
envelope is \(C^1\)-smooth with the gradient</p>

\[\nabla f_{\nu}(x)=\nu^{-1}(x-{\rm prox}_{\nu f}(x)).\]

<p>Rearranging the
gradient formula yields the useful interpretation of the proximal point
method as gradient descent on the Moreau envelope</p>

\[x_{t+1}=x_t-\nu\nabla f_{\nu}(x_t).\]

<p>In summary, the Moreau envelope \(f_{\nu}\) serves as a \(C^1\)-smooth
approximation of \(f\) for all small \(\nu\). Moreover, the two conditions</p>

\[\|\nabla f_{\nu}(x_{t})\|&lt; \varepsilon\]

<p>and</p>

\[\|\nu^{-1}(x_t-x_{t+1})\|&lt;\varepsilon,\]

<p>are equivalent for the
proximal point sequence \(\{x_t\}\). Hence, the step-size
\(\|x_t-x_{t+1}\|\) of the proximal point method serves as a convenient
termination criteria.</p>

<h2 id="examples-of-weakly-convex-functions">Examples of weakly convex functions</h2>

<p>Weakly convex functions are widespread in applications and are typically
easy to recognize. One common source of weakly convex functions is the
composite problem class \(\mathcal{C}\):</p>

\[\min_{x}~ F(x):=g(x)+h(c(x)),\]

<p>where
\(g\colon {\mathbb R}^d\to{\mathbb R}\cup\{+\infty\}\) is a closed convex
function, \(h\colon{\mathbb R}^m\to{\mathbb R}\) is convex and
\(L\)-Lipschitz, and \(c\colon{\mathbb R}^d\to{\mathbb R}^m\) is a
\(C^1\)-smooth map with \(\beta\)-Lipschitz gradient. An easy argument shows
that \(F\) is \(L\beta\)-weakly convex. This is a worst case estimate. In
concrete circumstances, the composite function \(F\) may have a much more
favorable weak convexity constant (e.g., phase retrieval (Duchi and Ruan
2017a, Section 3.2)).</p>

<ol>
  <li>
    <p>(Additive composite)
 The most prevalent example is additive composite minimization. In this
 case, the map \(c\) maps to the real line and \(h\) is the identity
 function:</p>

\[\label{eqn:add_comp}
 \min_{x}~ c(x)+g(x).\]

    <p>Such problems appear often in statistical
 learning and imaging. A variety of specialized algorithms are available;
 see for example Beck and Teboulle (2012) or Nesterov
 (2013).</p>
  </li>
  <li>
    <p>(Nonlinear least squares)</p>

    <p>The composite problem class also captures nonlinear least squares
 problems with bound constraints:</p>

\[\begin{aligned}
         \min_x~ \|c(x)\|_2\quad \textrm{subject to}\quad l_i\leq x_i\leq u_i ~\forall i.
 \end{aligned}\]

    <p>Such problems pervade engineering and scientific
 applications.</p>
  </li>
  <li>
    <p>(Exact penalty formulations)
 Consider a nonlinear optimization
 problem:</p>

\[\begin{aligned}
         \min_x~ \{f(x): G(x)\in \mathcal{K}\},
 \end{aligned}\]

    <p>where \(f\) and \(G\) are smooth maps and
 \(\mathcal{K}\) is a closed convex cone. An accompanying <em>penalty
 formulation</em> – ubiquitous in nonlinear optimization – takes the form</p>

\[\min_x~ f(x)+\lambda \cdot {\rm dist}_{\mathcal{K}}(G(x)),\]

    <p>where
 \({\rm dist}_{\mathcal{K}}(\cdot)\) is the distance to \(\mathcal{K}\) in
 some norm. Historically, exact penalty formulations served as the early
 motivation for the composite class \(\mathcal{C}\).</p>
  </li>
  <li>
    <p>(Robust phase retrieval)
 Phase retrieval is a common computational problem, with applications in
 diverse areas, such as imaging, X-ray crystallography, and speech
 processing. For simplicity, I will focus on the version of the problem
 over the reals. The (real) phase retrieval problem seeks to determine a
 point \(x\) satisfying the magnitude conditions,</p>

\[|\langle a_i,x\rangle|\approx b_i\quad \textrm{for }i=1,\ldots,m,\]

    <p>where \(a_i\in {\mathbb R}^d\) and \(b_i\in{\mathbb R}\) are given. Whenever
 there are gross outliers in the measurements \(b_i\), the following robust
 formulation of the problem is appealing (Eldar and Mendelson 2014; Duchi
 and Ruan 2017a; Davis, Drusvyatskiy, and Paquette 2017):</p>

\[\min_x ~\tfrac{1}{m}\sum_{i=1}^m |\langle a_i,x\rangle^2-b_i^2|.\]

    <p>Clearly, this is an instance of the composite class \(\mathcal{C}\).
 For some recent perspectives on phase retrieval, see the survey (Luke
 2017). There are numerous recent nonconvex approaches to phase
 retrieval, which rely on alternate problem formulations; e.g., (Candès,
 Li, and Soltanolkotabi 2015; Chen and Candès 2017; Sun, Qu, and Wright
 2017).</p>
  </li>
  <li>
    <p>(Robust PCA)
 In robust principal component analysis, one seeks to identify sparse
 corruptions of a low-rank matrix (Candès et al. 2011; Chandrasekaran et
 al. 2011). One typical example is image deconvolution, where the
 low-rank structure models the background of an image while the sparse
 corruption models the foreground. Formally, given a \(m\times n\) matrix
 \(M\), the goal is to find a decomposition \(M=L+S\), where \(L\) is low-rank
 and \(S\) is sparse. A common formulation of the problem reads:</p>

\[\min_{U\in {\mathbb R}^{m\times r},V\in {\mathbb R}^{n\times r}}~ \|UV^T-M\|_1,\]

    <p>where \(r\) is the target rank.</p>
  </li>
  <li>
    <p>(Censored \(\mathbb{Z}_2\) synchronization)
 A synchronization problem over a graph is to estimate group elements
 \(g_1,\ldots, g_n\) from pairwise products \(g_ig_j^{-1}\) over a set of
 edges \(ij\in E\). For a list of application of such problem see
 (Bandeira, Boumal, and Voroninski 2016; Singer 2011; Abbe et al. 2014),
 and references therein. A simple instance is \(\mathbb{Z}_2\)
 synchronization, corresponding to the group on two elements \(\{-1,+1\}\).
 The popular problem of detecting communities in a network, within the
 Binary Stochastic Block Model (SBM), can be modeled using \(\mathbb{Z}_2\)
 synchronization.</p>

    <p>Formally, given a partially observed matrix \(M\), the goal is to recover
 a vector \(\theta\in \{\pm 1\}^d\), satisfying
 \(M_{ij}\approx \theta_i \theta_j\) for all \(ij\in E\). When the entries of
 \(M\) are corrupted by adversarial sign flips, one can postulate the
 following formulation</p>

\[\min_{\theta\in {\mathbb R}^{d}}~ \|P_{E}(\theta\theta^T-M)\|_1,\]

    <p>where the operator \(P_E\) records the entries indexed by the edge set
 \(E\). Clearly, this is again an instance of the composite problem class
 \(\mathcal{C}\).</p>
  </li>
</ol>

<h1 id="references">References<a name="ref"></a></h1>
<p>Abbe, E., A.S. Bandeira, A. Bracher, and A. Singer. 2014. “Decoding
Binary Node Labels from Censored Edge Measurements: Phase Transition and
Efficient Recovery.” <em>IEEE Trans. Network Sci. Eng.</em> 1 (1):10–22.
<a href="https://doi.org/10.1109/TNSE.2014.2368716">https://doi.org/10.1109/TNSE.2014.2368716</a>.</p>

<p>Agarwal, A., and L. Bottou. 2015. “A Lower Bound for the Optimization of
Finite Sums.” In <em>Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>, 78–86.
<a href="http://leon.bottou.org/papers/agarwal-bottou-2015">http://leon.bottou.org/papers/agarwal-bottou-2015</a>.</p>

<p>Allen-Zhu, Z. 2016. “Katyusha: The First Direct Acceleration of
Stochastic Gradient Methods.” <em>Preprint arXiv:1603.05953 (Version 5)</em>.</p>

<p>Arjevani, Y. 2017. “Limitations on Variance-Reduction and Acceleration
Schemes for Finite Sums Optimization.” In <em>Advances in Neural
Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
3543–52. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf">http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf</a>.</p>

<p>Bandeira, A.S., N. Boumal, and V. Voroninski. 2016. “On the Low-Rank
Approach for Semidefinite Programs Arising in Synchronization and
Community Detection.” In <em>Proceedings of the 29th Conference on Learning
Theory, COLT 2016, New York, Usa, June 23-26, 2016</em>, 361–82.
<a href="http://jmlr.org/proceedings/papers/v49/bandeira16.html">http://jmlr.org/proceedings/papers/v49/bandeira16.html</a>.</p>

<p>Bartlett, P.L., M.I. Jordan, and J.D. McAuliffe. 2006. “Convexity,
Classification, and Risk Bounds.” <em>J. Amer. Statist. Assoc.</em> 101
(473):138–56.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907">https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907</a>.</p>

<p>Beck, A., and M. Teboulle. 2012. “Smoothing and First Order Methods: A
Unified Framework.” <em>SIAM J. Optim.</em> 22 (2):557–80.
<a href="https://doi.org/10.1137/100818327">https://doi.org/10.1137/100818327</a>.</p>

<p>Bottou, L., and O. Bousquet. 2008. “The Tradeoffs of Large Scale
Learning.” In <em>Advances in Neural Information Processing Systems</em>,
161–68. <a href="http://leon.bottou.org/publications/pdf/nips-2007.pdf">http://leon.bottou.org/publications/pdf/nips-2007.pdf</a>.</p>

<p>Burke, J.V., and M.C. Ferris. 1995. “A Gauss-Newton Method for Convex
Composite Optimization.” <em>Math. Programming</em> 71 (2, Ser. A):179–94.
<a href="https://doi.org/10.1007/BF01585997">https://doi.org/10.1007/BF01585997</a>.</p>

<p>Candès, E.J., X. Li, Y. Ma, and J. Wright. 2011. “Robust Principal
Component Analysis?” <em>J. ACM</em> 58 (3):Art. 11, 37.
<a href="https://doi.org/10.1145/1970392.1970395">https://doi.org/10.1145/1970392.1970395</a>.</p>

<p>Candès, E.J., X. Li, and M. Soltanolkotabi. 2015. “Phase Retrieval via
Wirtinger Flow: Theory and Algorithms.” <em>IEEE Trans. Inform. Theory</em> 61
(4):1985–2007. <a href="https://doi.org/10.1109/TIT.2015.2399924">https://doi.org/10.1109/TIT.2015.2399924</a>.</p>

<p>Carmon, Y., J.C. Duchi, O. Hinder, and A. Sidford. 2017a. “‘Convex Until
Proven Guilty’: Dimension-Free Acceleration of Gradient Descent on
Non-Convex Functions.” In <em>Proceedings of the 34th International
Conference on Machine Learning</em>, 70:654–63.</p>

<p>Y. Carmon, J.C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I.
<em>Preprint arXiv:1710.11606</em>.</p>

<p>Cartis, C., N.I.M. Gould, and P.L. Toint. 2011. “On the Evaluation
Complexity of Composite Function Minimization with Applications to
Nonconvex Nonlinear Programming.” <em>SIAM J. Optim.</em> 21 (4):1721–39.
<a href="https://doi.org/10.1137/11082381X">https://doi.org/10.1137/11082381X</a>.</p>

<p>Chambolle, A., and T. Pock. 2011. “A First-Order Primal-Dual Algorithm
for Convex Problems with Applications to Imaging.” <em>J. Math. Imaging
Vision</em> 40 (1):120–45. <a href="https://doi.org/10.1007/s10851-010-0251-1">https://doi.org/10.1007/s10851-010-0251-1</a>.</p>

<p>Chandrasekaran, V., S. Sanghavi, P. A. Parrilo, and A.S. Willsky. 2011.
“Rank-Sparsity Incoherence for Matrix Decomposition.” <em>SIAM J. Optim.</em>
21 (2):572–96. <a href="https://doi.org/10.1137/090761793">https://doi.org/10.1137/090761793</a>.</p>

<p>Chen, Y., and E.J. Candès. 2017. “Solving Random Quadratic Systems of
Equations Is Nearly as Easy as Solving Linear Systems.” <em>Comm. Pure
Appl. Math.</em> 70 (5):822–83.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638">https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638</a>.</p>

<p>Davis, D. 2016. “SMART: The Stochastic Monotone Aggregated Root-Finding
Algorithm.” <em>Preprint arXiv:1601.00698</em>.</p>

<p>Davis, D., D. Drusvyatskiy, and C. Paquette. 2017. “The Nonsmooth
Landscape of Phase Retrieval.” <em>Preprint arXiv:1711.03247</em>.</p>

<p>Davis, D., and B. Grimmer. 2017. “Proximally Guided Stochastic
Sbgradient Method for Nonsmooth, Nonconvex Problems.” <em>Preprint,
arXiv:1707.03505</em>.</p>

<p>Defazio, A. 2016. “A Simple Practical Accelerated Method for Finite
Sums.” In <em>Advances in Neural Information Processing Systems 29</em>, edited
by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
676–84. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf">http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf</a>.</p>

<p>Defazio, A., F. Bach, and S. Lacoste-Julien. 2014. “SAGA: A Fast
Incremental Gradient Method with Support for Non-Strongly Convex
Composite Objectives.” In <em>Advances in Neural Information Processing
Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, 1646–54. Curran Associates, Inc.</p>

<p>Defazio, A., J. Domke, and T.S. Caetano. 2014. “Finito: A Faster,
Permutable Incremental Gradient Method for Big Data Problems.” In
<em>ICML</em>, 1125–33.</p>

<p>Drusvyatskiy, D., and A.S. Lewis. 2013. “Tilt Stability, Uniform
Quadratic Growth, and Strong Metric Regularity of the Subdifferential.”
<em>SIAM J. Optim.</em> 23 (1):256–67. <a href="https://doi.org/10.1137/120876551">https://doi.org/10.1137/120876551</a>.</p>

<p>D. Drusvyatskiy and A.S. Lewis. 2016. “Error Bounds, Quadratic Growth, and Linear Convergence
of Proximal Methods.” <em>To Appear in Math. Oper. Res., arXiv:1602.06661</em>.</p>

<p>Drusvyatskiy, D., B.S. Mordukhovich, and T.T.A. Nghia. 2014.
“Second-Order Growth, Tilt-Stability, and Metric Regularity of the
Subdifferential.” <em>J. Convex Anal.</em> 21 (4):1165–92.</p>

<p>Drusvyatskiy, D., and C. Paquette. 2016. “Efficiency of Minimizing
Compositions of Convex Functions and Smooth Maps.” <em>Preprint,
arXiv:1605.00125</em>.</p>

<p>Duchi, J.C., and F. Ruan. 2017a. “Solving (Most) of a Set of Quadratic
Equalities: Composite Optimization for Robust Phase Retrieval.”
<em>Preprint arXiv:1705.02356</em>.</p>

<p>J.C. Duchi and F. Ruan. 2017b. “Stochastic Methods for Composite Optimization
Problems.” <em>Preprint arXiv:1703.08570</em>.</p>

<p>Eldar, Y.C., and S. Mendelson. 2014. “Phase Retrieval: Stability and
Recovery Guarantees.” <em>Appl. Comput. Harmon. Anal.</em> 36 (3):473–94.
<a href="https://doi.org/10.1016/j.acha.2013.08.003">https://doi.org/10.1016/j.acha.2013.08.003</a>.</p>

<p>Frostig, R., R. Ge, S.M. Kakade, and A. Sidford. 2015. “Un-Regularizing:
Approximate Proximal Point and Faster Stochastic Algorithms for
Empirical Risk Minimization.” In <em>Proceedings of the 32nd International
Conference on Machine Learning (ICML)</em>.</p>

<p>Ghadimi, S., and G. Lan. 2013. “Stochastic First- and Zeroth-Order
Methods for Nonconvex Stochastic Programming.” <em>SIAM J. Optim.</em> 23
(4):2341–68. <a href="https://doi.org/10.1137/120880811">https://doi.org/10.1137/120880811</a>.</p>

<p>Güler, O. 1992. “New Proximal Point Algorithms for Convex Minimization.”
<em>SIAM J. Optim.</em> 2 (4):649–64.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/0802032">https://doi-org.offcampus.lib.washington.edu/10.1137/0802032</a>.</p>

<p>Hazan, E., and S. Kale. 2011. “Beyond the Regret Minimization Barrier:
An Optimal Algorithm for Stochastic Strongly-Convex Optimization.” In
<em>Proceedings of the 24th Annual Conference on Learning Theory</em>, edited
by Sham M. Kakade and Ulrike von Luxburg, 19:421–36. Proceedings of
Machine Learning Research. Budapest, Hungary: PMLR.</p>

<p>Johnson, R., and T. Zhang. 2013. “Accelerating Stochastic Gradient
Descent Using Predictive Variance Reduction.” In <em>Proceedings of the
26th International Conference on Neural Information Processing Systems</em>,
315–23. NIPS’13. USA: Curran Associates Inc.
<a href="http://dl.acm.org/citation.cfm?id=2999611.2999647">http://dl.acm.org/citation.cfm?id=2999611.2999647</a>.</p>

<p>Juditsky, A., and Y. Nesterov. 2014. “Deterministic and Stochastic
Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization.”
<em>Stoch. Syst.</em> 4 (1):44–80.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010">https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010</a>.</p>

<p>Lacoste-Julien, S., M. Schmidt, and F. Bach. 2012. “A Simpler Approach
to Obtaining an \({O}(1/t)\) Convergence Rate for the Projected Stochastic
Subgradient Method.” <em>Arxiv arXiv:1212.2002</em>.</p>

<p>Lan, G. 2015. “An Optimal Randomized Incremental Gradient Method.”
<em>arXiv:1507.02000</em>.</p>

<p>Lemarechal, C., J.-J. Strodiot, and A. Bihain. 1981. “On a Bundle
Algorithm for Nonsmooth Optimization.” In <em>Nonlinear Programming, 4
(Madison, Wis., 1980)</em>, 245–82. Academic Press, New York-London.</p>

<p>Lewis, A.S., and S.J. Wright. 2015. “A Proximal Method for Composite
Minimization.” <em>Math. Program.</em> Springer Berlin Heidelberg, 1–46.
<a href="https://doi.org/10.1007/s10107-015-0943-9">https://doi.org/10.1007/s10107-015-0943-9</a>.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. 2015. “A Universal Catalyst for
First-Order Optimization.” In <em>Advances in Neural Information Processing
Systems</em>, 3366–74.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. “Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice.” <em>arXiv preprint arXiv:1712.05654</em> (2017).</p>

<p>Luke, R. 2017. “Phase Retrieval, What’s New?” <em>SIAG/OPT Views and News</em>
25 (1).</p>

<p>Mairal, J. 2015. “Incremental Majorization-Minimization Optimization
with Application to Large-Scale Machine Learning.” <em>SIAM Journal on
Optimization</em> 25 (2):829–55.</p>

<p>Nemirovski, A. 2004. “Prox-Method with Rate of Convergence \(O(1/t)\) for
Variational Inequalities with Lipschitz Continuous Monotone Operators
and Smooth Convex-Concave Saddle Point Problems.” <em>SIAM J. Optim.</em> 15
(1):229–51. <a href="https://doi.org/10.1137/S1052623403425629">https://doi.org/10.1137/S1052623403425629</a>.</p>

<p>Nemirovski, A., A. Juditsky, G. Lan, and A. Shapiro. 2008. “Robust
Stochastic Approximation Approach to Stochastic Programming.” <em>SIAM J.
Optim.</em> 19 (4):1574–1609.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/070704277">https://doi-org.offcampus.lib.washington.edu/10.1137/070704277</a>.</p>

<p>Nemirovsky, A.S., and D.B. Yudin. 1983. <em>Problem Complexity and Method
Efficiency in Optimization</em>. A Wiley-Interscience Publication. John
Wiley &amp; Sons, Inc., New York.</p>

<p>Nesterov, Y., and A. Nemirovskii. 1994. <em>Interior-Point Polynomial
Algorithms in Convex Programming</em>. Vol. 13. SIAM Studies in Applied
Mathematics. Society for Industrial; Applied Mathematics (SIAM),
Philadelphia, PA. <a href="https://doi.org/10.1137/1.9781611970791">https://doi.org/10.1137/1.9781611970791</a>.</p>

<p>Nesterov, Yu. 1983. “A Method for Solving the Convex Programming Problem
with Convergence Rate \(O(1/k^{2})\).” <em>Dokl. Akad. Nauk SSSR</em> 269
(3):543–47.</p>

<p>Nesterov, Yu. 2005. “Smooth Minimization of Non-Smooth Functions.” <em>Math.
Program.</em> 103 (1, Ser. A):127–52.
<a href="https://doi.org/10.1007/s10107-004-0552-5">https://doi.org/10.1007/s10107-004-0552-5</a>.</p>

<p>Nesterov, Yu. 2007. “Modified Gauss-Newton Scheme with Worst Case
Guarantees for Global Performance.” <em>Optim. Methods Softw.</em> 22
(3):469–83. <a href="https://doi.org/10.1080/08927020600643812">https://doi.org/10.1080/08927020600643812</a>.</p>

<p>Nesterov, Yu. 2013. “Gradient Methods for Minimizing Composite Functions.”
<em>Math. Program.</em> 140 (1, Ser. B):125–61.
<a href="https://doi.org/10.1007/s10107-012-0629-5">https://doi.org/10.1007/s10107-012-0629-5</a>.</p>

<p>Paquette, C., H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. 2017. 
“Catalyst Acceleration for Gradient-Based Non-Convex
Optimization.” <em>Preprint arXiv:1703.10993</em>.</p>

<p>Polyak, B.T., and A.B. Juditsky. 1992. “Acceleration of Stochastic
Approximation by Averaging.” <em>SIAM J. Control Optim.</em> 30 (4):838–55.
<a href="https://doi.org/10.1137/0330046">https://doi.org/10.1137/0330046</a>.</p>

<p>Rakhlin, A., O. Shamir, and K. Sridharan. 2012. “Making Gradient Descent
Optimal for Strongly Convex Stochastic Optimization.” In <em>Proceedings of
the 29th International Coference on International Conference on Machine
Learning</em>, 1571–8. ICML’12. USA: Omnipress.
<a href="http://dl.acm.org/citation.cfm?id=3042573.3042774">http://dl.acm.org/citation.cfm?id=3042573.3042774</a>.</p>

<p>Robbins, H., and S. Monro. 1951. “A Stochastic Approximation Method.”
<em>Ann. Math. Statistics</em> 22:400–407.</p>

<p>Schmidt, M., N. Le Roux, and F. Bach. 2013. “Minimizing Finite Sums with
the Stochastic Average Gradient.” <em>arXiv:1309.2388</em>.</p>

<p>Shalev-Shwartz, S., and T. Zhang. 2012. “Proximal Stochastic Dual
Coordinate Ascent.” <em>arXiv:1211.2717</em>.</p>

<p>S. Shalev-Shwartz and T. Zhang. 2015. “Accelerated Proximal Stochastic Dual Coordinate Ascent
for Regularized Loss Minimization.” <em>Mathematical Programming</em>.</p>

<p>Singer, A. 2011. “Angular Synchronization by Eigenvectors and
Semidefinite Programming.” <em>Appl. Comput. Harmon. Anal.</em> 30 (1):20–36.
<a href="https://doi.org/10.1016/j.acha.2010.02.001">https://doi.org/10.1016/j.acha.2010.02.001</a>.</p>

<p>Sun, J., Q. Qu, and J. Wright. 2017. “A Geometric Analysis of Phase
Retrieval.” <em>To Appear in Found. Comp. Math., arXiv:1602.06664</em>.</p>

<p>Woodworth, B.E., and N. Srebro. 2016. “Tight Complexity Bounds for
Optimizing Composite Objectives.” In <em>Advances in Neural Information
Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, 3639–47. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf">http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf</a>.</p>

<p>Wright, S.J. 1997. <em>Primal-Dual Interior-Point Methods</em>. Society for
Industrial; Applied Mathematics (SIAM), Philadelphia, PA.
<a href="https://doi.org/10.1137/1.9781611971453">https://doi.org/10.1137/1.9781611971453</a>.</p>

<p>Xiao, L., and T. Zhang. 2014. “A Proximal Stochastic Gradient Method
with Progressive Variance Reduction.” <em>SIAM J. Optim.</em> 24 (4):2057–75.
<a href="https://doi.org/10.1137/140961791">https://doi.org/10.1137/140961791</a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>To ensure that \({\rm prox}_{\nu f}(\cdot)\) is nonempty, it
suffices to assume that \(f\) is bounded from below. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>













      </div>
    </div>
  </div>

  <footer class="center">
</footer>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112084371-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112084371-1');
</script>

</body>
</html>
