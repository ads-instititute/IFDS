<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Proximal point algorithm revisited, episode 1. The proximally guided subgradient method &#8211; UW Institute on the Algorithmic Foundations of Data Science</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Revisiting the proximal point method, with the proximally guided subgradient method for stochastic optimization.">
    <meta name="robots" content="all">
    <meta name="author" content="Sham Kakade">
    
    <meta name="keywords" content="blog">
    <link rel="canonical" href="http://localhost:4000/blog/2018/01/24/proximal-subgrad/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for UW Institute on the Algorithmic Foundations of Data Science" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202011120026" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    
    <script type="text/javascript" async
        src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Proximal point algorithm revisited, episode 1. The proximally guided subgradient method">
    <meta property="og:description" content="">
    <meta property="og:url" content="http://localhost:4000/blog/2018/01/24/proximal-subgrad/">
    <meta property="og:site_name" content="UW Institute on the Algorithmic Foundations of Data Science">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Proximal point algorithm revisited, episode 1. The proximally guided subgradient method" />
    <meta name="twitter:description" content="Revisiting the proximal point method, with the proximally guided subgradient method for stochastic optimization." />
    <meta name="twitter:url" content="http://localhost:4000/blog/2018/01/24/proximal-subgrad/" />
    

    <!-- Icons -->
    <!-- <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> -->

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#ffffff">

    
</head>

<body class="site">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <div style="width:100%">
      <a href="/"><img src="/images/ifds.png" style="width:30%;float:left;"></a>
      <a href="https://nsf-tripods.org/"><img src="/images/NSF.gif" style="width:15%;margin-left:12.5%;float:left;"></a>
      <a href="http://escience.washington.edu/"><img src="/images/eScience.png" style="width:30%;float:right;"></a>
        <div style="clear: both;"></div>
      </div>
      
      <nav class="site-nav">
        



    
    
    
    

    

    
    
    
    
        <a href="/about.html">About</a>
    

    

    
    
    
    
        <a href="/index.html">News</a>
    

    

    
    
    
    
        <a href="/publications.html">Publications</a>
    

    

    
    
    
    
        <a href="/seminars.html">Seminars</a>
    

    

    
    
    
    
        <a href="/courses.html">Courses</a>
    

    

    
    
    
    
        <a href="/members.html">Members</a>
    

    

    
    
    
    
        <a href="/blog.html">Blog</a>
    

    



<div class="dropdown">
  <button class="dropbtn">Workshops</button>
  <div class="dropdown-content">
    <a href="https://alecgt.github.io/adsi_summer/" style="margin-left:0em;"><font size=2px>ADSI Summer School</font></a>
    <a href="https://ajwagen.github.io/adsi_learning_and_control/" style="margin-left:0em;"><font size=2px>ADSI Workshop</font></a>
  </div>
</div>


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
 
  
  <a href="http://sites.math.washington.edu/~ddrusv/" style="text-decoration: none" class="post-link">
  
    <img src="/images/prox.png" style="width:30%;float:right;">
  
  </a>
  
 
  
  
  <h1>Proximal point algorithm revisited, episode 1. The proximally guided subgradient method</h1>
  <span class="post-meta">Dmitriy Drusvyatskiy. Jan 24, 2018.</span><br>
  
  <span class="post-meta small">
  
    10 minute read
  
  </span>
</div>

<article class="post-content">
  <p>This is episode 1 of the three part series that revisits the classical proximal
point algorithm. See the <a href="../proximal-point">previous post</a> for 
introduction and notation.</p>

<h1 id="the-proximally-guided-subgradient-method"><a name="sec1"></a>The proximally guided subgradient method</h1>

<p>As the first example of contemporary applications of the proximal point
method, consider the problem of minimizing the expectation:<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">1</a></sup></p>

\[\min_{x\in {\mathbb R}^d}~ F(x)=\mathbb{E}_{\zeta} f(x,\zeta).\]

<p>Here,
\(\zeta\) is a random variable, and the only access to \(F\) is by sampling
\(\zeta\). It is difficult to overstate the importance of this problem
class (often called <em>stochastic approximation</em>) in large-scale
optimization; see e.g. Bottou and Bousquet (2008); Bartlett, Jordan, and
McAuliffe (2006).</p>

<p>When the problem is convex, the stochastic subgradient method (Polyak
and Juditsky 1992; Robbins and Monro 1951; Nemirovski et al. 2008) has
strong theoretical guarantees and is often the method of choice. In
contrast, when applied to nonsmooth and nonconvex problems, the behavior
of the method is poorly understood. The recent paper (Davis and Grimmer
2017) shows how to use the proximal point method to guide the
subgradient iterates in this broader setting, with rigorous guarantees.</p>

<p>Henceforth, assume that the function \(x\mapsto f(x,\zeta)\) is
\(\rho\)-weakly convex and \(L\)-Lipschitz for each \(\zeta\). Davis and
Grimmer (2017) proposed the scheme outlined below.</p>

<h4 id="proximally-guided-stochastic-subgradient-method">Proximally guided stochastic subgradient method</h4>

<ul>
  <li><strong>Data</strong>: \(x_0\in {\mathbb R}^d\), \(\{j_t\}\subset\mathbb{N}\),
\(\{\alpha_j\}\subset{\mathbb R}_{++}\)</li>
  <li><strong>For</strong> \(t=0,\ldots,T\) <strong>do</strong>
    <ul>
      <li>Set \(y_0=x_t\)</li>
      <li><strong>For</strong> \(j=0,\ldots,j_t-2\) <strong>do</strong>
        <ul>
          <li>Sample \(\zeta\) and choose \(v_j\in\partial (f(\cdot,\zeta)+\rho\|\cdot-x_t\|^2)(y_j)\)</li>
          <li>Set \(y_{j+1}= y_j-\alpha_jv_j\)</li>
        </ul>
      </li>
      <li>Set \(x_{t+1}= \frac{1}{j_t}\sum_{j=0}^{j_t-1}y_j\)</li>
    </ul>
  </li>
</ul>

<p>The method proceeds by applying a proximal point method with each
subproblem approximately solved by a stochastic subgradient method. The
intuition is that each proximal subproblem is \(\rho/2\)-strongly convex
and therefore according to well-known results (e.g. Lacoste-Julien,
Schmidt, and Bach (2012); Rakhlin, Shamir, and Sridharan (2012); Hazan and
Kale (2011); Juditsky and Nesterov (2014)), the stochastic subgradient
method should converge at the rate \(O(\frac{1}{T})\) on the subproblem,
in expectation. This intuition is not quite correct because the
objective function of the subproblem is not globally Lipschitz – a key
assumption for the \(O(\frac{1}{T})\) rate. Nonetheless, the authors show
that warm-starting the subgradient method for each proximal subproblem
with the current proximal iterate corrects this issue, yielding a
favorable guarantees (Davis and Grimmer 2017 Theorem 1).</p>

<p>To describe the rate of convergence, set
\(j_t=t+\lceil 648\log(648)\rceil\) and \(\alpha_j=\tfrac{2}{\rho(j+49)}\)
in the Proximally guided stochastic subgradient method. Then the scheme
will generate an iterate \(x\) satisfying</p>

\[\mathbb{E}_{\zeta}[\|\nabla F_{2\rho}(x)\|^2]\leq \varepsilon\]

<p>after
at most</p>

\[O\left(\frac{\rho^2(F(x_0)-\inf  F)^2}{\varepsilon^2}+\frac{L^4 \log^{4}(\varepsilon^{-1})}{\varepsilon^2}\right)\]

<p>subgradient evaluations. This rate agrees with analogous guarantees for
stochastic gradient methods for smooth nonconvex functions (Ghadimi and
Lan 2013). It is also worth noting that convex constraints on \(x\) can be
easily incorporated into the Proximally guided stochastic subgradient
method by introducing a nearest-point projection in the definition of
\(y_{j+1}\).</p>

<h1 id="references">References<a name="ref"></a></h1>
<p>Abbe, E., A.S. Bandeira, A. Bracher, and A. Singer. 2014. “Decoding
Binary Node Labels from Censored Edge Measurements: Phase Transition and
Efficient Recovery.” <em>IEEE Trans. Network Sci. Eng.</em> 1 (1):10–22.
<a href="https://doi.org/10.1109/TNSE.2014.2368716">https://doi.org/10.1109/TNSE.2014.2368716</a>.</p>

<p>Agarwal, A., and L. Bottou. 2015. “A Lower Bound for the Optimization of
Finite Sums.” In <em>Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>, 78–86.
<a href="http://leon.bottou.org/papers/agarwal-bottou-2015">http://leon.bottou.org/papers/agarwal-bottou-2015</a>.</p>

<p>Allen-Zhu, Z. 2016. “Katyusha: The First Direct Acceleration of
Stochastic Gradient Methods.” <em>Preprint arXiv:1603.05953 (Version 5)</em>.</p>

<p>Arjevani, Y. 2017. “Limitations on Variance-Reduction and Acceleration
Schemes for Finite Sums Optimization.” In <em>Advances in Neural
Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
3543–52. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf">http://papers.nips.cc/paper/6945-limitations-on-variance-reduction-and-acceleration-schemes-for-finite-sums-optimization.pdf</a>.</p>

<p>Bandeira, A.S., N. Boumal, and V. Voroninski. 2016. “On the Low-Rank
Approach for Semidefinite Programs Arising in Synchronization and
Community Detection.” In <em>Proceedings of the 29th Conference on Learning
Theory, COLT 2016, New York, Usa, June 23-26, 2016</em>, 361–82.
<a href="http://jmlr.org/proceedings/papers/v49/bandeira16.html">http://jmlr.org/proceedings/papers/v49/bandeira16.html</a>.</p>

<p>Bartlett, P.L., M.I. Jordan, and J.D. McAuliffe. 2006. “Convexity,
Classification, and Risk Bounds.” <em>J. Amer. Statist. Assoc.</em> 101
(473):138–56.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907">https://doi-org.offcampus.lib.washington.edu/10.1198/016214505000000907</a>.</p>

<p>Beck, A., and M. Teboulle. 2012. “Smoothing and First Order Methods: A
Unified Framework.” <em>SIAM J. Optim.</em> 22 (2):557–80.
<a href="https://doi.org/10.1137/100818327">https://doi.org/10.1137/100818327</a>.</p>

<p>Bottou, L., and O. Bousquet. 2008. “The Tradeoffs of Large Scale
Learning.” In <em>Advances in Neural Information Processing Systems</em>,
161–68. <a href="http://leon.bottou.org/publications/pdf/nips-2007.pdf">http://leon.bottou.org/publications/pdf/nips-2007.pdf</a>.</p>

<p>Burke, J.V., and M.C. Ferris. 1995. “A Gauss-Newton Method for Convex
Composite Optimization.” <em>Math. Programming</em> 71 (2, Ser. A):179–94.
<a href="https://doi.org/10.1007/BF01585997">https://doi.org/10.1007/BF01585997</a>.</p>

<p>Candès, E.J., X. Li, Y. Ma, and J. Wright. 2011. “Robust Principal
Component Analysis?” <em>J. ACM</em> 58 (3):Art. 11, 37.
<a href="https://doi.org/10.1145/1970392.1970395">https://doi.org/10.1145/1970392.1970395</a>.</p>

<p>Candès, E.J., X. Li, and M. Soltanolkotabi. 2015. “Phase Retrieval via
Wirtinger Flow: Theory and Algorithms.” <em>IEEE Trans. Inform. Theory</em> 61
(4):1985–2007. <a href="https://doi.org/10.1109/TIT.2015.2399924">https://doi.org/10.1109/TIT.2015.2399924</a>.</p>

<p>Carmon, Y., J.C. Duchi, O. Hinder, and A. Sidford. 2017a. “‘Convex Until
Proven Guilty’: Dimension-Free Acceleration of Gradient Descent on
Non-Convex Functions.” In <em>Proceedings of the 34th International
Conference on Machine Learning</em>, 70:654–63.</p>

<p>Y. Carmon, J.C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I.
<em>Preprint arXiv:1710.11606</em>.</p>

<p>Cartis, C., N.I.M. Gould, and P.L. Toint. 2011. “On the Evaluation
Complexity of Composite Function Minimization with Applications to
Nonconvex Nonlinear Programming.” <em>SIAM J. Optim.</em> 21 (4):1721–39.
<a href="https://doi.org/10.1137/11082381X">https://doi.org/10.1137/11082381X</a>.</p>

<p>Chambolle, A., and T. Pock. 2011. “A First-Order Primal-Dual Algorithm
for Convex Problems with Applications to Imaging.” <em>J. Math. Imaging
Vision</em> 40 (1):120–45. <a href="https://doi.org/10.1007/s10851-010-0251-1">https://doi.org/10.1007/s10851-010-0251-1</a>.</p>

<p>Chandrasekaran, V., S. Sanghavi, P. A. Parrilo, and A.S. Willsky. 2011.
“Rank-Sparsity Incoherence for Matrix Decomposition.” <em>SIAM J. Optim.</em>
21 (2):572–96. <a href="https://doi.org/10.1137/090761793">https://doi.org/10.1137/090761793</a>.</p>

<p>Chen, Y., and E.J. Candès. 2017. “Solving Random Quadratic Systems of
Equations Is Nearly as Easy as Solving Linear Systems.” <em>Comm. Pure
Appl. Math.</em> 70 (5):822–83.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638">https://doi-org.offcampus.lib.washington.edu/10.1002/cpa.21638</a>.</p>

<p>Davis, D. 2016. “SMART: The Stochastic Monotone Aggregated Root-Finding
Algorithm.” <em>Preprint arXiv:1601.00698</em>.</p>

<p>Davis, D., D. Drusvyatskiy, and C. Paquette. 2017. “The Nonsmooth
Landscape of Phase Retrieval.” <em>Preprint arXiv:1711.03247</em>.</p>

<p>Davis, D., and B. Grimmer. 2017. “Proximally Guided Stochastic
Sbgradient Method for Nonsmooth, Nonconvex Problems.” <em>Preprint,
arXiv:1707.03505</em>.</p>

<p>Defazio, A. 2016. “A Simple Practical Accelerated Method for Finite
Sums.” In <em>Advances in Neural Information Processing Systems 29</em>, edited
by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
676–84. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf">http://papers.nips.cc/paper/6154-a-simple-practical-accelerated-method-for-finite-sums.pdf</a>.</p>

<p>Defazio, A., F. Bach, and S. Lacoste-Julien. 2014. “SAGA: A Fast
Incremental Gradient Method with Support for Non-Strongly Convex
Composite Objectives.” In <em>Advances in Neural Information Processing
Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, 1646–54. Curran Associates, Inc.</p>

<p>Defazio, A., J. Domke, and T.S. Caetano. 2014. “Finito: A Faster,
Permutable Incremental Gradient Method for Big Data Problems.” In
<em>ICML</em>, 1125–33.</p>

<p>Drusvyatskiy, D., and A.S. Lewis. 2013. “Tilt Stability, Uniform
Quadratic Growth, and Strong Metric Regularity of the Subdifferential.”
<em>SIAM J. Optim.</em> 23 (1):256–67. <a href="https://doi.org/10.1137/120876551">https://doi.org/10.1137/120876551</a>.</p>

<p>D. Drusvyatskiy and A.S. Lewis. 2016. “Error Bounds, Quadratic Growth, and Linear Convergence
of Proximal Methods.” <em>To Appear in Math. Oper. Res., arXiv:1602.06661</em>.</p>

<p>Drusvyatskiy, D., B.S. Mordukhovich, and T.T.A. Nghia. 2014.
“Second-Order Growth, Tilt-Stability, and Metric Regularity of the
Subdifferential.” <em>J. Convex Anal.</em> 21 (4):1165–92.</p>

<p>Drusvyatskiy, D., and C. Paquette. 2016. “Efficiency of Minimizing
Compositions of Convex Functions and Smooth Maps.” <em>Preprint,
arXiv:1605.00125</em>.</p>

<p>Duchi, J.C., and F. Ruan. 2017a. “Solving (Most) of a Set of Quadratic
Equalities: Composite Optimization for Robust Phase Retrieval.”
<em>Preprint arXiv:1705.02356</em>.</p>

<p>J.C. Duchi and F. Ruan. 2017b. “Stochastic Methods for Composite Optimization
Problems.” <em>Preprint arXiv:1703.08570</em>.</p>

<p>Eldar, Y.C., and S. Mendelson. 2014. “Phase Retrieval: Stability and
Recovery Guarantees.” <em>Appl. Comput. Harmon. Anal.</em> 36 (3):473–94.
<a href="https://doi.org/10.1016/j.acha.2013.08.003">https://doi.org/10.1016/j.acha.2013.08.003</a>.</p>

<p>Frostig, R., R. Ge, S.M. Kakade, and A. Sidford. 2015. “Un-Regularizing:
Approximate Proximal Point and Faster Stochastic Algorithms for
Empirical Risk Minimization.” In <em>Proceedings of the 32nd International
Conference on Machine Learning (ICML)</em>.</p>

<p>Ghadimi, S., and G. Lan. 2013. “Stochastic First- and Zeroth-Order
Methods for Nonconvex Stochastic Programming.” <em>SIAM J. Optim.</em> 23
(4):2341–68. <a href="https://doi.org/10.1137/120880811">https://doi.org/10.1137/120880811</a>.</p>

<p>Güler, O. 1992. “New Proximal Point Algorithms for Convex Minimization.”
<em>SIAM J. Optim.</em> 2 (4):649–64.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/0802032">https://doi-org.offcampus.lib.washington.edu/10.1137/0802032</a>.</p>

<p>Hazan, E., and S. Kale. 2011. “Beyond the Regret Minimization Barrier:
An Optimal Algorithm for Stochastic Strongly-Convex Optimization.” In
<em>Proceedings of the 24th Annual Conference on Learning Theory</em>, edited
by Sham M. Kakade and Ulrike von Luxburg, 19:421–36. Proceedings of
Machine Learning Research. Budapest, Hungary: PMLR.</p>

<p>Johnson, R., and T. Zhang. 2013. “Accelerating Stochastic Gradient
Descent Using Predictive Variance Reduction.” In <em>Proceedings of the
26th International Conference on Neural Information Processing Systems</em>,
315–23. NIPS’13. USA: Curran Associates Inc.
<a href="http://dl.acm.org/citation.cfm?id=2999611.2999647">http://dl.acm.org/citation.cfm?id=2999611.2999647</a>.</p>

<p>Juditsky, A., and Y. Nesterov. 2014. “Deterministic and Stochastic
Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization.”
<em>Stoch. Syst.</em> 4 (1):44–80.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010">https://doi-org.offcampus.lib.washington.edu/10.1214/10-SSY010</a>.</p>

<p>Lacoste-Julien, S., M. Schmidt, and F. Bach. 2012. “A Simpler Approach
to Obtaining an \({O}(1/t)\) Convergence Rate for the Projected Stochastic
Subgradient Method.” <em>Arxiv arXiv:1212.2002</em>.</p>

<p>Lan, G. 2015. “An Optimal Randomized Incremental Gradient Method.”
<em>arXiv:1507.02000</em>.</p>

<p>Lemarechal, C., J.-J. Strodiot, and A. Bihain. 1981. “On a Bundle
Algorithm for Nonsmooth Optimization.” In <em>Nonlinear Programming, 4
(Madison, Wis., 1980)</em>, 245–82. Academic Press, New York-London.</p>

<p>Lewis, A.S., and S.J. Wright. 2015. “A Proximal Method for Composite
Minimization.” <em>Math. Program.</em> Springer Berlin Heidelberg, 1–46.
<a href="https://doi.org/10.1007/s10107-015-0943-9">https://doi.org/10.1007/s10107-015-0943-9</a>.</p>

<p>Lin, H., J. Mairal, and Z. Harchaoui. 2015. “A Universal Catalyst for
First-Order Optimization.” In <em>Advances in Neural Information Processing
Systems</em>, 3366–74.</p>

<p>Luke, R. 2017. “Phase Retrieval, What’s New?” <em>SIAG/OPT Views and News</em>
25 (1).</p>

<p>Mairal, J. 2015. “Incremental Majorization-Minimization Optimization
with Application to Large-Scale Machine Learning.” <em>SIAM Journal on
Optimization</em> 25 (2):829–55.</p>

<p>Nemirovski, A. 2004. “Prox-Method with Rate of Convergence \(O(1/t)\) for
Variational Inequalities with Lipschitz Continuous Monotone Operators
and Smooth Convex-Concave Saddle Point Problems.” <em>SIAM J. Optim.</em> 15
(1):229–51. <a href="https://doi.org/10.1137/S1052623403425629">https://doi.org/10.1137/S1052623403425629</a>.</p>

<p>Nemirovski, A., A. Juditsky, G. Lan, and A. Shapiro. 2008. “Robust
Stochastic Approximation Approach to Stochastic Programming.” <em>SIAM J.
Optim.</em> 19 (4):1574–1609.
<a href="https://doi-org.offcampus.lib.washington.edu/10.1137/070704277">https://doi-org.offcampus.lib.washington.edu/10.1137/070704277</a>.</p>

<p>Nemirovsky, A.S., and D.B. Yudin. 1983. <em>Problem Complexity and Method
Efficiency in Optimization</em>. A Wiley-Interscience Publication. John
Wiley &amp; Sons, Inc., New York.</p>

<p>Nesterov, Y., and A. Nemirovskii. 1994. <em>Interior-Point Polynomial
Algorithms in Convex Programming</em>. Vol. 13. SIAM Studies in Applied
Mathematics. Society for Industrial; Applied Mathematics (SIAM),
Philadelphia, PA. <a href="https://doi.org/10.1137/1.9781611970791">https://doi.org/10.1137/1.9781611970791</a>.</p>

<p>Nesterov, Yu. 1983. “A Method for Solving the Convex Programming Problem
with Convergence Rate \(O(1/k^{2})\).” <em>Dokl. Akad. Nauk SSSR</em> 269
(3):543–47.</p>

<p>Nesterov, Yu. 2005. “Smooth Minimization of Non-Smooth Functions.” <em>Math.
Program.</em> 103 (1, Ser. A):127–52.
<a href="https://doi.org/10.1007/s10107-004-0552-5">https://doi.org/10.1007/s10107-004-0552-5</a>.</p>

<p>Nesterov, Yu. 2007. “Modified Gauss-Newton Scheme with Worst Case
Guarantees for Global Performance.” <em>Optim. Methods Softw.</em> 22
(3):469–83. <a href="https://doi.org/10.1080/08927020600643812">https://doi.org/10.1080/08927020600643812</a>.</p>

<p>Nesterov, Yu. 2013. “Gradient Methods for Minimizing Composite Functions.”
<em>Math. Program.</em> 140 (1, Ser. B):125–61.
<a href="https://doi.org/10.1007/s10107-012-0629-5">https://doi.org/10.1007/s10107-012-0629-5</a>.</p>

<p>Paquette, C., H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. 2017. 
“Catalyst Acceleration for Gradient-Based Non-Convex
Optimization.” <em>Preprint arXiv:1703.10993</em>.</p>

<p>Polyak, B.T., and A.B. Juditsky. 1992. “Acceleration of Stochastic
Approximation by Averaging.” <em>SIAM J. Control Optim.</em> 30 (4):838–55.
<a href="https://doi.org/10.1137/0330046">https://doi.org/10.1137/0330046</a>.</p>

<p>Rakhlin, A., O. Shamir, and K. Sridharan. 2012. “Making Gradient Descent
Optimal for Strongly Convex Stochastic Optimization.” In <em>Proceedings of
the 29th International Coference on International Conference on Machine
Learning</em>, 1571–8. ICML’12. USA: Omnipress.
<a href="http://dl.acm.org/citation.cfm?id=3042573.3042774">http://dl.acm.org/citation.cfm?id=3042573.3042774</a>.</p>

<p>Robbins, H., and S. Monro. 1951. “A Stochastic Approximation Method.”
<em>Ann. Math. Statistics</em> 22:400–407.</p>

<p>Schmidt, M., N. Le Roux, and F. Bach. 2013. “Minimizing Finite Sums with
the Stochastic Average Gradient.” <em>arXiv:1309.2388</em>.</p>

<p>Shalev-Shwartz, S., and T. Zhang. 2012. “Proximal Stochastic Dual
Coordinate Ascent.” <em>arXiv:1211.2717</em>.</p>

<p>S. Shalev-Shwartz and T. Zhang. 2015. “Accelerated Proximal Stochastic Dual Coordinate Ascent
for Regularized Loss Minimization.” <em>Mathematical Programming</em>.</p>

<p>Singer, A. 2011. “Angular Synchronization by Eigenvectors and
Semidefinite Programming.” <em>Appl. Comput. Harmon. Anal.</em> 30 (1):20–36.
<a href="https://doi.org/10.1016/j.acha.2010.02.001">https://doi.org/10.1016/j.acha.2010.02.001</a>.</p>

<p>Sun, J., Q. Qu, and J. Wright. 2017. “A Geometric Analysis of Phase
Retrieval.” <em>To Appear in Found. Comp. Math., arXiv:1602.06664</em>.</p>

<p>Woodworth, B.E., and N. Srebro. 2016. “Tight Complexity Bounds for
Optimizing Composite Objectives.” In <em>Advances in Neural Information
Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, 3639–47. Curran Associates, Inc.
<a href="http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf">http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf</a>.</p>

<p>Wright, S.J. 1997. <em>Primal-Dual Interior-Point Methods</em>. Society for
Industrial; Applied Mathematics (SIAM), Philadelphia, PA.
<a href="https://doi.org/10.1137/1.9781611971453">https://doi.org/10.1137/1.9781611971453</a>.</p>

<p>Xiao, L., and T. Zhang. 2014. “A Proximal Stochastic Gradient Method
with Progressive Variance Reduction.” <em>SIAM J. Optim.</em> 24 (4):2057–75.
<a href="https://doi.org/10.1137/140961791">https://doi.org/10.1137/140961791</a>.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:3" role="doc-endnote">
      <p>For simplicity of the exposition, the minimization problem is
unconstrained. Simple constraints can be accommodated using a
projection operation. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>













      </div>
    </div>
  </div>

  <footer class="center">
</footer>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112084371-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112084371-1');
</script>

</body>
</html>
